diff --git a/kiji-express/pom.xml b/kiji-express/pom.xml
new file mode 100644
index 0000000000000000000000000000000000000000..723d3ec2eaaf3c0239f9808cff515129c31475c5
--- /dev/null
+++ b/kiji-express/pom.xml
@@ -0,0 +1,203 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+  <!--
+    (c) Copyright 2013 WibiData, Inc.
+
+    See the NOTICE file distributed with this work for additional
+    information regarding copyright ownership.
+
+    Licensed under the Apache License, Version 2.0 (the "License");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+        http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+  -->
+  <modelVersion>4.0.0</modelVersion>
+
+  <artifactId>kiji-express</artifactId>
+  <packaging>jar</packaging>
+
+  <parent>
+    <groupId>org.kiji.express</groupId>
+    <artifactId>kiji-express-root</artifactId>
+    <version>0.14.0-SNAPSHOT</version>
+    <relativePath>..</relativePath>
+  </parent>
+
+  <dependencies>
+    <!--
+      Dependencies, alphabetized by groupId:artifactId:type:version
+    -->
+    <dependency>
+      <groupId>com.twitter</groupId>
+      <artifactId>scalding-core_${scala.version}</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.esotericsoftware.kryo</groupId>
+      <artifactId>kryo</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.twitter</groupId>
+      <artifactId>scalding-args_${scala.version}</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.twitter</groupId>
+      <artifactId>util-eval_${scala.version}</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.twitter.elephantbird</groupId>
+      <artifactId>elephant-bird-hadoop-compat</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.twitter.elephantbird</groupId>
+      <artifactId>elephant-bird-core</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>log4j</groupId>
+      <artifactId>log4j</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.avro</groupId>
+      <artifactId>avro</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-core</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.easymock</groupId>
+      <artifactId>easymock</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.kiji.annotations</groupId>
+      <artifactId>annotations</artifactId>
+      <scope>compile</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.kiji.mapreduce</groupId>
+      <artifactId>kiji-mapreduce</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.kiji.schema</groupId>
+      <artifactId>kiji-schema</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <!--
+      Keep this dependency in compile scope so that users can depend on the KijiExpress testing
+      framework by only depending on the KijiExpress test jar.
+    -->
+    <dependency>
+      <groupId>org.kiji.schema</groupId>
+      <artifactId>kiji-schema</artifactId>
+      <version>${kiji-schema.version}</version>
+      <type>test-jar</type>
+      <scope>compile</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.kiji.schema-shell</groupId>
+      <artifactId>kiji-schema-shell</artifactId>
+      <version>${kiji-schema-shell.version}</version>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.kiji.schema-shell</groupId>
+      <artifactId>kiji-schema-shell</artifactId>
+      <version>${kiji-schema-shell.version}</version>
+      <scope>test</scope>
+      <type>test-jar</type>
+    </dependency>
+    <!--
+      Keep this dependency in compile scope so that users can depend on the KijiExpress testing
+      framework by only depending on the KijiExpress test jar.
+    -->
+    <dependency>
+      <groupId>org.kiji.testing</groupId>
+      <artifactId>fake-hbase</artifactId>
+      <scope>compile</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase</artifactId>
+      <scope>test</scope>
+      <type>test-jar</type>
+    </dependency>
+    <dependency>
+      <groupId>org.scalatest</groupId>
+      <artifactId>scalatest_${scala.version}</artifactId>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.scala-lang</groupId>
+      <artifactId>jline</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.scala-lang</groupId>
+      <artifactId>scala-compiler</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.scala-lang</groupId>
+      <artifactId>scala-library</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.slf4j</groupId>
+      <artifactId>slf4j-api</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.slf4j</groupId>
+      <artifactId>slf4j-log4j12</artifactId>
+      <scope>provided</scope>
+    </dependency>
+  </dependencies>
+
+  <build>
+    <plugins>
+      <plugin>
+        <groupId>org.codehaus.mojo</groupId>
+        <artifactId>versions-maven-plugin</artifactId>
+      </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+      </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-assembly-plugin</artifactId>
+      </plugin>
+      <plugin>
+        <groupId>org.apache.avro</groupId>
+        <artifactId>avro-maven-plugin</artifactId>
+      </plugin>
+      <plugin>
+        <groupId>com.google.code.maven-replacer-plugin</groupId>
+        <artifactId>maven-replacer-plugin</artifactId>
+      </plugin>
+      <plugin>
+        <groupId>net.alchim31.maven</groupId>
+        <artifactId>scala-maven-plugin</artifactId>
+      </plugin>
+      <plugin>
+        <groupId>org.scalastyle</groupId>
+        <artifactId>scalastyle-maven-plugin</artifactId>
+      </plugin>
+
+      <!-- ScalaTest -->
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-surefire-plugin</artifactId>
+      </plugin>
+    </plugins>
+  </build>
+
+</project>
diff --git a/kiji-express/src/main/assembly/release.xml b/kiji-express/src/main/assembly/release.xml
new file mode 100644
index 0000000000000000000000000000000000000000..53abe52c135cfde4023516a0fe9eb17d91edee2e
--- /dev/null
+++ b/kiji-express/src/main/assembly/release.xml
@@ -0,0 +1,90 @@
+<!-- Assembly configuration for the release bundle. -->
+<assembly
+    xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2"
+    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+    xsi:schemaLocation="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2
+                        http://maven.apache.org/xsd/assembly-1.1.2.xsd">
+  <!--
+    (c) Copyright 2013 WibiData, Inc.
+
+    See the NOTICE file distributed with this work for additional
+    information regarding copyright ownership.
+
+    Licensed under the Apache License, Version 2.0 (the "License");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+        http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+  -->
+  <id>release</id>
+  <formats>
+    <format>dir</format>
+    <format>tar.gz</format>
+  </formats>
+
+  <includeBaseDirectory>true</includeBaseDirectory>
+
+  <fileSets>
+    <fileSet>
+      <useDefaultExcludes>true</useDefaultExcludes>
+      <outputDirectory></outputDirectory>
+      <directory></directory>
+      <fileMode>0644</fileMode>
+      <includes>
+        <include>NOTICE.txt</include>
+        <include>LICENSE.txt</include>
+        <include>README.md</include>
+        <include>RELEASE_NOTES.txt</include>
+      </includes>
+      <filtered>true</filtered>
+    </fileSet>
+    <fileSet>
+      <useDefaultExcludes>false</useDefaultExcludes>
+      <outputDirectory>bin</outputDirectory>
+      <directory>src/main/scripts</directory>
+      <fileMode>0755</fileMode>
+      <excludes>
+        <exclude>*~</exclude>
+        <exclude>*.swp</exclude>
+      </excludes>
+      <filtered>false</filtered>
+    </fileSet>
+    <fileSet>
+      <useDefaultExcludes>false</useDefaultExcludes>
+      <outputDirectory>conf</outputDirectory>
+      <directory>src/main/conf</directory>
+      <fileMode>0644</fileMode>
+      <excludes>
+        <exclude>*~</exclude>
+        <exclude>*.swp</exclude>
+      </excludes>
+      <filtered>true</filtered>
+    </fileSet>
+    <fileSet>
+      <!-- scala api documentation -->
+      <useDefaultExcludes>false</useDefaultExcludes>
+      <outputDirectory>docs/apidocs</outputDirectory>
+      <directory>target/apidocs</directory>
+      <fileMode>0644</fileMode>
+    </fileSet>
+  </fileSets>
+
+  <dependencySets>
+    <dependencySet>
+      <outputDirectory>lib</outputDirectory>
+      <scope>runtime</scope>
+      <useTransitiveFiltering>true</useTransitiveFiltering>
+      <fileMode>0644</fileMode>
+      <excludes>
+        <exclude>org.kiji.schema:kiji-schema</exclude>
+      </excludes>
+    </dependencySet>
+  </dependencySets>
+
+</assembly>
diff --git a/kiji-express/src/main/conf/log4j.properties b/kiji-express/src/main/conf/log4j.properties
new file mode 100644
index 0000000000000000000000000000000000000000..a0fdb229aa458ce8f558b23eb5f7b2756f0c332e
--- /dev/null
+++ b/kiji-express/src/main/conf/log4j.properties
@@ -0,0 +1,38 @@
+#
+#   (c) Copyright 2013 WibiData, Inc.
+#
+#   See the NOTICE file distributed with this work for additional
+#   information regarding copyright ownership.
+#
+#   Licensed under the Apache License, Version 2.0 (the "License");
+#   you may not use this file except in compliance with the License.
+#   You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#   Unless required by applicable law or agreed to in writing, software
+#   distributed under the License is distributed on an "AS IS" BASIS,
+#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#   See the License for the specific language governing permissions and
+#   limitations under the License.
+
+log4j.rootLogger=${kiji.logger}
+
+# By default, log INFO to the console.
+kiji.logger=INFO,console
+
+# Console appender:
+log4j.appender.console=org.apache.log4j.ConsoleAppender
+log4j.appender.console.target=System.err
+log4j.appender.console.layout=org.apache.log4j.PatternLayout
+log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c: %m%n
+
+# Quiet down zookeeper, it's too noisy:
+log4j.logger.org.apache.zookeeper=WARN
+log4j.logger.org.apache.hadoop.hbase.zookeeper=WARN
+log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=ERROR
+
+# Loggers in the Kiji framework:
+log4j.logger.org.kiji.schema=INFO
+log4j.logger.org.kiji.mapreduce=INFO
+log4j.logger.org.kiji.express=INFO
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/ColumnFilterSpec.scala b/kiji-express/src/main/scala/org/kiji/express/flow/ColumnFilterSpec.scala
new file mode 100644
index 0000000000000000000000000000000000000000..c3fe8ae2a416b6b41c98f1e5e10e3922a093ccd8
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/ColumnFilterSpec.scala
@@ -0,0 +1,183 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.filter.Filters
+import org.kiji.schema.filter.KijiColumnFilter
+import org.kiji.schema.filter.KijiColumnRangeFilter
+import org.kiji.schema.filter.RegexQualifierColumnFilter
+
+/**
+ * An extendable trait used for column filters in Express, which correspond to Kiji and HBase column
+ * filters.
+ *
+ * Filters are implemented via HBase filters, not on the client side, so they can cut down on the
+ * amount of data transferred over your network.
+ *
+ * These can be used in [[org.kiji.express.flow.ColumnInputSpec]].  Currently the filters provided
+ * are only useful for [[org.kiji.express.flow.ColumnFamilyInputSpec]], because they are filters for
+ * qualifiers when an entire column family is requested.  In the future, there may be filters
+ * provided that can filter on the data returned from a fully qualified column.
+ *
+ * By default, no filter is applied, but you can specify your own.  Only data that pass these
+ * filters will be requested and populated into the tuple.  Two column filters are currently
+ * provided: [[org.kiji.express.flow.ColumnRangeFilterSpec]] and
+ * [[org.kiji.express.flow.RegexQualifierFilterSpec]].  Both of these filter the data
+ * returned from a ColumnFamilyInputSpec by qualifier in some way.  These filters can be composed
+ * with [[org.kiji.express.flow.AndFilterSpec]] and [[org.kiji.express.flow.OrFilterSpec]].
+ *
+ * To specify a range of qualifiers for the cells that should be returned.
+ * {{{
+ *     ColumnRangeFilterSpec(
+ *         minimum = “c”,
+ *         maximum = “m”,
+ *         minimumIncluded = true,
+ *         maximumIncluded = false)
+ * }}}
+ *
+ * A `ColumnInputSpec` with the above filter specified will return all data from all  columns with
+ * qualifiers “c” and later, up to but not including “m”.  You can omit any of the parameters, for
+ * instance, you can write ``ColumnRangeFilterSpec(minimum = “m”, minimumIncluded = true)` to
+ * specify columns with qualifiers “m” and later.
+ *
+ * To specify a regex for the qualifier names that you want data from:
+ * {{{
+ *     RegexQualifierFilterSpec(“http://.*”)
+ * }}}
+ * In this example, only data from columns whose qualifier names start with “http://” are returned.
+ *
+ * See the Sun Java documentation for regular expressions:
+ * http://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html
+ *
+ * To compose filters using `AndFilterSpec`:
+ * {{{
+ *     AndFilterSpec(List(mRegexFilter, mQualifierFilter))
+ * }}}
+ * The `AndFilterSpec` composes a list of `FilterSpec`s, returning only data from columns that
+ * satisfy all the filters in the `AndFilterSpec`.
+ *
+ * Analogously, you can compose them with `OrFilterSpec`:
+ * {{{
+ *     OrFilterSpec(List(mRegexFilter, mQualifierFilter))
+ * }}}
+ * This returns only data from columns that satisfy at least one of the filters.
+ *
+ * `OrFilterSpec` and `AndFilterSpec` can themselves be composed.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+sealed trait ColumnFilterSpec {
+  /** @return a KijiColumnFilter that corresponds to the Express column filter. */
+  private[kiji] def toKijiColumnFilter: KijiColumnFilter
+}
+
+/**
+ * An Express column filter which combines a list of column filters using a logical "and" operator.
+ *
+ * See the scaladocs for [[org.kiji.express.flow.ColumnFilterSpec]] for information on other
+ * filters.
+ *
+ * @param filters to combine with a logical "and" operation.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class AndFilterSpec(filters: Seq[ColumnFilterSpec])
+    extends ColumnFilterSpec {
+  private[kiji] override def toKijiColumnFilter: KijiColumnFilter = {
+    val schemaFilters = filters
+        .map { filter: ColumnFilterSpec => filter.toKijiColumnFilter }
+        .toArray
+
+    Filters.and(schemaFilters: _*)
+  }
+}
+
+/**
+ * An Express column filter which combines a list of column filters using a logical "or" operator.
+ *
+ * See the scaladocs for [[org.kiji.express.flow.ColumnFilterSpec]] for information on other
+ * filters.
+ *
+ * @param filters to combine with a logical "or" operation.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class OrFilterSpec(filters: Seq[ColumnFilterSpec])
+    extends ColumnFilterSpec {
+  private[kiji] override def toKijiColumnFilter: KijiColumnFilter = {
+    val orParams = filters
+        .map { filter: ColumnFilterSpec => filter.toKijiColumnFilter }
+        .toArray
+
+    Filters.or(orParams: _*)
+  }
+}
+
+/**
+ * An Express column filter based on the given minimum and maximum qualifier bounds.
+ *
+ * See the scaladocs for [[org.kiji.express.flow.ColumnFilterSpec]] for information on other
+ * filters.
+ *
+ * @param minimum qualifier bound.
+ * @param maximum qualifier bound.
+ * @param minimumIncluded determines if the lower bound is inclusive.
+ * @param maximumIncluded determines if the upper bound is inclusive.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class ColumnRangeFilterSpec(
+    minimum: Option[String] = None,
+    maximum: Option[String] = None,
+    minimumIncluded: Boolean = true,
+    maximumIncluded: Boolean = false)
+    extends ColumnFilterSpec {
+  private[kiji] override def toKijiColumnFilter: KijiColumnFilter = {
+    new KijiColumnRangeFilter(
+        minimum.getOrElse { null },
+        minimumIncluded,
+        maximum.getOrElse { null },
+        maximumIncluded)
+  }
+}
+
+/**
+ * An Express column filter which matches a regular expression against the full qualifier.
+ *
+ * See the scaladocs for [[org.kiji.express.flow.ColumnFilterSpec]] for information on other
+ * filters.
+ *
+ * @param regex to match on.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class RegexQualifierFilterSpec(regex: String)
+    extends ColumnFilterSpec {
+  private[kiji] override def toKijiColumnFilter: KijiColumnFilter =
+      new RegexQualifierColumnFilter(regex)
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/ColumnInputSpec.scala b/kiji-express/src/main/scala/org/kiji/express/flow/ColumnInputSpec.scala
new file mode 100644
index 0000000000000000000000000000000000000000..66848857437e1d8addec51cad6789b8dfbea178c
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/ColumnInputSpec.scala
@@ -0,0 +1,396 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.apache.avro.Schema
+import org.apache.avro.specific.SpecificRecord
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiInvalidNameException
+
+/**
+ * A request for data from a Kiji table. Provides access to options common to all types of column
+ * input specs. There are two types of column input specs:
+ * <ul>
+ *   <li>
+ *     [[org.kiji.express.flow.QualifiedColumnInputSpec]] - Requests versions of cells from an
+ *     fully-qualified column.
+ *   </li>
+ *   <li>
+ *     [[org.kiji.express.flow.ColumnFamilyInputSpec]] - Requests versions of cells from columns in
+ *     a column family.
+ *   </li>
+ * </ul>
+ *
+ * Requested data will be represented as a sequence of flow cells (`Seq[FlowCell[T] ]`).
+ *
+ * Note: Subclasses of ColumnInputSpec are case classes that override ColumnInputSpec's abstract
+ * methods (e.g., `schema`) with `val`s.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+sealed trait ColumnInputSpec {
+  /**
+   * Maximum number of cells to retrieve starting from the most recent cell. By default, only the
+   * most recent cell is retrieved.
+   *
+   * @return the maximum number of cells to retrieve.
+   */
+  def maxVersions: Int
+
+  /**
+   * Filter that a cell must pass in order to be retrieved. If `None`, no filter is used.
+   *
+   * @return `Some(filter)` if specified or `None`.
+   */
+  def filter: Option[ColumnFilterSpec]
+
+  /**
+   * Specifies the maximum number of cells to maintain in memory when paging through a column.
+   *
+   * @return the paging specification for this column.
+   */
+  def paging: PagingSpec
+
+  /**
+   * Specifies the schema that should be applied to the requested data.
+   *
+   * @return the schema that should be used for reading.
+   */
+  def schemaSpec: SchemaSpec
+
+  /**
+   * Column family of the requested data.
+   *
+   * @return the column family of the requested data.
+   */
+  def family: String
+
+  /**
+   * The [[org.kiji.schema.KijiColumnName]] of the requested data.
+   *
+   * @return the column name of the requested data.
+   */
+  def columnName: KijiColumnName
+}
+
+/**
+ * Provides convenience factory methods for creating [[org.kiji.express.flow.ColumnInputSpec]]
+ * instances.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+object ColumnInputSpec {
+  /**
+   * A request for data from a Kiji table column. The input spec will be for a qualified column if
+   * the column parameter contains a ':', otherwise the input will assumed to be for a column family
+   * (column family names cannot contain ';' characters).
+   *
+   * @param column name of the requested data.
+   * @param maxVersions to read back from the requested column (default is only most recent).
+   * @param filter to use when reading back cells (default is `None`).
+   * @param paging options specifying the maximum number of cells to retrieve from Kiji per page.
+   * @param schemaSpec specifies the schema to use when reading cells. Defaults to
+   *     [[org.kiji.express.flow.SchemaSpec.Writer]].
+   * @return a new column input spec with supplied options.
+   */
+  def apply(
+      column: String,
+      maxVersions: Int = latest,
+      filter: Option[ColumnFilterSpec] = None,
+      paging: PagingSpec = PagingSpec.Off,
+      schemaSpec: SchemaSpec = SchemaSpec.Writer
+  ): ColumnInputSpec = {
+    column.split(':') match {
+      case Array(family, qualifier) =>
+          QualifiedColumnInputSpec(
+              family,
+              qualifier,
+              maxVersions,
+              filter,
+              paging,
+              schemaSpec
+          )
+      case Array(family) =>
+          ColumnFamilyInputSpec(
+              family,
+              maxVersions,
+              filter,
+              paging,
+              schemaSpec
+          )
+      case _ => throw new IllegalArgumentException("column name must contain 'family:qualifier'" +
+        " for a group-type, or 'family' for a map-type column.")
+    }
+  }
+
+  /**
+   * A request for data from a Kiji table column. The input spec will be for a qualified column if
+   * the column parameter contains a ':', otherwise the input will assumed to be for a column family
+   * (column family names cannot contain ';' characters). Data will be read back as the specified
+   * avro class.
+   *
+   * @param column name of the requested data.
+   * @param specificRecord class to read from the column.
+   * @return a new column input spec with supplied options.
+   */
+  def apply(
+      column: String,
+      specificRecord: Class[_ <: SpecificRecord]
+  ): ColumnInputSpec = {
+    ColumnInputSpec(column, schemaSpec = SchemaSpec.Specific(specificRecord))
+  }
+
+  /**
+   * A request for data from a Kiji table column. The input spec will be for a qualified column if
+   * the column parameter contains a ':', otherwise the input will assumed to be for a column family
+   * (column family names cannot contain ';' characters). Data will be read back applying the
+   * specified avro schema.
+   *
+   * @param column name of the requested data.
+   * @param schema to apply to the data.
+   * @return a new column input spec with supplied options.
+   */
+  def apply(
+      column: String,
+      schema: Schema
+  ): ColumnInputSpec = {
+    ColumnInputSpec(column, schemaSpec = SchemaSpec.Generic(schema))
+  }
+}
+
+/**
+ * Specifies a request for versions of cells from a fully-qualified column.
+ *
+ * Basic column example:
+ * {{{
+ *   // Request the latest version of data stored in the "info:name" column.
+ *   val myColumnSpec: QualifiedColumnInputSpec =
+ *       QualifiedColumnInputSpec(
+ *           family = "info",
+ *           qualifier = "name",
+ *           maxVersions = 1
+ *       )
+ * }}}
+ *
+ * Paging can be enabled on a column input specification causing blocks of cells to be retrieved
+ * from Kiji at a time:
+ * {{{
+ *   // Request cells from the "info:status" column retrieving 1000 cells per block.
+ *   val myPagedColumn: QualifiedColumnInputSpec =
+ *       QualifiedColumnInputSpec(
+ *           family = "info",
+ *           qualifier = "status",
+ *           maxVersions = Int.MaxValue,
+ *           paging = PagingSpec.Cells(1000)
+ *       )
+ * }}}
+ *
+ * If compiled avro classes are being used, a class that data should be read as can be specified:
+ * {{{
+ *   // Request cells from the "info:user" column containing User records.
+ *   val myColumnSpec: QualifiedColumnInputSpec =
+ *       QualifiedColumnInputSpec(
+ *           family = "info",
+ *           qualifier = "user",
+ *           maxVersions = 1,
+ *           schemaSpec = SchemaSpec.Specific(classOf[User])
+ *       )
+ * }}}
+ *
+ * @param family of columns the requested data belongs to.
+ * @param qualifier of the column the requested data belongs to.
+ * @param maxVersions to read back from the requested column (default is only most recent).
+ * @param filter to use when reading back cells (default is `None`).
+ * @param paging options specifying the maximum number of cells to retrieve from Kiji per page.
+ * @param schemaSpec specifies the schema to use when reading cells. Defaults to
+ *     [[org.kiji.express.flow.SchemaSpec.Writer]].
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class QualifiedColumnInputSpec(
+    family: String,
+    qualifier: String,
+    maxVersions: Int = latest,
+    filter: Option[ColumnFilterSpec] = None,
+    paging: PagingSpec = PagingSpec.Off,
+    schemaSpec: SchemaSpec = SchemaSpec.Writer
+) extends ColumnInputSpec {
+  override val columnName: KijiColumnName = new KijiColumnName(family, qualifier)
+}
+
+/**
+ * Provides factory functions for creating [[org.kiji.express.flow.QualifiedColumnInputSpec]]
+ * instances.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+object QualifiedColumnInputSpec {
+  /**
+   * Convenience function for creating a [[org.kiji.express.flow.QualifiedColumnInputSpec]] with
+   * a specific Avro record type.
+   *
+   * @param family of columns the requested data belongs to.
+   * @param qualifier of the column the requested data belongs to.
+   * @param specificRecord class to read from the column.
+   * @return a new column input spec with supplied options.
+   */
+  def apply(
+      family: String,
+      qualifier: String,
+      specificRecord: Class[_ <: SpecificRecord]
+  ): QualifiedColumnInputSpec = {
+    QualifiedColumnInputSpec(family, qualifier, schemaSpec = SchemaSpec.Specific(specificRecord))
+  }
+
+  /**
+   * Convenience function for creating a [[org.kiji.express.flow.QualifiedColumnInputSpec]] with
+   * a generic Avro type specified by a [[org.apache.avro.Schema]].
+   *
+   * @param family of columns the requested data belongs to.
+   * @param qualifier of the column the requested data belongs to.
+   * @param schema of generic Avro type to read from the column.
+   * @return a new column input spec with supplied options.
+   */
+  def apply(
+      family: String,
+      qualifier: String,
+      schema: Schema
+  ): QualifiedColumnInputSpec = {
+    QualifiedColumnInputSpec(family, qualifier, schemaSpec = SchemaSpec.Generic(schema))
+  }
+}
+
+/**
+ * Specifies a request for versions of cells from a column family.
+ *
+ * Basic column family example:
+ * {{{
+ *   // Request the latest version of data stored in the "matrix" column family.
+ *   val myColumnFamilySpec: ColumnFamilyInputSpec =
+ *       ColumnFamilyInputSpec(
+ *           family = "matrix",
+ *           maxVersions = 1
+ *       )
+ * }}}
+ *
+ * Filters can be applied to the column qualifier of cells in a column family.
+ * {{{
+ *   // Request cells from the "hits" column that are from columns with qualifiers that begin with
+ *   // the string "http://www.wibidata.com/".
+ *   val myFilteredColumnSpec: ColumnFamilyInputSpec =
+ *       ColumnFamilyInputSpec(
+ *           family = "hits",
+ *           maxVersions = Int.MaxValue,
+ *           filter = RegexQualifierFilterSpec("http://www\.wibidata\.com/.*")
+ *       )
+ * }}}
+ *
+ * Paging can be enabled on a column input specification causing blocks of cells to be retrieved
+ * from Kiji at a time:
+ * {{{
+ *   // Request cells from the "metadata" column family retrieving 1000 cells per block.
+ *   val myPagedColumn: ColumnFamilyInputSpec =
+ *       ColumnFamilyInputSpec(
+ *           family = "metadata",
+ *           maxVersions = Int.MaxValue,
+ *           paging = PagingSpec.Cells(1000)
+ *       )
+ * }}}
+ *
+ * If compiled avro classes are being used, a class that data should be read as can be specified:
+ * {{{
+ *   // Request cells from the "users" column family containing User records.
+ *   val myColumnSpec: ColumnFamilyInputSpec =
+ *       ColumnFamilyInputSpec(
+ *           family = "users",
+ *           maxVersions = 1,
+ *           schemaSpec = SchemaSpec.Specific(classOf[User])
+ *       )
+ * }}}
+ *
+ * @param family of columns the requested data belongs to.
+ * @param maxVersions to read back from the requested column family (default is only most recent).
+ * @param filter to use when reading back cells (default is `None`).
+ * @param paging options specifying the maximum number of cells to retrieve from Kiji per page.
+ * @param schemaSpec specifies the schema to use when reading cells. Defaults to
+ *     [[org.kiji.express.flow.SchemaSpec.Writer]].
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class ColumnFamilyInputSpec(
+    family: String,
+    maxVersions: Int = latest,
+    filter: Option[ColumnFilterSpec] = None,
+    paging: PagingSpec = PagingSpec.Off,
+    schemaSpec: SchemaSpec = SchemaSpec.Writer
+) extends ColumnInputSpec {
+  if (family.contains(':')) {
+    throw new KijiInvalidNameException("Cannot have a ':' in family name for column family request")
+  }
+  override val columnName: KijiColumnName = new KijiColumnName(family)
+}
+
+/**
+ * Provides factory functions for creating [[org.kiji.express.flow.ColumnFamilyInputSpec]]
+ * instances.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+object ColumnFamilyInputSpec {
+  /**
+   * Convenience function for creating a [[org.kiji.express.flow.ColumnFamilyInputSpec]] with a
+   * specific Avro record type.
+   *
+   * @param family of columns the requested data belongs to.
+   * @param specificRecord class to read from the column.
+   * @return a new column input spec with supplied options.
+   */
+  def apply(
+      family: String,
+      specificRecord: Class[_ <: SpecificRecord]
+  ): ColumnFamilyInputSpec = {
+    ColumnFamilyInputSpec(family, schemaSpec = SchemaSpec.Specific(specificRecord))
+  }
+
+  /**
+   * Convenience function for creating a [[org.kiji.express.flow.ColumnFamilyInputSpec]] with a
+   * generic Avro type specified by a [[org.apache.avro.Schema]].
+   *
+   * @param family of columns the requested data belongs to.
+   * @param schema of Avro type to read from the column.
+   * @return a new column input spec with supplied options.
+   */
+  def apply(
+      family: String,
+      schema: Schema
+  ): ColumnFamilyInputSpec = {
+    ColumnFamilyInputSpec(family, schemaSpec = SchemaSpec.Generic(schema))
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/ColumnOutputSpec.scala b/kiji-express/src/main/scala/org/kiji/express/flow/ColumnOutputSpec.scala
new file mode 100644
index 0000000000000000000000000000000000000000..05df82a032aceb6a872ee0a1b1aeb416855f3865
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/ColumnOutputSpec.scala
@@ -0,0 +1,272 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.apache.avro.Schema
+import org.apache.avro.specific.SpecificRecord
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.SchemaSpec.Generic
+import org.kiji.express.flow.SchemaSpec.Specific
+import org.kiji.express.flow.SchemaSpec.Writer
+import org.kiji.express.flow.util.AvroUtil
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiInvalidNameException
+
+/**
+ * Interface for all column output specification objects. ColumnOutputSpec
+ * implementations specify how to write individual fields in an Express flow to a Kiji column or
+ * column family.
+ *
+ * Use the [[org.kiji.express.flow.QualifiedColumnOutputSpec]] to write a field to an individual
+ * Kiji column.
+ *
+ * Use the [[org.kiji.express.flow.ColumnFamilyOutputSpec]] to write a field to a column
+ * family, with a qualifier determined as part of the flow.  The qualifier should be written to a
+ * field specified as part of the ColumnFamilyOutputSpec.
+ *
+ * Note that the subclasses of ColumnOutputSpec are case classes, and so they override
+ * ColumnOutputSpec's abstract methods (e.g., schema) with vals.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+trait ColumnOutputSpec {
+
+  /**
+   * Family which this [[org.kiji.express.flow.ColumnOutputSpec]] belongs to.
+   *
+   * @return family name of output column.
+   */
+  def family: String
+
+  /**
+   * [[org.kiji.schema.KijiColumnName]] of this [[org.kiji.express.flow.ColumnOutputSpec]].
+   *
+   *  @return the name of the column to output to.
+   */
+  def columnName: KijiColumnName
+
+  /**
+   * Specifies the schema of data to be written to the column.
+   * @return the schema specification of data written to the column.
+   */
+  def schemaSpec: SchemaSpec
+
+  /**
+   * Make a best effort attempt to encode a provided value to a type that will be compatible with
+   * the column.  If no such conversion can be made, the original value will be returned.
+   */
+  private[express] def encode: Any => Any = {
+    schemaSpec.schema.map(AvroUtil.avroEncoder).getOrElse(identity)
+  }
+}
+
+/**
+ * Specification for writing to a Kiji column.
+ *
+ * @param family of the output column.
+ * @param qualifier of the output column.
+ * @param schemaSpec The schema specification with which to write values. By default uses
+ *     [[org.kiji.express.flow.SchemaSpec.Writer]].
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class QualifiedColumnOutputSpec(
+    family: String,
+    qualifier: String,
+    schemaSpec: SchemaSpec = Writer
+) extends ColumnOutputSpec {
+  override val columnName: KijiColumnName = new KijiColumnName(family, qualifier)
+}
+
+/**
+ * Provides factory functions for creating [[org.kiji.express.flow.QualifiedColumnOutputSpec]]
+ * instances.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+object QualifiedColumnOutputSpec {
+  /**
+   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]] with a
+   * generic Avro writer schema.
+   *
+   * @param family of the output column.
+   * @param qualifier of the output column.
+   * @param schema with which to write data.
+   */
+  def apply(
+    family: String,
+    qualifier: String,
+    schema: Schema
+  ): QualifiedColumnOutputSpec = {
+    QualifiedColumnOutputSpec(family, qualifier, Generic(schema))
+  }
+
+  /**
+   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]] with a
+   * specific Avro record writer schema.
+   *
+   * @param family of the output column.
+   * @param qualifier of the output column.
+   * @param specificClass of Avro record with which to write.
+   */
+  def apply(
+    family: String,
+    qualifier: String,
+    specificClass: Class[_ <: SpecificRecord]
+  ): QualifiedColumnOutputSpec = {
+    QualifiedColumnOutputSpec(family, qualifier, Specific(specificClass))
+  }
+
+  /**
+   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]].
+   * This constructor takes a column string which must contain the column family and qualifier
+   * in the form 'family:qualifier'.
+   *
+   * @param column The output family and column in format 'family:column'.
+   * @param schemaSpec specification with which to write data.
+   */
+  def apply(
+      column: String,
+      schemaSpec: SchemaSpec
+  ): QualifiedColumnOutputSpec = {
+    column.split(':').toList match {
+      case family :: qualifier :: Nil => QualifiedColumnOutputSpec(family, qualifier, schemaSpec)
+      case _ => throw new IllegalArgumentException(
+          "Must specify column to GroupTypeOutputColumnSpec in the format 'family:qualifier'.")
+    }
+  }
+
+  /**
+   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]] with
+   * a generic Avro writer schema. This constructor takes a column string which must contain the
+   * column family and qualifier in the form 'family:qualifier'.
+   *
+   * @param column The output family and column in format 'family:column'.
+   * @param schema with which to write data.
+   */
+  def apply(
+      column: String,
+      schema: Schema
+  ): QualifiedColumnOutputSpec = {
+    QualifiedColumnOutputSpec(column, Generic(schema))
+  }
+
+  /**
+   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]] with
+   * a generic Avro writer schema. This constructor takes a column string which must contain the
+   * column family and qualifier in the form 'family:qualifier'.
+   *
+   * @param column The output family and column in format 'family:column'.
+   * @param specificClass of Avro record with which to write.
+   */
+  def apply(
+    column: String,
+    specificClass: Class[_ <: SpecificRecord]
+  ): QualifiedColumnOutputSpec = {
+    QualifiedColumnOutputSpec(column, Specific(specificClass))
+  }
+
+  /**
+   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]] with
+   * the [[org.kiji.express.flow.SchemaSpec.Writer]] schema spec. This constructor takes a column
+   * string which must contain the column family and qualifier in the form 'family:qualifier'.
+   *
+   * @param column The output family and column in format 'family:column'.
+   */
+  def apply(
+    column: String
+  ): QualifiedColumnOutputSpec = {
+    QualifiedColumnOutputSpec(column, Writer)
+  }
+}
+
+/**
+ * Specification for writing to a Kiji column family.
+ *
+ * @param family of the output column.
+ * @param qualifierSelector The field in the Express flow indicating the qualifier of the
+ *     output column.
+ * @param schemaSpec The schema spec to use for writing data. By default uses
+ *     [[org.kiji.express.flow.SchemaSpec.Writer]].
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class ColumnFamilyOutputSpec(
+    family: String,
+    qualifierSelector: Symbol,
+    schemaSpec: SchemaSpec = Writer
+) extends ColumnOutputSpec {
+  if (family.contains(':')) {
+    throw new KijiInvalidNameException(
+        "family name in ColumnFamilyOutputSpec may not contain a ':'")
+  }
+  override def columnName: KijiColumnName = new KijiColumnName(family)
+}
+
+/**
+ * Provides factory functions for creating [[org.kiji.express.flow.ColumnFamilyOutputSpec]]
+ * instances.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+object ColumnFamilyOutputSpec {
+  /**
+   * Factory function for creating a [[org.kiji.express.flow.ColumnFamilyOutputSpec]] with a
+   * generic Avro writer schema.
+   *
+   * @param family of the output column.
+   * @param qualifierSelector The field in the Express flow indicating the qualifier of the
+   *     output column.
+   * @param schema The schema to use for writing values.
+   */
+  def apply(
+      family: String,
+      qualifierSelector: Symbol,
+      schema: Schema
+  ): ColumnFamilyOutputSpec = {
+    ColumnFamilyOutputSpec(family, qualifierSelector, Generic(schema))
+  }
+
+  /**
+   * Factory function for creating a [[org.kiji.express.flow.ColumnFamilyOutputSpec]] with a
+   * specific Avro record writer schema.
+   *
+   * @param family of the output column.
+   * @param qualifierSelector The field in the Express flow indicating the qualifier of the
+   *     output column.
+   * @param specificClass The specific record class to use for writes.
+   */
+  def apply(
+    family: String,
+    qualifierSelector: Symbol,
+    specificClass: Class[_ <: SpecificRecord]
+  ): ColumnFamilyOutputSpec = {
+    ColumnFamilyOutputSpec(family, qualifierSelector, Specific(specificClass))
+  }
+}
+
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/EntityId.scala b/kiji-express/src/main/scala/org/kiji/express/flow/EntityId.scala
new file mode 100644
index 0000000000000000000000000000000000000000..cb189a14e03c266dea917ae6b220f10f6eda6471
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/EntityId.scala
@@ -0,0 +1,293 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import java.lang.IllegalStateException
+
+import scala.collection.JavaConverters.asScalaBufferConverter
+import scala.collection.JavaConverters.seqAsJavaListConverter
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.EntityIdFactory
+import org.kiji.schema.{EntityId => JEntityId}
+
+/**
+ * An entity ID, or row key, that can be used to address a row in a Kiji table. This is the
+ * Express representation of a [[org.kiji.schema.EntityId]].
+ *
+ * When writing to a Kiji table using KijiExpress, the 'entityId field of each tuple must contain a
+ * single [[org.kiji.express.flow.EntityId]].  The rest of the tuple fields will be written to the
+ * table according to the field -> column mapping specified by the user in
+ * [[org.kiji.express.flow.KijiOutput]], for the same row that the 'entityId indicated.
+ *
+ * Users can create EntityIds by passing in the objects that compose it. For example, if a Kiji
+ * table uses formatted row keys composed of a string as their first component and a long as the
+ * second, the user can create this as:
+ * {{{
+ * EntityId("myString", 1L)
+ * }}}
+ *
+ * When reading from a Kiji table using [[org.kiji.express.flow.KijiInput]], each row is read into a
+ * tuple, and the 'entityId field of each tuple is automatically populated with an instance of
+ * [[org.kiji.express.flow.EntityId]] corresponding to the EntityID of that row.
+ *
+ * Users can retrieve the index'th element of an EntityId (0-based), as follows:
+ * {{{
+ * MyEntityId(index)
+ * }}}
+ *
+ * EntityIds can either be [[org.kiji.express.flow.EntityId.MaterializedEntityId]] (in the case of
+ * EntityIds from tables with formatted row keys, and all user-created EntityIds), or
+ * [[org.kiji.express.flow.EntityId.HashedEntityId]] (in the case of EntityIds from tables with
+ * hashed or materialization-suppressed row keys).
+ *
+ * Note for joining on EntityIds: MaterializedEntityIds can be compared with each other.
+ * HashedEntityIds can be compared with other HashedEntityIds, but only their hash values are
+ * compared.  You should only attempt to join on HashedEntityIds if they are from the same table.
+ * If you join two pipes on EntityIds, and one of them comes straight from a table and contains
+ * HashedEntityIds, while the other only has the components of the EntityIds, you can't compare an
+ * EntityId constructed directly from the components with the HashedEntityId.  Instead, you need to
+ * construct an EntityId containing the hash of the components according to the table your
+ * HashedEntityId is from.  You can do this using [[org.kiji.schema.KijiTable#getEntityId]].
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+trait EntityId extends Product with Ordered[EntityId] {
+  /**
+   * Get the Java [[org.kiji.schema.EntityId]] associated with this Scala EntityId.
+   *
+   * @param eidFactory is the EntityIdFactory used to convert the underlying components to a Java
+   *     EntityId.
+   * @return the Java EntityId backing this EntityId.
+   */
+  def toJavaEntityId(eidFactory: EntityIdFactory): JEntityId
+
+  /**
+   * Get the index'th component of the EntityId.
+   *
+   * @param index of the component to retrieve.
+   * @return the component at index.
+   */
+  def apply(index: Int): Any = components(index)
+
+  override def productPrefix: String = "EntityId"
+
+  override def canEqual(that: Any): Boolean = that.isInstanceOf[EntityId]
+
+  /**
+   * Returns whether this is equal to `other`.  EntityIds are only comparable to other EntityIds.
+   *
+   * HashedEntityIds can be compared with other HashedEntityIds, but only their hash values are
+   * compared.  You should only attempt to join on HashedEntityIds if they are from the same table.
+   * If you join two pipes on EntityIds, and one of them comes straight from a table and contains
+   * HashedEntityIds, while the other only has the components of the EntityIds, you can't compare an
+   * EntityId constructed directly from the components with the HashedEntityId.  Instead, you need
+   * to construct an EntityId containing the hash of the components according to the table your
+   * HashedEntityId is from.  You can do this using [[org.kiji.schema.KijiTable#getEntityId]].
+   *
+   * @param other object to compare this to.
+   * @return whether the two objects are "equal" according to the definition in this scaladoc.
+   */
+  override def equals(other: Any): Boolean = {
+    if(other.isInstanceOf[EntityId]) {
+      compare(other.asInstanceOf[EntityId]) == 0
+    } else {
+      false
+    }
+  }
+
+  /**
+   * Returns the hashcode of the underlying entityId. For a materialized entity id it returns
+   * the hashcode of the underlying list of components. For a hashed entityId it, it returns
+   * the hashcode of the encoded byte array wrapped as a string.
+   *
+   * @return entityId hashcode.
+   */
+  override def hashCode(): Int
+
+  /**
+   * Returns the underlying components of this entityId. For a materialized entityId, this will
+   * be the list of components. For a hashed entityId, this will be a singleton list containing
+   * the encoded byte[].
+   *
+   * @return a list of the underlying components.
+   */
+  def components: Seq[AnyRef]
+
+  /**
+   * Returns the comparison result ( > 0, 0, < 0).
+   *
+   * MaterializedEntityIds can be compared with each other.
+   * HashedEntityIds can be compared with other HashedEntityIds, but only their hash values are
+   * compared.  You should only attempt to join on HashedEntityIds if they are from the same table.
+   * If you join two pipes on EntityIds, and one of them comes straight from a table and contains
+   * HashedEntityIds, while the other only has the components of the EntityIds, you can't compare an
+   * EntityId constructed directly from the components with the HashedEntityId.  Instead, you need
+   * to construct an EntityId containing the hash of the components according to the table your
+   * HashedEntityId is from.  You can do this using [[org.kiji.schema.KijiTable#getEntityId]].
+   *
+   * @return the comparison result ( > 0, 0, < 0).
+   */
+  override def compare(rhs: EntityId): Int = {
+    val zipped = this.components.zip(rhs.components)
+    // Compare each element lexicographically.
+    zipped.foreach {
+      case (mine, theirs) => {
+        try {
+          val compareResult =
+            if (mine.isInstanceOf[Array[Byte]] && theirs.isInstanceOf[Array[Byte]]) {
+              val myArray = mine.asInstanceOf[Array[Byte]]
+              val rhsArray = theirs.asInstanceOf[Array[Byte]]
+              new String(myArray).compareTo(new String(rhsArray))
+            } else {
+              mine.asInstanceOf[Comparable[Any]].compareTo(theirs)
+            }
+          if (compareResult != 0) {
+            // Return out of the function if these two elements are not equal.
+            return compareResult
+          }
+          // Otherwise, continue.
+        } catch {
+          case e: ClassCastException =>
+            throw new EntityIdFormatMismatchException(components, rhs.components)
+        }
+      }
+    }
+    // If all elements in "zipped" were equal, we compare the lengths.
+    this.components.length.compare(rhs.components.length)
+  }
+}
+
+/**
+ * Companion object for EntityId. Provides factory methods and implementations for EntityIds.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+object EntityId {
+  /**
+   * Creates a KijiExpress EntityId from a Java EntityId.  This is used internally to convert
+   * between kiji-schema and kiji-express.
+   *
+   * Users should not need to use this method.
+   *
+   * @param entityId is the Java EntityId to convert.
+   */
+  @ApiAudience.Framework
+  def fromJavaEntityId(entityId: JEntityId): EntityId = {
+    val hbaseKey = entityId.getHBaseRowKey()
+
+    try {
+      val components = entityId
+        .getComponents
+        .asScala
+        .toSeq
+      MaterializedEntityId(components)
+    } catch {
+      // This is an exception thrown when we try to access components of an entityId which has
+      // materialization suppressed. E.g. Hashed EntityIds. So we are unable to retrieve components,
+      // but the behavior is legal.
+      case ise: IllegalStateException => {
+        HashedEntityId(hbaseKey)
+      }
+    }
+  }
+
+  /**
+   * Creates a new EntityId with the components specified by the user.
+   *
+   * @param components of the EntityId to create.
+   *
+   * @return the created entity id.
+   */
+  def apply(components: Any*): EntityId = {
+    MaterializedEntityId(components.toSeq.map { _.asInstanceOf[AnyRef] })
+  }
+
+  /**
+   * Creates a new EntityId given an array of bytes representing the raw
+   * HBase rowkey.
+   *
+   * @param encoded is the raw hbase rowkey.
+   *
+   * @return the created entity id.
+   */
+  def apply(encoded: Array[Byte]): EntityId = {
+    HashedEntityId(encoded)
+  }
+
+  /**
+   * An EntityId that does not provide access to its components.  It only contains the encoded hash.
+   *
+   * These are never user-created.  They are constructed by KijiExpress when reading from a table
+   * with row key format HASHED or with suppress-materialization enabled.
+   *
+   * @param encoded byte array representation of this EntityId.
+   */
+  @ApiAudience.Private
+  @ApiStability.Experimental
+  @Inheritance.Sealed
+  private[express] case class HashedEntityId(encoded: Array[Byte])
+      extends EntityId {
+
+    /** Lazily create a string encoding of this byte array for hash code purposes. **/
+    @transient
+    private lazy val stringEncoding = new String(encoded)
+
+    /** Lazily create a memoized list of components for the components method. **/
+    @transient
+    override lazy val components: Seq[AnyRef] = List(encoded)
+
+    override def hashCode(): Int = {
+      stringEncoding.hashCode
+    }
+
+    override def toJavaEntityId(eidFactory: EntityIdFactory): JEntityId = {
+      eidFactory.getEntityIdFromHBaseRowKey(components(0).asInstanceOf[Array[Byte]])
+    }
+  }
+
+  /**
+   * An EntityId that provides access to its components.  It can be constructed by the user:
+   *
+   * {{{EntityId(component1, component2, component3)}}}
+   *
+   * KijiExpress will return an instance of this class in the 'entityId field of the tuple if the
+   * row key format in the layout of the table supports returning the components of the EntityId.
+   *
+   * @param components of an EntityId.
+   */
+  @ApiAudience.Private
+  @ApiStability.Experimental
+  @Inheritance.Sealed
+  private[express] case class MaterializedEntityId(override val components: Seq[AnyRef])
+      extends EntityId {
+
+    override def toJavaEntityId(eidFactory: EntityIdFactory): JEntityId = {
+      eidFactory.getEntityId(components.asJava)
+    }
+
+    override def hashCode(): Int = {
+      components.hashCode
+    }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/EntityIdFormatMismatchException.scala b/kiji-express/src/main/scala/org/kiji/express/flow/EntityIdFormatMismatchException.scala
new file mode 100644
index 0000000000000000000000000000000000000000..5d01ec47fe48f27e1466dbd24c89d8d61f370550
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/EntityIdFormatMismatchException.scala
@@ -0,0 +1,38 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+
+/**
+ * A runtime exception thrown when two EntityIds with different formats are compared.
+ *
+ * @param thisComponents components of one of the EntityIds that is mismatched.
+ * @param thatComponents components of the other of the EntityIds that is mismatched.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+class EntityIdFormatMismatchException(thisComponents: Seq[Any],thatComponents: Seq[Any])
+    extends RuntimeException("Mismatched Formats: %s and  %s do not match.".format(
+        "Components: [%s]".format(thisComponents.map { _.getClass.getName }.mkString(",")),
+        "Components: [%s]".format(thatComponents.map { _.getClass.getName }.mkString(","))))
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/FlowCell.scala b/kiji-express/src/main/scala/org/kiji/express/flow/FlowCell.scala
new file mode 100644
index 0000000000000000000000000000000000000000..b79335740d092920037df978f8a024c25a238e48
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/FlowCell.scala
@@ -0,0 +1,123 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import scala.annotation.implicitNotFound
+
+import org.apache.hadoop.hbase.HConstants
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.util.AvroUtil
+import org.kiji.schema.KijiCell
+
+/**
+ * A container for data from a Kiji table. Contains some datum tagged with a column family, column
+ * qualifier, and version. Flow cells are provided when requesting a Kiji table column or column
+ * family.
+ *
+ * Example of accesssing data stored within a flow cell:
+ * {{{
+ *   // Extracts the data stored within cell.
+ *   val myData: T = cell.datum
+ *
+ *   // Extracts the family, qualifier, and version of the cell.
+ *   val myFamily: String = cell.family
+ *   val myQualifier: String = cell.qualifier
+ *   val myVersion: Long = cell.version
+ * }}}
+ *
+ * @tparam T is the type of the datum in the cell.
+ * @param family of columns that this cell comes from.
+ * @param qualifier of the column that this cell comes from.
+ * @param version of the column data that this cell represents. Defaults to the latest timestamp.
+ * @param datum stored in this cell.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+case class FlowCell[T] (
+    family: String,
+    qualifier: String,
+    version: Long = HConstants.LATEST_TIMESTAMP,
+    datum: T)
+
+/**
+ * Companion object containing factory methods for creating flow cells and orderings for sorting
+ * cells.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+object FlowCell {
+  /**
+   * Creates an object that contains the coordinates (family, qualifier, and version) of data in a
+   * Kiji table along with the data itself.
+   *
+   * @tparam T is the type of the datum that this cell contains.
+   * @param cell from a Kiji table produced by the Java API.
+   * @return a FlowCell for use in KijiExpress containing the same family, qualifier, version, and
+   *     datum as the provided KijiCell.
+   */
+  private[kiji] def apply[T](cell: KijiCell[T]): FlowCell[T] = {
+    new FlowCell[T](
+        cell.getFamily,
+        cell.getQualifier,
+        cell.getTimestamp.longValue,
+        AvroUtil.avroToScala(cell.getData).asInstanceOf[T])
+  }
+
+  /**
+   * Provides an implementation of the `scala.Ordering` trait that sorts
+   * [[org.kiji.express.flow.FlowCell]]s by value.
+   *
+   * @tparam T is the type of the datum in the [[org.kiji.express.flow.FlowCell]].
+   * @return an ordering that sorts cells by their value.
+   */
+  @implicitNotFound("The type of the datum in the cells is not Orderable. You may be trying to " +
+      "order a cell that contains a complex type (such as an avro record).")
+  implicit def valueOrder[T](implicit order: Ordering[T]): Ordering[FlowCell[T]] = {
+    Ordering.by { cell: FlowCell[T] => cell.datum }
+  }
+
+  /**
+   * Provides an implementation of the `scala.Ordering` trait that sorts
+   * [[org.kiji.express.flow.FlowCell]]s by version.
+   *
+   * @tparam T is the type of the datum in the [[org.kiji.express.flow.FlowCell]].
+   * @return an ordering that sorts cells by version.
+   */
+  def versionOrder[T]: Ordering[FlowCell[T]] = {
+    Ordering.by { cell: FlowCell[T] => cell.version }
+  }
+
+  /**
+   * Provides an implementation of the `scala.Ordering` trait that sorts
+   * [[org.kiji.express.flow.FlowCell]]s first by the cell's family and then by it's qualifier.
+   *
+   * @tparam T is the type of the datum in the [[org.kiji.express.flow.FlowCell]].
+   * @return an ordering that sorts cells by qualifier.
+   */
+  def qualifierOrder[T]: Ordering[FlowCell[T]] = {
+    Ordering.by { cell: FlowCell[T] =>
+      (cell.family, cell.qualifier)
+    }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/InvalidKijiTapException.scala b/kiji-express/src/main/scala/org/kiji/express/flow/InvalidKijiTapException.scala
new file mode 100644
index 0000000000000000000000000000000000000000..74d4efa153f1a556b9d68e9482c64b606f748b99
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/InvalidKijiTapException.scala
@@ -0,0 +1,34 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+
+/**
+ * This exception is thrown when a KijiTap cannot validate that the Kiji tables or columns it
+ * requires exist.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final class InvalidKijiTapException(message: String)
+    extends RuntimeException(message)
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/KijiInput.scala b/kiji-express/src/main/scala/org/kiji/express/flow/KijiInput.scala
new file mode 100644
index 0000000000000000000000000000000000000000..3bc3d502b9a02962d8fa8d90993a4a1b1687907b
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/KijiInput.scala
@@ -0,0 +1,198 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+
+/**
+ * Factory methods for constructing [[org.kiji.express.flow.KijiSource]]s that will be used as
+ * inputs to a KijiExpress flow. Two basic APIs are provided with differing complexity.
+ *
+ * Simple:
+ * {{{
+ *   // Create a KijiSource that reads from the table named `mytable` reading the columns
+ *   // `info:column1` and `info:column2` to the fields `'column1` and `'column2`.
+ *   KijiInput(
+ *       tableUri = "kiji://localhost:2181/default/mytable",
+ *       "info:column1" -> 'column1,
+ *       "info:column2" -> 'column2)
+ * }}}
+ *
+ * Verbose:
+ * {{{
+ *   // Create a KijiSource that reads from the table named `mytable` reading the columns
+ *   // `info:column1` and `info:column2` to the fields `'column1` and `'column2`.
+ *   KijiInput(
+ *       tableUri = "kiji://localhost:2181/default/mytable",
+ *       columns = Map(
+ *           QualifiedColumnInputSpec("info", "column1") -> 'column1,
+ *           QualifiedColumnInputSpec("info", "column2") -> 'column2)
+ * }}}
+ *
+ * The verbose methods allow you to instantiate explicity
+ * [[org.kiji.express.flow.QualifiedColumnInputSpec]] and
+ * [[org.kiji.express.flow.ColumnInputSpec]] objects.
+ * Use the verbose method to specify options for the input columns, e.g.,
+ * {{{
+ *   // Create a KijiSource that reads from the table named `mytable` reading the columns
+ *   // `info:column1` and `info:column2` to the fields `'column1` and `'column2`.
+ *   KijiInput(
+ *       tableUri = "kiji://localhost:2181/default/mytable",
+ *       columns = Map(
+ *           QualifiedColumnInputSpec("info", "column1", maxVersions=5) -> 'column1,
+ *           QualifiedColumnInputSpec("info", "column2", paging=Cells(10)) -> 'column2)
+ * }}}
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+object KijiInput {
+  /** Default time range for KijiSource */
+  private val defaultTimeRange: TimeRange = All
+
+  /**
+   * An internal factory method for creating a [[org.kiji.express.flow.KijiSource]] for reading
+   * cells from a Kiji table.
+   *
+   * @param tableUri addressing a table in a Kiji instance.
+   * @param timeRange that cells must fall into to be retrieved.
+   * @param columns are a series of pairs mapping column input specs to tuple field names.
+   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
+   *     simply "family".
+   * @return a source for data in the Kiji table, whose row tuples will contain fields with cell
+   *     data from the requested columns and map-type column families.
+   */
+  private def applyAllArgsSym(
+      tableUri: String,
+      timeRange: TimeRange,
+      columns: (String, Symbol)*
+  ): KijiSource = {
+    val columnMap = columns
+        .map { case (col, field) => (field, ColumnInputSpec(col)) }
+        .toMap
+
+    new KijiSource(
+        tableUri,
+        timeRange,
+        None,
+        inputColumns = columnMap
+    )
+  }
+
+  /**
+   * An internal factory method for creating a [[org.kiji.express.flow.KijiSource]] for
+   * reading cells from a Kiji table.
+   *
+   * @param tableUri addressing a table in a Kiji instance.
+   * @param timeRange that cells must fall into to be retrieved.
+   * @param columns are a series of pairs mapping column input specs to tuple field names.
+   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
+   *     simply "family".
+   * @return a source for data in the Kiji table, whose row tuples will contain fields with
+   *     cell data from the requested columns and map-type column families.
+   */
+  private def applyAllArgsMap(
+      tableUri: String,
+      timeRange: TimeRange,
+      columns: Map[_ <: ColumnInputSpec, Symbol]
+  ): KijiSource = {
+    val columnMap = columns
+        .map { entry: (ColumnInputSpec, Symbol) => entry.swap }
+
+    new KijiSource(
+        tableUri,
+        timeRange,
+        None,
+        inputColumns = columnMap
+    )
+  }
+
+  /**
+   * A factory method for creating a KijiSource.
+   *
+   * @param tableUri addressing a table in a Kiji instance.
+   * @param columns are a series of pairs mapping column input specs to tuple field names.
+   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
+   *     simply "family".
+   * @return a source for data in the Kiji table, whose row tuples will contain fields with cell
+   *     data from the requested columns and map-type column families.
+   */
+  def apply(
+      tableUri: String,
+      columns: (String, Symbol)*
+  ): KijiSource = {
+    applyAllArgsSym(tableUri, defaultTimeRange, columns: _*)
+  }
+
+  /**
+   * A factory method for creating a KijiSource.
+   *
+   * @param tableUri addressing a table in a Kiji instance.
+   * @param timeRange that cells must fall into to be retrieved.
+   * @param columns are a series of pairs mapping column input specs to tuple field names.
+   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
+   *     simply "family".
+   * @return a source for data in the Kiji table, whose row tuples will contain fields with cell
+   *     data from the requested columns and map-type column families.
+   */
+  def apply(
+      tableUri: String,
+      timeRange: TimeRange,
+      columns: (String, Symbol)*
+  ): KijiSource = {
+    applyAllArgsSym(tableUri, timeRange, columns: _*)
+  }
+
+  /**
+   * A factory method for creating a KijiSource.
+   *
+   * @param tableUri addressing a table in a Kiji instance.
+   * @param columns are a series of pairs mapping column input specs to tuple field names.
+   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
+   *     simply "family".
+   * @return a source for data in the Kiji table, whose row tuples will contain fields with cell
+   *     data from the requested columns and map-type column families.
+   */
+  def apply(
+      tableUri: String,
+      columns: Map[_ <: ColumnInputSpec, Symbol]
+  ): KijiSource = {
+    applyAllArgsMap(tableUri, defaultTimeRange, columns)
+  }
+
+  /**
+   * A factory method for creating a KijiSource.
+   *
+   * @param tableUri addressing a table in a Kiji instance.
+   * @param timeRange that cells must fall into to be retrieved.
+   * @param columns are a series of pairs mapping column input specs to tuple field names.
+   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
+   *     simply "family".
+   * @return a source for data in the Kiji table, whose row tuples will contain fields with cell
+   *     data from the requested columns and map-type column families.
+   */
+  def apply(
+      tableUri: String,
+      timeRange: TimeRange,
+      columns: Map[_ <: ColumnInputSpec, Symbol]
+  ): KijiSource = {
+    applyAllArgsMap(tableUri, timeRange, columns)
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/KijiJob.scala b/kiji-express/src/main/scala/org/kiji/express/flow/KijiJob.scala
new file mode 100644
index 0000000000000000000000000000000000000000..b2bb164646521b4add511879a65deec546b47785
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/KijiJob.scala
@@ -0,0 +1,115 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import scala.collection.JavaConverters.collectionAsScalaIterableConverter
+
+import java.util.Properties
+
+import cascading.flow.hadoop.util.HadoopUtil
+import cascading.tap.Tap
+import com.twitter.scalding.Args
+import com.twitter.scalding.HadoopTest
+import com.twitter.scalding.Hdfs
+import com.twitter.scalding.Job
+import com.twitter.scalding.Mode
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.hbase.HBaseConfiguration
+import org.apache.hadoop.mapred.JobConf
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.framework.KijiTap
+import org.kiji.express.flow.framework.LocalKijiTap
+import org.kiji.express.flow.util.AvroTupleConversions
+import org.kiji.express.flow.util.PipeConversions
+
+/**
+ * KijiJob is KijiExpress's extension of Scalding's `Job`, and users should extend it when writing
+ * their own jobs in KijiExpress.  It provides extra conversions that Express needs for KijiPipes.
+ *
+ * @param args to the job. These get parsed in from the command line by Scalding.  Within your own
+ *     KijiJob, `args("input")` will evaluate to "SomeFile.txt" if your command line contained the
+ *     argument `--input SomeFile.txt`
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Extensible
+class KijiJob(args: Args = Args(Nil))
+    extends Job(args)
+    with PipeConversions
+    with AvroTupleConversions {
+  override def validateSources(mode: Mode): Unit = {
+    val taps: List[Tap[_, _, _]] =
+        flowDef.getSources.values.asScala.toList ++
+        flowDef.getSinks.values.asScala.toList
+
+    // Retrieve the configuration
+    var conf: Configuration = HBaseConfiguration.create()
+    implicitly[Mode] match {
+      case Hdfs(_, configuration) => {
+        conf = configuration
+      }
+      case HadoopTest(configuration, _) => {
+        conf = configuration
+      }
+      case _ =>
+    }
+
+    // Validate that the Kiji parts of the sources (tables, columns) are valid and exist.
+    taps.foreach {
+      case kijiTap: KijiTap => kijiTap.validate(new JobConf(conf))
+      case localKijiTap: LocalKijiTap => {
+        val properties: Properties = new Properties()
+        properties.putAll(HadoopUtil.createProperties(conf))
+        localKijiTap.validate(properties)
+      }
+      case _ => // No Kiji parts to verify.
+    }
+
+    // Call any validation that scalding's Job class does.
+    super.validateSources(mode)
+  }
+
+  override def config(implicit mode: Mode): Map[AnyRef, AnyRef] = {
+    val baseConfig = super.config(mode)
+
+    // We configure as is done in Scalding's Job, but then append to mapred.child.java.opts to
+    // disable schema validation. This system property is only useful for KijiSchema v1.1. In newer
+    // versions of KijiSchema, this property has no effect.
+    val disableValidation = " -Dorg.kiji.schema.impl.AvroCellEncoder.SCHEMA_VALIDATION=DISABLED"
+    val oldJavaOptions = baseConfig
+        .get("mapred.child.java.opts")
+        .getOrElse("")
+
+    // Add support for our Kryo Avro serializers (see org.kiji.express.flow.framework.KryoKiji).
+    val oldSerializations = baseConfig("io.serializations").toString
+    require(oldSerializations.contains("com.twitter.scalding.serialization.KryoHadoop"))
+    val newSerializations = oldSerializations.replaceFirst(
+        "com.twitter.scalding.serialization.KryoHadoop",
+        "org.kiji.express.flow.framework.serialization.KryoKiji")
+
+    // Append all the new keys.
+    baseConfig +
+        ("mapred.child.java.opts" -> (oldJavaOptions + disableValidation)) +
+        ("io.serializations" -> newSerializations)
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/KijiOutput.scala b/kiji-express/src/main/scala/org/kiji/express/flow/KijiOutput.scala
new file mode 100644
index 0000000000000000000000000000000000000000..0340bb4cb50cffa1c57f855060564e417dad51b3
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/KijiOutput.scala
@@ -0,0 +1,160 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+
+/**
+ * Factory methods for constructing [[org.kiji.express.flow.KijiSource]]s that will be used as
+ * outputs of a KijiExpress flow. Two basic APIs are provided with differing complexity.
+ *
+ * Simple:
+ * {{{
+ *   // Create a KijiOutput that writes to the table named `mytable` putting timestamps in the
+ *   // `'timestamps` field and writing the fields `'column1` and `'column2` to the columns
+ *   // `info:column1` and `info:column2`.
+ *   KijiOutput(
+ *       tableUri = "kiji://localhost:2181/default/mytable",
+ *       timestampField = 'timestamps,
+ *       'column1 -> "info:column1",
+ *       'column2 -> "info:column2")
+ * }}}
+ *
+ * Verbose:
+ * {{{
+ *   // Create a KijiOutput that writes to the table named `mytable` putting timestamps in the
+ *   // `'timestamps` field and writing the fields `'column1` and `'column2` to the columns
+ *   // `info:column1` and `info:column2`.
+ *   KijiOutput(
+ *       tableUri = "kiji://localhost:2181/default/mytable",
+ *       timestampField = 'timestamps,
+ *       columns = Map(
+ *           // Enable paging for `info:column1`.
+ *           'column1 -> QualifiedColumnOutputSpec("info", "column1"),
+ *           'column2 -> QualifiedColumnOutputSpec("info", "column2")))
+ * }}}
+ *
+ * The verbose methods allow you to instantiate explicity
+ * [[org.kiji.express.flow.QualifiedColumnOutputSpec]] and
+ * [[org.kiji.express.flow.ColumnOutputSpec]] objects.
+ * Use the verbose method to specify options for the output columns, e.g.,
+ * {{{
+ *   // Create a KijiSource that reads from the table named `mytable` reading the columns
+ *   // `info:column1` and `info:column2` to the fields `'column1` and `'column2`.
+ *   KijiOutput(
+ *       tableUri = "kiji://localhost:2181/default/mytable",
+ *       timestampField = 'timestamps,
+ *       columns = Map(
+ *           QualifiedColumnOutputSpec("info", "column1", schemaId=Some(12)) -> 'column1,
+ *           QualifiedColumnOutputSpec("info", "column2") -> 'column2)
+ * }}}
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+object KijiOutput {
+  /**
+   * A factory method for instantiating [[org.kiji.express.flow.KijiSource]]s used as sinks. This
+   * method permits specifying the full range of read options for each column. Values written will
+   * be tagged with the current time at write.
+   *
+   * @param tableUri that addresses a table in a Kiji instance.
+   * @param columns is a mapping specifying what column to write each field value to.
+   * @return a source that can write tuple field values to columns of a Kiji table.
+   */
+  def apply(
+      tableUri: String,
+      columns: Map[Symbol, _ <: ColumnOutputSpec]
+  ): KijiSource = {
+    new KijiSource(
+        tableAddress = tableUri,
+        timeRange = All,
+        timestampField = None,
+        outputColumns = columns)
+  }
+
+  /**
+   * A factory method for instantiating [[org.kiji.express.flow.KijiSource]]s used as sinks. This
+   * method permits specifying the full range of read options for each column.
+   *
+   * @param tableUri that addresses a table in a Kiji instance.
+   * @param columns is a mapping specifying what column to write each field value to.
+   * @param timestampField is the name of a tuple field that will contain cell timestamps when the
+   *     source is used for writing.
+   * @return a source that can write tuple field values to columns of a Kiji table.
+   */
+  def apply(
+      tableUri: String,
+      timestampField: Symbol,
+      columns: Map[Symbol, _ <: ColumnOutputSpec]
+  ): KijiSource = {
+    require(timestampField != null)
+
+    new KijiSource(
+        tableAddress = tableUri,
+        timeRange = All,
+        timestampField = Some(timestampField),
+        outputColumns = columns)
+  }
+
+  /**
+   * A factory method for instantiating [[org.kiji.express.flow.KijiSource]]s used as sinks. Values
+   * written will be tagged with the current time at write.
+   *
+   * @param tableUri that addresses a table in a Kiji instance.
+   * @param columns are a series of pairs mapping tuple field names to Kiji column names. When
+   *     naming columns, use the format `"family:qualifier"`.
+   * @return a source that can write tuple field values to columns of a Kiji table.
+   */
+  def apply(
+      tableUri: String,
+      columns: (Symbol, String)*
+  ): KijiSource = {
+    val columnMap = columns
+        .toMap
+        .mapValues(QualifiedColumnOutputSpec(_))
+
+    KijiOutput(tableUri, columnMap)
+  }
+
+  /**
+   * A factory method for instantiating [[org.kiji.express.flow.KijiSource]]s used as sinks.
+   *
+   * @param tableUri that addresses a table in a Kiji instance.
+   * @param columns are a series of pairs mapping tuple field names to Kiji column names. When
+   *     naming columns, use the format `"family:qualifier"`.
+   * @param timestampField is the name of a tuple field that will contain cell timestamps when the
+   *     source is used for writing.
+   * @return a source that can write tuple field values to columns of a Kiji table.
+   */
+  def apply(
+      tableUri: String,
+      timestampField: Symbol,
+      columns: (Symbol, String)*
+  ): KijiSource = {
+    require(timestampField != null)
+
+    val columnMap = columns
+        .toMap
+        .mapValues(QualifiedColumnOutputSpec(_))
+
+    KijiOutput(tableUri, timestampField, columnMap)
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/KijiPipe.scala b/kiji-express/src/main/scala/org/kiji/express/flow/KijiPipe.scala
new file mode 100644
index 0000000000000000000000000000000000000000..ac8a3395e9bc9c4826dd4d51f679863f5fa4feac
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/KijiPipe.scala
@@ -0,0 +1,178 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import cascading.flow.Flow
+import cascading.pipe.Pipe
+import cascading.tuple.Fields
+import com.twitter.scalding.Args
+import com.twitter.scalding.Job
+import com.twitter.scalding.Mode
+import com.twitter.scalding.TupleConversions
+import com.twitter.scalding.TupleSetter
+import org.apache.avro.Schema
+import org.apache.avro.generic.GenericRecord
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.framework.serialization.KijiLocker
+import org.kiji.express.flow.util.AvroGenericTupleConverter
+import org.kiji.express.repl.ExpressShell
+import org.kiji.express.repl.Implicits
+import org.kiji.express.repl.Implicits.pipeToRichPipe
+
+/**
+ * A class that adds Kiji-specific functionality to a Cascading pipe. This includes running pipes
+ * outside of the context of a Scalding Job.
+ *
+ * A `KijiPipe` should be obtained by end-users during the course of authoring a Scalding flow via
+ * an implicit conversion available in [[org.kiji.express.repl.Implicits]].
+ *
+ * @param pipe enriched with extra functionality.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+class KijiPipe(private[express] val pipe: Pipe) extends TupleConversions {
+  /**
+   * Gets a job that can be used to run the data pipeline.
+   *
+   * @param args that should be used to construct the job.
+   * @return a job that can be used to run the data pipeline.
+   */
+  private[express] def getJob(args: Args): Job = new KijiJob(args) {
+    // The job's constructor should evaluate to the pipe to run.
+    pipe
+
+    /**
+     *  The flow definition used by this job, which should be the same as that used by the user
+     *  when creating their pipe.
+     */
+    override implicit val flowDef = Implicits.flowDef
+
+    /**
+     * Obtains a configuration used when running the job.
+     *
+     * This overridden method uses the same configuration as a standard Scalding job,
+     * but adds options specific to KijiExpress, including adding a jar containing compiled REPL
+     * code to the distributed cache if the REPL is running.
+     *
+     * @param mode used to run the job (either local or hadoop).
+     * @return the configuration that should be used to run the job.
+     */
+    override def config(implicit mode: Mode): Map[AnyRef, AnyRef] = {
+      // Use the configuration from Scalding Job as our base.
+      val configuration = super.config(mode)
+
+      /** Appends a comma to the end of a string. */
+      def appendComma(str: Any): String = str.toString + ","
+
+      // If the REPL is running, we should add tmpjars passed in from the command line,
+      // and a jar of REPL code, to the distributed cache of jobs run through the REPL.
+      val replCodeJar = ExpressShell.createReplCodeJar()
+      val tmpJarsConfig =
+        if (replCodeJar.isDefined) {
+          Map("tmpjars" -> {
+              // Use tmpjars already in the configuration.
+              configuration
+                  .get("tmpjars")
+                  .map(appendComma)
+                  .getOrElse("") +
+              // And tmpjars passed to ExpressShell from the command line when started.
+              ExpressShell.tmpjars
+                  .map(appendComma)
+                  .getOrElse("") +
+              // And a jar of code compiled by the REPL.
+              "file://" + replCodeJar.get.getAbsolutePath
+          })
+        } else {
+          // No need to add the tmpjars to the configuration
+          Map[String, String]()
+        }
+
+      val userClassPathFirstConfig = Map("mapreduce.task.classpath.user.precedence" -> "true")
+
+      configuration ++ tmpJarsConfig ++ userClassPathFirstConfig
+    }
+
+    /**
+     * Builds a flow from the flow definition used when creating the pipeline run by this job.
+     *
+     * This overridden method operates the same as that of the super class,
+     * but clears the implicit flow definition defined in [[org.kiji.express.repl.Implicits]]
+     * after the flow has been built from the flow definition. This allows additional pipelines
+     * to be constructed and run after the pipeline encapsulated by this job.
+     *
+     * @param mode the mode in which the built flow will be run.
+     * @return the flow created from the flow definition.
+     */
+    override def buildFlow(implicit mode: Mode): Flow[_] = {
+      val flow = super.buildFlow(mode)
+      Implicits.resetFlowDef()
+      flow
+    }
+  }
+
+  /**
+   * Runs this pipe as a Scalding job.
+   */
+  def run() {
+    getJob(new Args(Map())).run(Mode.mode)
+
+    // Clear the REPL state after running a job.
+    Implicits.resetFlowDef()
+  }
+
+  /**
+   * Packs the specified fields into an Avro [[org.apache.avro.generic.GenericRecord]].  The
+   * provided field names must match the fields of the generic record specified by the schema.
+   *
+   * @param fields is the mapping of input fields (to be packed into the
+   *     [[org.apache.avro.generic.GenericRecord]]) to output field which will contain
+   *     the [[org.apache.avro.generic.GenericRecord]].
+   * @return a pipe containing all input fields, and an additional field containing an
+   *     [[org.apache.avro.generic.GenericRecord]].
+   */
+  def packGenericRecord(fields: (Fields, Fields))(schema: Schema): Pipe = {
+    require(fields._2.size == 1, "Cannot pack generic record to more than a single field.")
+    require(schema.getType == Schema.Type.RECORD, "Cannot pack non-record Avro type.")
+    pipe.map(fields) { input: GenericRecord => input } (
+      new AvroGenericTupleConverter(KijiLocker(schema)), implicitly[TupleSetter[GenericRecord]])
+  }
+
+  /**
+   * Packs the specified fields into an Avro [[org.apache.avro.generic.GenericRecord]] and drops
+   * other fields from the flow.  The provided field names must match the fields of the
+   * generic record specified by the schema.
+   *
+   * @param fields is the mapping of input fields (to be packed into the
+   *     [[org.apache.avro.generic.GenericRecord]]) to new output field which will
+   *     contain the [[org.apache.avro.generic.GenericRecord]].
+   * @return a pipe containing a single field with an Avro
+   *     [[org.apache.avro.generic.GenericRecord]].
+   */
+  def packGenericRecordTo(fields: (Fields, Fields))(schema: Schema): Pipe = {
+    require(fields._2.size == 1, "Cannot pack generic record to more than a single field.")
+    require(schema.getType == Schema.Type.RECORD, "Cannot pack to non-record Avro type.")
+    pipe.mapTo(fields) { input: GenericRecord => input } (
+      new AvroGenericTupleConverter(KijiLocker(schema)), implicitly[TupleSetter[GenericRecord]])
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/KijiSource.scala b/kiji-express/src/main/scala/org/kiji/express/flow/KijiSource.scala
new file mode 100644
index 0000000000000000000000000000000000000000..3467b53507f0865adeb8887235d2685c17b6cfd0
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/KijiSource.scala
@@ -0,0 +1,518 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import scala.collection.JavaConverters.asScalaIteratorConverter
+import scala.collection.JavaConverters.mapAsJavaMapConverter
+import scala.collection.mutable.Buffer
+
+import java.io.OutputStream
+import java.util.Properties
+
+import cascading.flow.FlowProcess
+import cascading.flow.hadoop.util.HadoopUtil
+import cascading.scheme.Scheme
+import cascading.scheme.SinkCall
+import cascading.tap.Tap
+import cascading.tuple.Fields
+import cascading.tuple.Tuple
+import cascading.tuple.TupleEntry
+import com.google.common.base.Objects
+import com.twitter.scalding.AccessMode
+import com.twitter.scalding.Test
+import com.twitter.scalding.HadoopTest
+import com.twitter.scalding.Hdfs
+import com.twitter.scalding.Local
+import com.twitter.scalding.Mode
+import com.twitter.scalding.Read
+import com.twitter.scalding.Source
+import com.twitter.scalding.Write
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.hbase.HBaseConfiguration
+import org.apache.hadoop.mapred.JobConf
+import org.apache.hadoop.mapred.OutputCollector
+import org.apache.hadoop.mapred.RecordReader
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.express.flow.framework.KijiScheme
+import org.kiji.express.flow.framework.KijiTap
+import org.kiji.express.flow.framework.LocalKijiScheme
+import org.kiji.express.flow.framework.LocalKijiTap
+import org.kiji.express.flow.framework.OutputContext
+import org.kiji.express.flow.util.Resources._
+import org.kiji.mapreduce.framework.KijiConfKeys
+import org.kiji.schema.EntityIdFactory
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiDataRequest
+import org.kiji.schema.KijiRowData
+import org.kiji.schema.KijiRowScanner
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiTableWriter
+import org.kiji.schema.KijiURI
+import org.kiji.schema.layout.CellSpec
+
+/**
+ * A read or write view of a Kiji table.
+ *
+ * A Scalding `Source` provides a view of a data source that can be read as Scalding tuples. It
+ * is comprised of a Cascading tap [[cascading.tap.Tap]], which describes where the data is and how
+ * to access it, and a Cascading Scheme [[cascading.scheme.Scheme]], which describes how to read
+ * and interpret the data.
+ *
+ * When reading from a Kiji table, a `KijiSource` will provide a view of a Kiji table as a
+ * collection of tuples that correspond to rows from the Kiji table. Which columns will be read
+ * and how they are associated with tuple fields can be configured,
+ * as well as the time span that cells retrieved must belong to.
+ *
+ * When writing to a Kiji table, a `KijiSource` views a Kiji table as a collection of tuples that
+ * correspond to cells from the Kiji table. Each tuple to be written must provide a cell address
+ * by specifying a Kiji `EntityID` in the tuple field `entityId`, a value to be written in a
+ * configurable field, and (optionally) a timestamp in a configurable field.
+ *
+ * End-users cannot directly obtain instances of `KijiSource`. Instead,
+ * they should use the factory methods provided as part of the [[org.kiji.express.flow]] module.
+ *
+ * @param tableAddress is a Kiji URI addressing the Kiji table to read or write to.
+ * @param timeRange that cells read must belong to. Ignored when the source is used to write.
+ * @param timestampField is the name of a tuple field that will contain cell timestamp when the
+ *     source is used for writing. Specify `None` to write all
+ *     cells at the current time.
+ * @param inputColumns is a one-to-one mapping from field names to Kiji columns. The columns in the
+ *     map will be read into their associated tuple fields.
+ * @param outputColumns is a one-to-one mapping from field names to Kiji columns. Values from the
+ *     tuple fields will be written to their associated column.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+final class KijiSource private[express] (
+    val tableAddress: String,
+    val timeRange: TimeRange,
+    val timestampField: Option[Symbol],
+    val inputColumns: Map[Symbol, ColumnInputSpec] = Map(),
+    val outputColumns: Map[Symbol, ColumnOutputSpec] = Map()
+) extends Source {
+  import KijiSource._
+
+  private type HadoopScheme = Scheme[JobConf, RecordReader[_, _], OutputCollector[_, _], _, _]
+
+  /** The URI of the target Kiji table. */
+  private val tableUri: KijiURI = KijiURI.newBuilder(tableAddress).build()
+
+  /** A Kiji scheme intended to be used with Scalding/Cascading's hdfs mode. */
+  private val kijiScheme: KijiScheme =
+      new KijiScheme(
+          timeRange,
+          timestampField,
+          convertKeysToStrings(inputColumns),
+          convertKeysToStrings(outputColumns)
+      )
+
+  /** A Kiji scheme intended to be used with Scalding/Cascading's local mode. */
+  private val localKijiScheme: LocalKijiScheme =
+      new LocalKijiScheme(
+          timeRange,
+          timestampField,
+          convertKeysToStrings(inputColumns),
+          convertKeysToStrings(outputColumns)
+      )
+
+  /**
+   * Creates a Scheme that writes to/reads from a Kiji table for usage with
+   * the hadoop runner.
+   */
+  override val hdfsScheme: HadoopScheme = kijiScheme
+      // This cast is required due to Scheme being defined with invariant type parameters.
+      .asInstanceOf[HadoopScheme]
+
+  /**
+   * Creates a Scheme that writes to/reads from a Kiji table for usage with
+   * the local runner.
+   */
+  override val localScheme: LocalScheme = localKijiScheme
+      // This cast is required due to Scheme being defined with invariant type parameters.
+      .asInstanceOf[LocalScheme]
+
+  /**
+   * Create a connection to the physical data source (also known as a Tap in Cascading)
+   * which, in this case, is a [[org.kiji.schema.KijiTable]].
+   *
+   * @param readOrWrite Specifies if this source is to be used for reading or writing.
+   * @param mode Specifies which job runner/flow planner is being used.
+   * @return A tap to use for this data source.
+   */
+  override def createTap(readOrWrite: AccessMode)(implicit mode: Mode): Tap[_, _, _] = {
+    /** Combination of normal input columns and input versions of the output columns (the latter are
+     * needed for reading back written results) */
+    def getInputColumnsForTesting: Map[String, ColumnInputSpec] = {
+      val testingInputColumnsFromReads = inputColumnSpecifyAllData(
+          convertKeysToStrings(inputColumns))
+      val testingInputColumnsFromWrites = inputColumnSpecifyAllData(
+          convertKeysToStrings(outputColumns)
+          .mapValues { x: ColumnOutputSpec => ColumnInputSpec(x.columnName.toString) })
+      testingInputColumnsFromReads ++ testingInputColumnsFromWrites
+    }
+
+    val tap: Tap[_, _, _] = mode match {
+      // Production taps.
+      case Hdfs(_,_) => new KijiTap(tableUri, kijiScheme).asInstanceOf[Tap[_, _, _]]
+      case Local(_) => new LocalKijiTap(tableUri, localKijiScheme).asInstanceOf[Tap[_, _, _]]
+
+      // Test taps.
+      case HadoopTest(conf, buffers) => {
+        readOrWrite match {
+          case Read => {
+            val scheme = kijiScheme
+            populateTestTable(tableUri, buffers(this), scheme.getSourceFields, conf)
+
+            new KijiTap(tableUri, scheme).asInstanceOf[Tap[_, _, _]]
+          }
+          case Write => {
+            val scheme = new TestKijiScheme(
+                timestampField,
+                getInputColumnsForTesting,
+                convertKeysToStrings(outputColumns))
+
+            new KijiTap(tableUri, scheme).asInstanceOf[Tap[_, _, _]]
+          }
+        }
+      }
+      case Test(buffers) => {
+        readOrWrite match {
+          // Use Kiji's local tap and scheme when reading.
+          case Read => {
+            val scheme = localKijiScheme
+            populateTestTable(
+                tableUri,
+                buffers(this),
+                scheme.getSourceFields,
+                HBaseConfiguration.create())
+
+            new LocalKijiTap(tableUri, scheme).asInstanceOf[Tap[_, _, _]]
+          }
+
+          // After performing a write, use TestLocalKijiScheme to populate the output buffer.
+          case Write => {
+            val scheme = new TestLocalKijiScheme(
+                buffers(this),
+                timeRange,
+                timestampField,
+                getInputColumnsForTesting,
+                convertKeysToStrings(outputColumns))
+
+            new LocalKijiTap(tableUri, scheme).asInstanceOf[Tap[_, _, _]]
+          }
+        }
+      }
+
+      // Delegate any other tap types to Source's default behaviour.
+      case _ => super.createTap(readOrWrite)(mode)
+    }
+
+    return tap
+  }
+
+ override def toString: String = {
+   Objects
+       .toStringHelper(this)
+       .add("tableAddress", tableAddress)
+       .add("timeRange", timeRange)
+       .add("timestampField", timestampField)
+       .add("inputColumns", inputColumns)
+       .add("outputColumns", outputColumns)
+       .toString
+  }
+
+  override def equals(other: Any): Boolean = {
+    other match {
+      case source: KijiSource => {
+        Objects.equal(tableAddress, source.tableAddress) &&
+        Objects.equal(inputColumns, source.inputColumns) &&
+        Objects.equal(outputColumns, source.outputColumns) &&
+        Objects.equal(timestampField, source.timestampField) &&
+        Objects.equal(timeRange, source.timeRange)
+      }
+      case _ => false
+    }
+  }
+
+  override def hashCode(): Int =
+      Objects.hashCode(tableAddress, inputColumns, outputColumns, timestampField, timeRange)
+}
+
+/**
+ * Contains a private, inner class used by [[org.kiji.express.flow.KijiSource]] when working with
+ * tests.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+private[express] object KijiSource {
+  /**
+   * Convert scala columns definition into its corresponding java variety.
+   *
+   * @param columnMap Mapping from field name to Kiji column name.
+   * @return Java map from field name to column definition.
+   */
+  private[express] def convertKeysToStrings[T <: Any](columnMap: Map[Symbol, T])
+      : Map[String, T] = {
+    columnMap.map { case (symbol, column) => (symbol.name, column) }
+  }
+
+  /**
+   * Takes a buffer containing rows and writes them to the table at the specified uri.
+   *
+   * @param tableUri of the table to populate.
+   * @param rows Tuples to write to populate the table with.
+   * @param fields Field names for elements in the tuple.
+   * @param configuration defining the cluster to use.
+   */
+  private def populateTestTable(
+      tableUri: KijiURI,
+      rows: Buffer[Tuple],
+      fields: Fields,
+      configuration: Configuration) {
+    doAndRelease(Kiji.Factory.open(tableUri)) { kiji: Kiji =>
+      // Layout to get the default reader schemas from.
+      val layout = withKijiTable(tableUri, configuration) { table: KijiTable =>
+        table.getLayout
+      }
+
+      val eidFactory = EntityIdFactory.getFactory(layout)
+
+      // Write the desired rows to the table.
+      withKijiTableWriter(tableUri, configuration) { writer: KijiTableWriter =>
+        rows.foreach { row: Tuple =>
+          val tupleEntry = new TupleEntry(fields, row)
+          val iterator = fields.iterator()
+
+          // Get the entity id field.
+          val entityIdField = iterator.next().toString
+          val entityId = tupleEntry
+            .getObject(entityIdField)
+            .asInstanceOf[EntityId]
+
+          // Iterate through fields in the tuple, adding each one.
+          while (iterator.hasNext) {
+            val field = iterator.next().toString
+
+            // Get the timeline to be written.
+            val cells: Seq[FlowCell[Any]] = tupleEntry
+                .getObject(field)
+                .asInstanceOf[Seq[FlowCell[Any]]]
+
+            // Write the timeline to the table.
+            cells.foreach { cell: FlowCell[Any] =>
+              writer.put(
+                  entityId.toJavaEntityId(eidFactory),
+                  cell.family,
+                  cell.qualifier,
+                  cell.version,
+                  cell.datum
+              )
+            }
+          }
+        }
+      }
+    }
+  }
+
+  private[express] def newGetAllData(col: ColumnInputSpec): ColumnInputSpec = {
+    ColumnInputSpec(
+        col.columnName.toString,
+        Integer.MAX_VALUE,
+        col.filter,
+        col.paging,
+        col.schemaSpec)
+  }
+
+  /**
+   * Returns a map from field name to column input spec where the column input spec has been
+   * configured as an output column.
+   *
+   * This is used in tests, when we use KijiScheme to read tuples from a Kiji table, and we want
+   * to read all data in all of the columns, so the test can inspect all data in the table.
+   *
+   * @param columns to transform.
+   * @return transformed map where the column input specs are configured for output.
+   */
+  private def inputColumnSpecifyAllData(
+      columns: Map[String, ColumnInputSpec]): Map[String, ColumnInputSpec] = {
+    columns.mapValues(newGetAllData)
+        // Need this to make the Map serializable (issue with mapValues)
+        .map(identity)
+  }
+
+  /**
+   * A LocalKijiScheme that loads rows in a table into the provided buffer. This class
+   * should only be used during tests.
+   *
+   * @param buffer to fill with post-job table rows for tests.
+   * @param timeRange of timestamps to read from each column.
+   * @param timestampField is the name of a tuple field that will contain cell timestamp when the
+   *     source is used for writing. Specify the empty field name to write all
+   *     cells at the current time.
+   * @param inputColumns is a map of Scalding field name to ColumnInputSpec.
+   * @param outputColumns is a map of ColumnOutputSpec to Scalding field name.
+   */
+  private class TestLocalKijiScheme(
+      val buffer: Buffer[Tuple],
+      timeRange: TimeRange,
+      timestampField: Option[Symbol],
+      inputColumns: Map[String, ColumnInputSpec],
+      outputColumns: Map[String, ColumnOutputSpec])
+      extends LocalKijiScheme(
+          timeRange,
+          timestampField,
+          inputColumnSpecifyAllData(inputColumns),
+          outputColumns) {
+    override def sinkCleanup(
+        process: FlowProcess[Properties],
+        sinkCall: SinkCall[OutputContext, OutputStream]) {
+      // Store the output table.
+      val conf: JobConf = HadoopUtil
+          .createJobConf(process.getConfigCopy, new JobConf(HBaseConfiguration.create()))
+      val uri: KijiURI = KijiURI
+          .newBuilder(conf.get(KijiConfKeys.KIJI_OUTPUT_TABLE_URI))
+          .build()
+
+      // Read table into buffer.
+      withKijiTable(uri, conf) { table: KijiTable =>
+        val layout = table.getLayout
+
+        // Determine what type of record to use (generic, specific).
+        val cellSpecOverrides: Map[KijiColumnName, CellSpec] = outputColumns
+            .values
+            .map { column => (column.columnName, column.schemaSpec) }
+            .collect {
+              case (name, SchemaSpec.DefaultReader) => {
+                val cellSpec = layout.getCellSpec(name).setUseDefaultReaderSchema()
+                (name, cellSpec)
+              }
+              case (name, SchemaSpec.Writer) => {
+                val cellSpec = layout.getCellSpec(name).setUseWriterSchema()
+                (name, cellSpec)
+              }
+              case (name, SchemaSpec.Generic(avroSchema)) => {
+                val cellSpec = layout.getCellSpec(name).setReaderSchema(avroSchema)
+                (name, cellSpec)
+              }
+              case (name, SchemaSpec.Specific(avroClass)) => {
+                val cellSpec = layout.getCellSpec(name).setSpecificRecord(avroClass)
+                (name, cellSpec)
+              }
+            }
+            .toMap
+
+        // Open a table reader that reads data using the generic api.
+        val readerFactory = table.getReaderFactory
+        doAndClose(readerFactory.openTableReader(cellSpecOverrides.asJava)) { reader =>
+          // We also want the entire timerange, so the test can inspect all data in the table.
+          val request: KijiDataRequest = KijiScheme.buildRequest(All, inputColumns.values)
+
+          doAndClose(reader.getScanner(request)) { scanner: KijiRowScanner =>
+            val rows: Iterator[KijiRowData] = scanner.iterator().asScala
+            rows.foreach { row: KijiRowData =>
+              val tuple = KijiScheme
+                  .rowToTuple(
+                      // Use input columns that are based on the output columns
+                      inputColumns,
+                      getSourceFields,
+                      timestampField,
+                      row,
+                      table.getURI,
+                      conf
+                  )
+
+              val newTupleValues = tuple
+                  .iterator()
+                  .asScala
+                  .map {
+                    // This converts stream into a list to force the stream to compute all of the
+                    // transformations that have been applied lazily to it. This is necessary
+                    // because some of the transformations applied in KijiScheme#rowToTuple have
+                    // dependencies on an open connection to a schema table.
+                    case stream: Stream[_] => stream.toList
+                    case x => x
+                  }
+                  .toSeq
+
+              buffer += new Tuple(newTupleValues: _*)
+            }
+          }
+        }
+      }
+
+      super.sinkCleanup(process, sinkCall)
+    }
+  }
+
+  /**
+   * Merges an input column mapping with an output column mapping producing an input column mapping.
+   * This is used to configure input columns for reading back written data on a source that has just
+   * been used as a sink.
+   *
+   * @param inputs describing which columns to request and what fields to associate them with.
+   * @param outputs describing which columns fields should be output to.
+   * @return a merged mapping from field names to input column requests.
+   */
+  private def mergeColumnMapping(
+      inputs: Map[String, ColumnInputSpec],
+      outputs: Map[String, ColumnOutputSpec]
+  ): Map[String, ColumnInputSpec] = {
+    def mergeEntry(
+        inputs: Map[String, ColumnInputSpec],
+        entry: (String, ColumnOutputSpec)
+    ): Map[String, ColumnInputSpec] = {
+      val (fieldName, columnRequest) = entry
+      val input = ColumnInputSpec(
+          column = columnRequest.columnName.getName,
+          maxVersions = Int.MaxValue,
+          schemaSpec = columnRequest.schemaSpec
+      )
+
+      inputs + ((fieldName, input))
+    }
+
+    outputs
+        .foldLeft(inputs)(mergeEntry)
+  }
+
+  /**
+   * A KijiScheme that loads rows in a table into the provided buffer. This class should only be
+   * used during tests.
+   *
+   * @param timestampField is the name of a tuple field that will contain cell timestamp when the
+   *     source is used for writing. Specify the empty field name to write all cells at the current
+   *     time.
+   * @param inputColumns Scalding field name to column input spec mapping.
+   * @param outputColumns Scalding field name to column output spec mapping.
+   */
+  private class TestKijiScheme(
+      timestampField: Option[Symbol],
+      inputColumns: Map[String, ColumnInputSpec],
+      outputColumns: Map[String, ColumnOutputSpec])
+      extends KijiScheme(
+          All,
+          timestampField,
+          mergeColumnMapping(inputColumns, outputColumns),
+          outputColumns) {
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/PagingSpec.scala b/kiji-express/src/main/scala/org/kiji/express/flow/PagingSpec.scala
new file mode 100644
index 0000000000000000000000000000000000000000..c17c85ed86b2f8bba735352464af8414369f9fe0
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/PagingSpec.scala
@@ -0,0 +1,87 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+
+/**
+ * A specification of the type of paging to use.
+ *
+ * These can be used in [[org.kiji.express.flow.ColumnInputSpec]].  The default in
+ * [[org.kiji.express.flow.ColumnInputSpec]] is [[org.kiji.express.flow.PagingSpec.Off]], which
+ * disables all paging.  With paging disabled, all cells from the specified column will be loaded
+ * into memory at once.  If the size of all of the loaded cells exceeds the capacity of the
+ * receiving machine's main memory, the Scalding job will fail at runtime.  In these cases you can
+ * specify how many cells should be paged into memory at a time with
+ * [[org.kiji.express.flow.PagingSpec.Cells]]:
+ *
+ * {{{
+ *   paging = PagingSpec.Cells(10)
+ * }}}
+ *
+ * This will load only 10 cells at a time into memory.
+ *
+ * The appropriate number of cells to be paged in depends on the size of each cell. Users should
+ * try to retrieve as many cells as possible (without causing OOME) in order to increase
+ * performance.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+sealed trait PagingSpec {
+  private[kiji] def cellsPerPage: Option[Int]
+}
+
+/**
+ * Module to provide PagingSpec implementations.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+object PagingSpec {
+  /**
+   * Specifies that paging should not be used. Each row requested from Kiji tables will be fully
+   * materialized into RAM.
+   */
+  @ApiAudience.Public
+  @ApiStability.Experimental
+  case object Off extends PagingSpec {
+    override val cellsPerPage: Option[Int] = None
+  }
+
+  /**
+   * Specifies that paging should be enabled. Each page will contain the specified number of cells.
+   *
+   * Note: Cells may not all be the same size (in bytes).
+   *
+   * Note: There are known issues with paging in Express.  See https://jira.kiji.org/browse/EXP-326,
+   *    [[org.kiji.express.flow.TransientSeq]] and [[org.kiji.express.flow.TransientSeqSuite]] for
+   *    details and workarounds.
+   *
+   * @param count of the cells per page.
+   */
+  @ApiAudience.Public
+  @ApiStability.Experimental
+  @Inheritance.Sealed
+  final case class Cells(count: Int) extends PagingSpec {
+    override val cellsPerPage: Option[Int] = Some(count)
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/SchemaSpec.scala b/kiji-express/src/main/scala/org/kiji/express/flow/SchemaSpec.scala
new file mode 100644
index 0000000000000000000000000000000000000000..14800d1123248206716c2d039af054c353244d2a
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/SchemaSpec.scala
@@ -0,0 +1,123 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.apache.avro.Schema
+import org.apache.avro.specific.SpecificRecord
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+
+/**
+ * A specification of how to read or write values to a Kiji column.
+ *
+ * An instance of one of the subclasses of SchemaSpec, [[org.kiji.express.flow.SchemaSpec.Generic]],
+ * [[org.kiji.express.flow.SchemaSpec.Specific]],
+ * [[org.kiji.express.flow.SchemaSpec.DefaultReader]], or
+ * [[org.kiji.express.flow.SchemaSpec.Writer]], can be used as an optional parameter to
+ * [[org.kiji.express.flow.ColumnFamilyInputSpec]],
+ * [[org.kiji.express.flow.QualifiedColumnInputSpec]],
+ * [[org.kiji.express.flow.ColumnFamilyOutputSpec]], and
+ * [[org.kiji.express.flow.QualifiedColumnOutputSpec]].
+ *
+ * These classes specify the Avro schema to read the data in a column with, or the Avro schema to
+ * write the data to a column with.  Here are the possible subclasses you may use in your
+ * `ColumnInputSpec` or `ColumnOutputSpec`:
+ * <ul>
+ *   <li>`SchemaSpec.Specific(classOf[MySpecificRecordClass])`: This option should be used when you
+ *   have a specific class that has been compiled by Avro.  `MySpecificRecordClass` must extend
+ *   `org.apache.avro.SpecificRecord`</li>
+ *   <li>`SchemaSpec.Generic(myGenericSchema)`: If you don’t have the specific class you want to use
+ *   to read or write on the classpath, you can construct a generic schema and use it as the reader
+ *   schema.</li>
+ *   <li>`SchemaSpec.Writer`: used when you want to read with the same schema that the data
+ *   was written with, or a schema attached to or inferred from the value to write with.  This is
+ *   the default if you don’t specify any `SchemaSpec` for reading or writing.</li>
+ *   <li>`SchemaSpec.DefaultReader`: specifies that the default reader for this column, stored in
+ *   the table layout, should be used for reading or writing this data.  If you use this option,
+ *   first make sure the column in your Kiji table has a default reader specified.</li>
+ * </ul>
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+sealed trait SchemaSpec extends java.io.Serializable {
+  /**
+   * Retrieve the Avro [[org.apache.avro.Schema]] object associated with this SchemaSpec,
+   * if possible.
+   */
+  private[kiji] def schema: Option[Schema]
+}
+
+/**
+ * Module to provide SchemaSpec implementations.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+object SchemaSpec {
+  /**
+   * Specifies reading or writing with the supplied [[org.apache.avro.Schema]].
+   *
+   * @param genericSchema of data
+   */
+  @ApiAudience.Public
+  @ApiStability.Experimental
+  @Inheritance.Sealed
+  final case class Generic(genericSchema: Schema) extends SchemaSpec {
+    override val schema: Option[Schema] = Some(genericSchema)
+  }
+
+  /**
+   * A specification for reading or writing as an instance of the supplied Avro specific record.
+   *
+   * @param klass of the specific record.
+   */
+  @ApiAudience.Public
+  @ApiStability.Experimental
+  @Inheritance.Sealed
+  final case class Specific(klass: Class[_ <: SpecificRecord]) extends SchemaSpec {
+    override val schema: Option[Schema] = Some(klass.newInstance.getSchema)
+  }
+
+  /**
+   * Use the writer schema associated with a value to read or write.
+   *
+   * In the case of reading a value, the writer schema used to serialize the value will be used.
+   * In the case of writing a value, the schema attached to or inferred from the value will be used.
+   */
+  @ApiAudience.Public
+  @ApiStability.Experimental
+  case object Writer extends SchemaSpec {
+    override val schema: Option[Schema] = None
+  }
+
+  /**
+   * Specifies that the default reader for this column, stored in the table layout, should be used
+   * for reading or writing this data.  If you use this option, first make sure the column in your
+   * Kiji table has a default reader specified.
+   */
+  @ApiAudience.Public
+  @ApiStability.Experimental
+  case object DefaultReader extends SchemaSpec {
+    override val schema: Option[Schema] = None
+  }
+}
+
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/TimeRange.scala b/kiji-express/src/main/scala/org/kiji/express/flow/TimeRange.scala
new file mode 100644
index 0000000000000000000000000000000000000000..6c60a5ffa81a7ef4e8dbfef7058db7c1e9a34bb7
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/TimeRange.scala
@@ -0,0 +1,139 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.KConstants
+
+/**
+ * A trait implemented by classes that specify time ranges when reading data from Kiji tables. This
+ * class is used to specify the range of cell versions to request from a column in a Kiji table.
+ *
+ * To specify that [[org.kiji.express.flow.All]] versions should be requested:
+ * {{{
+ *   val timeRange: TimeRange = All
+ * }}}
+ * To specify that a specific version should be requested ([[org.kiji.express.flow.At]]):
+ * {{{
+ *   // Gets only cells with the version `123456789`.
+ *   val timeRange: TimeRange = At(123456789L)
+ * }}}
+ * To specify that all versions [[org.kiji.express.flow.After]] the specified version should be
+ * requested:
+ * {{{
+ *   // Gets only cells with versions larger than `123456789`.
+ *   val timeRange: TimeRange = After(123456789L)
+ * }}}
+ * To specify that all versions [[org.kiji.express.flow.Before]] the specified version should be
+ * requested:
+ * {{{
+ *   // Gets only cells with versions smaller than `123456789`.
+ *   val timeRange: TimeRange = Before(123456789L)
+ * }}}
+ * To specify that all versions [[org.kiji.express.flow.Between]] the two specified bounds should be
+ * requested:
+ * {{{
+ *   // Gets only cells with versions between `12345678` and `123456789`.
+ *   val timeRange: TimeRange = Between(12345678, 123456789)
+ * }}}
+ *
+ * See [[org.kiji.express.flow.KijiInput]] for more information.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+sealed trait TimeRange extends Serializable {
+  /** Earliest version of the TimeRange, inclusive. */
+  def begin: Long
+
+  /** Latest version of the TimeRange, exclusive. */
+  def end: Long
+}
+
+/**
+ * Implementation of [[org.kiji.express.flow.TimeRange]] for specifying that all versions should be
+ * requested.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+case object All extends TimeRange {
+  override val begin: Long = KConstants.BEGINNING_OF_TIME
+  override val end: Long = KConstants.END_OF_TIME
+}
+
+/**
+ * Implementation of [[org.kiji.express.flow.TimeRange]] for specifying that only the provided
+ * version should be requested.
+ *
+ * @param version to request.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class At(version: Long) extends TimeRange {
+  override val begin: Long = version
+  override val end: Long = version
+}
+
+/**
+ * Implementation of [[org.kiji.express.flow.TimeRange]] for specifying that all versions after the
+ * provided version should be requested (exclusive).
+ *
+ * @param begin is the earliest version that should be requested (exclusive).
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class After(override val begin: Long) extends TimeRange {
+  override val end: Long = KConstants.END_OF_TIME
+}
+
+/**
+ * Implementation of [[org.kiji.express.flow.TimeRange]] for specifying that all versions before the
+ * provided version should be requested (inclusive).
+ *
+ * @param end is the latest version that should be requested (inclusive).
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class Before(override val end: Long) extends TimeRange {
+  override val begin: Long = KConstants.BEGINNING_OF_TIME
+}
+
+/**
+ * Implementation of [[org.kiji.express.flow.TimeRange]] for specifying that all versions between
+ * the provided begin and end versions should be requested.
+ *
+ * @param begin is the earliest version that should be requested (inclusive).
+ * @param end is the latest version that should be requested (exclusive).
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+final case class Between(
+    override val begin: Long,
+    override val end: Long) extends TimeRange {
+  // Ensure that the timerange bounds are sensible.
+  require(begin <= end, "Invalid time range specified: (%d, %d)".format(begin, end))
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/TransientSeq.scala b/kiji-express/src/main/scala/org/kiji/express/flow/TransientSeq.scala
new file mode 100644
index 0000000000000000000000000000000000000000..32061fd6045ce18c2944001e56e4b03de2b445f5
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/TransientSeq.scala
@@ -0,0 +1,88 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import scala.collection.SeqView
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+
+/**
+ * `TransientSeq` is a special case of [[scala.collection.Seq]] meant to be backed by a source which
+ * is potentially larger than memory or mutable. The `TransientSeq` constructor takes an iterator
+ * generator- a no arg function which returns an iterator over the backing collection.
+ * `TransientSeq` is a [[scala.collection.SeqView]], and thus it applies all transformations lazily.
+ * Furthermore, when `force` is called on a `TransientSeq`, an iterator is created, and a lazily
+ * evaluated Stream is returned. Operations which truly force a `TransientSeq` into evaluation
+ * (and thus creating and using an iterator on the backing collection), include `head`, `tail`,
+ * `foreach`, `apply`, `length`, `size`, `contains`, `diff`, `union`, `intersect`, and `sorted`.
+ *
+ * From a user's perspective, a `TransientSeq` is not mutable, but the underlying collection may be
+ * mutable. [[scala.collection.mutable]] mutator methods are not provided, thus users cannot mutate
+ * a `TransientSeq`. If the collection underlying the iterator generator is immutable (that is, the
+ * iterator generator always returns iterators containing the same elements in the same order), then
+ * the `TransientSeq` can be treated as immutable.  If the backing collection is not immutable, then
+ * `TransientSeq` will not be immutable, because it will not guarantee referential transparency.
+ *
+ * It is important when using a `TransientSeq` to limit the calls to operations which create and use
+ * an iterator on the backing collection.  For instance:
+ * {{
+ *    val tseq = new TransientSeq(...)
+ *
+ *    // Slow! creates 3 separate iterators over the backing collection
+ *    val first = tseq(0)
+ *    val second = tseq(1)
+ *    val third = tseq(2)
+ *
+ *    // Better, only creates a single iterator over the backing collection
+ *    val elements = tseq.take(3).toList // forces first 3 elements of transient representation
+ *                                       // to in-memory list
+ *    val first = elements(0)
+ *    val second = elements(1)
+ *    val third = elements(2)
+ * }}
+ *
+ * Be careful when using `toList` to force the `TransientSeq` to an in-memory list.  This could
+ * cause an [[java.lang.OutOfMemoryError]] if the items in the `TransientSeq` do not fit in the
+ * VM's heap space. In this case, use filters, aggregation, take, or drop to reduce the number of
+ * elements.
+ *
+ * Additionally, `TransientSeq`s should not be passed to recursive methods, instead, `force` the
+ * `TransientSeq` into a `Stream` and then pass it to the recursive method, otherwise a new
+ * iterator will be created for every recursive call to `tail`.
+ *
+ * @param genItr function produces a new iterator of the underlying collection of elements.
+ * @tparam T type of contained elements.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+class TransientSeq[+T](genItr: () => Iterator[T]) extends SeqView[T, Stream[T]] {
+  require(genItr != null)
+
+  override def length: Int = iterator.size
+
+  override def apply(idx: Int): T = iterator.drop(idx).next()
+
+  override def iterator: Iterator[T] = genItr()
+
+  protected val underlying: Stream[T] = Stream()
+
+  override def toString: String = "TransientSeq(...)"
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiInputFormat.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiInputFormat.scala
new file mode 100644
index 0000000000000000000000000000000000000000..c2f5d5ad246d3af999507e7af1010f134c6331c7
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiInputFormat.scala
@@ -0,0 +1,131 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import scala.collection.JavaConverters._
+
+import com.google.common.base.Preconditions.checkNotNull
+import org.apache.hadoop.hbase.mapreduce.TableSplit
+import org.apache.hadoop.mapred.InputFormat
+import org.apache.hadoop.mapred.InputSplit
+import org.apache.hadoop.mapred.JobConf
+import org.apache.hadoop.mapred.RecordReader
+import org.apache.hadoop.mapred.Reporter
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.util.Resources.doAndClose
+import org.kiji.express.flow.util.Resources.doAndRelease
+import org.kiji.mapreduce.framework.KijiConfKeys
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiRegion
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiURI
+import org.kiji.schema.impl.HBaseKijiTable
+
+/**
+ * Tells the MapReduce framework how to divide a Kiji table into input splits for tasks,
+ * and how to read records from those splits.
+ *
+ * MapReduce views a data set as a collection of key-value pairs divided into input splits,
+ * where each input split is processed by a MapReduce task. This input format divides a Kiji
+ * table into one input split per HBase region in the table. It also provides access to a record
+ * reader (specifically [[org.kiji.express.flow.framework.KijiRecordReader]]) which knows how to
+ * read rows from a Kiji table as key-value pairs.
+ *
+ * A MapReduce job reading from a Kiji table as part of the KijiExpress framework should be
+ * configured with this input format. The job using this input format should have a configuration
+ * containing a serialized `KijiDataRequest` at the key `kiji.input.data.request` and a Kiji URI
+ * addressing the target table at the key `kiji.input.table.uri`.
+ *
+ * The Kiji framework already has an input format for reading from Kiji tables,
+ * but it is written against a newer MapReduce API than the one supported by Cascading. This
+ * input format exists to address this compatibility issue.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+@Inheritance.Sealed
+final class KijiInputFormat
+    extends InputFormat[KijiKey, KijiValue] {
+  /**
+   * Creates one input split per HBase region of a Kiji table.
+   *
+   * @param configuration containing a Kiji URI at the key `kiji.input.table.uri` that addresses
+   *     the table to generate splits for.
+   * @param numSplits is a MapReduce framework hint for the number of splits to produce,
+   *     and is ignored here.
+   * @return one input split per HBase region of the Kiji table.
+   */
+  override def getSplits(configuration: JobConf, numSplits: Int): Array[InputSplit] = {
+    val uriString: String = checkNotNull(configuration.get(KijiConfKeys.KIJI_INPUT_TABLE_URI))
+    val inputTableURI: KijiURI = KijiURI.newBuilder(uriString).build()
+
+    doAndRelease(Kiji.Factory.open(inputTableURI, configuration)) { kiji: Kiji =>
+      doAndRelease(kiji.openTable(inputTableURI.getTable())) { table: KijiTable =>
+        doAndClose(HBaseKijiTable.downcast(table).openHTableConnection()) { htable =>
+          table.getRegions().asScala.map { region: KijiRegion =>
+            val startKey: Array[Byte] = region.getStartKey()
+            // TODO(KIJIMR-65): For now pick the first available location (ie. region server),
+            //     if any.
+            val location: String = {
+              if (region.getLocations().isEmpty()) {
+                null
+              } else {
+                region.getLocations().iterator().next()
+              }
+            }
+            val tableSplit: TableSplit = {
+              new TableSplit(
+                  htable.getTableName(),
+                  startKey,
+                  region.getEndKey(),
+                  location)
+            }
+            new KijiTableSplit(tableSplit)
+          }
+          .toArray
+        }
+      }
+    }
+  }
+
+  /**
+   * Creates a record reader that will read rows from a region of a Kiji table as key-value pairs.
+   *
+   * @param split identifies the HBase region of the Kiji table that should be read.
+   * @param configuration containing a serialized data request at the key
+   *     `kiji.input.data.request` which will be used to configure how data will be read from
+   *     HBase.
+   * @param reporter is provided by the MapReduce framework as a means for tasks to report status,
+   *     and is ignored here.
+   * @return a record reader that will read rows from a region of a Kiji table as key-value pairs.
+   */
+  override def getRecordReader(
+      split: InputSplit,
+      configuration: JobConf,
+      reporter: Reporter): RecordReader[KijiKey, KijiValue] = {
+    split match {
+      // TODO: Use reporter to report progress.
+      case kijiSplit: KijiTableSplit => new KijiRecordReader(kijiSplit, configuration)
+      case _ => sys.error("KijiInputFormat requires a KijiTableSplit.")
+    }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiKey.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiKey.scala
new file mode 100644
index 0000000000000000000000000000000000000000..978b177750571f5cc26b399c97a62451150e8fbf
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiKey.scala
@@ -0,0 +1,59 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.{EntityId => JEntityId}
+
+/**
+ * A reusable container for [[org.kiji.schema.EntityId]]s.
+ *
+ * The MapReduce framework views a data set as a collection of key-value pairs,
+ * and likes to read those pairs into a reusable instance of the key or value class. When a row
+ * is read from a Kiji table, its [[org.kiji.schema.EntityId]] is emitted as the key, and
+ * [[org.kiji.schema.KijiRowData]] is emitted as the value. Because instances of
+ * [[org.kiji.schema.EntityId]] are not reusable, this class is provided to give the MapReduce
+ * framework a reusable container.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+final class KijiKey {
+  /** The entity id contained by this instance. */
+  private var currentKey: JEntityId = null
+
+  /**
+   *  Retrieves the [[org.kiji.schema.EntityId]] wrapped by this instance.
+   *
+   * @return the entity id contained in this instance.
+   */
+  def get(): JEntityId = currentKey
+
+  /**
+   * Sets the [[org.kiji.schema.EntityId]] contained in this instance.
+   *
+   * @param key that will be wrapped by this instance.
+   */
+  def set(key: JEntityId) {
+    currentKey = key
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiRecordReader.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiRecordReader.scala
new file mode 100644
index 0000000000000000000000000000000000000000..721b8aa2dd1efa34f9f08bd9698d95ab8d101dae
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiRecordReader.scala
@@ -0,0 +1,203 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import org.apache.commons.codec.binary.Base64
+import org.apache.commons.lang.SerializationUtils
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.hbase.util.Bytes
+import org.apache.hadoop.mapred.RecordReader
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.util.GenericCellSpecs
+import org.kiji.express.flow.util.SpecificCellSpecs
+import org.kiji.express.flow.util.Resources.doAndRelease
+import org.kiji.mapreduce.framework.KijiConfKeys
+import org.kiji.schema.HBaseEntityId
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiDataRequest
+import org.kiji.schema.KijiRowData
+import org.kiji.schema.KijiRowScanner
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiTableReader
+import org.kiji.schema.KijiTableReader.KijiScannerOptions
+import org.kiji.schema.KijiURI
+import org.kiji.schema.hbase.HBaseScanOptions
+
+/**
+ * Reads rows from an HBase region of a Kiji table as key-value pairs that can be used with the
+ * MapReduce framework.
+ *
+ * MapReduce views a data set as a collection of key-value pairs divided into input splits,
+ * where each input split is processed by a MapReduce task. This record reader can scan a subset
+ * of rows from a Kiji table (really an HBase region of the Kiji table identified by an input
+ * split) and transform them into key-value pairs that can be processed by the MapReduce framework.
+ *
+ * Key-value pairs are obtained from Kiji rows as follows. For each row read,
+ * the row's entity id is used as key (by populating an instance of
+ * [[org.kiji.express.flow.framework.KijiKey]]) and the row data itself is used as value (by
+ * populating an instance of [[org.kiji.express.flow.framework.KijiValue]]). The classes
+ * [[org.kiji.express.flow.framework.KijiKey]] and [[org.kiji.express.flow.framework.KijiValue]]
+ * are simply reusable containers wrapping entity ids and row data.
+ *
+ * A MapReduce job reading from a Kiji table as part of the KijiExpress framework should be
+ * configured to use [[org.kiji.express.flow.framework.KijiInputFormat]],
+ * which allow the job to use this record reader. The job using this record reader should have a
+ * configuration containing a serialized `KijiDataRequest` at the key `kiji.input.data.request`
+ * and a Kiji URI addressing the target table at the key `kiji.input.table.uri`.
+ *
+ * @param split identifying the HBase region of a Kiji table whose rows will be scanned.
+ * @param configuration containing the Kiji URI of the target Kiji table, and a serialized data
+ *     request.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+@Inheritance.Sealed
+final class KijiRecordReader(
+    private val split: KijiTableSplit,
+    private val configuration: Configuration)
+    extends RecordReader[KijiKey, KijiValue] {
+  if (!split.isInstanceOf[KijiTableSplit]) {
+    sys.error("KijiRecordReader received an InputSplit that was not a KijiTableSplit.")
+  }
+
+  /** The data request used to read from the Kiji table. */
+  private val dataRequest: KijiDataRequest = {
+    // Get data request from the job configuration.
+    val dataRequestB64: String = {
+      Option(configuration.get(KijiConfKeys.KIJI_INPUT_DATA_REQUEST)) match {
+        case Some(str) => str
+        case None => sys.error("Missing data request in job configuration.")
+      }
+    }
+
+    val dataRequestBytes: Array[Byte] = Base64.decodeBase64(Bytes.toBytes(dataRequestB64))
+    SerializationUtils.deserialize(dataRequestBytes).asInstanceOf[KijiDataRequest]
+  }
+
+  /** A Kiji URI addressing the target table. */
+  private val inputURI: KijiURI = KijiURI
+      .newBuilder(configuration.get(KijiConfKeys.KIJI_INPUT_TABLE_URI))
+      .build()
+
+  /** A reader for the above table. */
+  private val reader: KijiTableReader = {
+    doAndRelease(Kiji.Factory.open(inputURI, configuration)) { kiji: Kiji =>
+      doAndRelease(kiji.openTable(inputURI.getTable())) { table: KijiTable =>
+        val serializedOverrides: String =
+            configuration.get(SpecificCellSpecs.CELLSPEC_OVERRIDE_CONF_KEY)
+        val cellSpecOverrides = SpecificCellSpecs.deserializeOverrides(table, serializedOverrides)
+        val completeCellSpecs =
+            SpecificCellSpecs.mergeCellSpecs(GenericCellSpecs(table), cellSpecOverrides)
+        table.getReaderFactory.openTableReader(completeCellSpecs)
+      }
+    }
+  }
+  /** Used to scan a subset of rows from the table. */
+  private val scanner: KijiRowScanner = {
+    val hbaseScannerOptions: HBaseScanOptions = new HBaseScanOptions()
+    hbaseScannerOptions.setCacheBlocks(false)
+    val scannerOptions: KijiScannerOptions = new KijiScannerOptions()
+        .setStartRow(HBaseEntityId.fromHBaseRowKey(split.getStartRow()))
+        .setStopRow(HBaseEntityId.fromHBaseRowKey(split.getEndRow()))
+        .setHBaseScanOptions(hbaseScannerOptions)
+
+    reader.getScanner(dataRequest, scannerOptions)
+  }
+  /** An iterator over the rows retrieved by the scanner. */
+  private val iterator: java.util.Iterator[KijiRowData] = scanner.iterator()
+
+  /** This prevents errors when KijiRecordReader is closed multiple times. See CHOP-56. */
+  private var isClosed: Boolean = false
+
+  /**
+   * Gets a key instance that can be populated with entity ids scanned from the table.
+   *
+   * @return a new, empty, reusable key instance that will hold entity ids scanned by this record
+   *   reader. Note that until populated, the key instance will return `null` if you attempt to
+   *   retrieve the entity id from the key.
+   */
+  override def createKey(): KijiKey = new KijiKey()
+
+  /**
+   * Gets a value instance that can be populated with row data scanned from the table.
+   *
+   * @return a new, empty, reusable value instance that will hold row data scanned by this record
+   *     reader. Note that until populated, the value instance will return `null` if you attempt
+   *     to retrieve the row data from the value.
+   */
+  override def createValue(): KijiValue = new KijiValue()
+
+  /**
+   * @return `OL` always, because it's impossible to tell how much we've read through
+   *     a particular key range, because we have no knowledge of how many rows are actually in
+   *     the range.
+   */
+  override def getPos(): Long = 0L
+
+  /**
+   * @return `0.0` always, because it's impossible to tell how much we've read through
+   *     a particular key range, because we have no knowledge of how many rows are actually in
+   *     the range.
+   */
+  override def getProgress(): Float = 0.0f
+
+  /**
+   * Scans the next row from the region of the Kiji table, and populates a key and value with the
+   * entity id and row data obtained.
+   *
+   * @param key instance to populate with the next entity id read.
+   * @param value instance to populate with the next row data read.
+   * @return `true` if a new row was scanned and the key and value populated, `false` if the end
+   *     of the region has been reached.
+   */
+  override def next(key: KijiKey, value: KijiValue): Boolean = {
+    if (iterator.hasNext()) {
+      // Read the next row and store it in the provided key/value pair.
+      val row: KijiRowData = iterator.next()
+      if (null != key) {
+        key.set(row.getEntityId())
+      }
+      if (null != value) {
+        value.set(row)
+      }
+      true
+    } else {
+      false
+    }
+  }
+
+  /**
+   * Closes the table scanner and table reader used by this instance to scan a region of a Kiji
+   * table.
+   *
+   * It is safe (but unnecessary) to call this method multiple times.
+   */
+  override def close() {
+    if (!isClosed) {
+      isClosed = true
+
+      scanner.close()
+      reader.close()
+    }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiScheme.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiScheme.scala
new file mode 100644
index 0000000000000000000000000000000000000000..9ef49e022d716577c0fc20e6e4504ff3c7974207
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiScheme.scala
@@ -0,0 +1,619 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import scala.collection.JavaConverters.asScalaIteratorConverter
+
+import cascading.flow.FlowProcess
+import cascading.scheme.Scheme
+import cascading.scheme.SinkCall
+import cascading.scheme.SourceCall
+import cascading.tap.Tap
+import cascading.tuple.Fields
+import cascading.tuple.Tuple
+import cascading.tuple.TupleEntry
+import com.google.common.base.Objects
+import org.apache.avro.Schema
+import org.apache.commons.codec.binary.Base64
+import org.apache.commons.lang.SerializationUtils
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.hbase.HConstants
+import org.apache.hadoop.mapred.JobConf
+import org.apache.hadoop.mapred.OutputCollector
+import org.apache.hadoop.mapred.RecordReader
+import org.slf4j.Logger
+import org.slf4j.LoggerFactory
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.express.flow.ColumnFamilyInputSpec
+import org.kiji.express.flow.ColumnFamilyOutputSpec
+import org.kiji.express.flow.ColumnInputSpec
+import org.kiji.express.flow.ColumnOutputSpec
+import org.kiji.express.flow.EntityId
+import org.kiji.express.flow.FlowCell
+import org.kiji.express.flow.PagingSpec
+import org.kiji.express.flow.QualifiedColumnInputSpec
+import org.kiji.express.flow.QualifiedColumnOutputSpec
+import org.kiji.express.flow.TimeRange
+import org.kiji.express.flow.TransientSeq
+import org.kiji.express.flow.framework.serialization.KijiLocker
+import org.kiji.express.flow.util.AvroUtil
+import org.kiji.express.flow.util.Resources.doAndRelease
+import org.kiji.express.flow.util.SpecificCellSpecs
+import org.kiji.mapreduce.framework.KijiConfKeys
+import org.kiji.schema.ColumnVersionIterator
+import org.kiji.schema.EntityIdFactory
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiCell
+import org.kiji.schema.KijiDataRequest
+import org.kiji.schema.KijiDataRequestBuilder
+import org.kiji.schema.KijiRowData
+import org.kiji.schema.KijiSchemaTable
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiTableWriter
+import org.kiji.schema.KijiURI
+import org.kiji.schema.MapFamilyVersionIterator
+import org.kiji.schema.avro.AvroSchema
+import org.kiji.schema.filter.KijiColumnFilter
+import org.kiji.schema.layout.KijiTableLayout
+
+/**
+ * A Kiji-specific implementation of a Cascading `Scheme`, which defines how to read and write the
+ * data stored in a Kiji table.
+ *
+ * KijiScheme is responsible for converting rows from a Kiji table that are input to a Cascading
+ * flow into Cascading tuples
+ * (see `source(cascading.flow.FlowProcess, cascading.scheme.SourceCall)`) and writing output
+ * data from a Cascading flow to a Kiji table
+ * (see `sink(cascading.flow.FlowProcess, cascading.scheme.SinkCall)`).
+ *
+ * KijiScheme must be used with [[org.kiji.express.flow.framework.KijiTap]],
+ * since it expects the Tap to have access to a Kiji table.  [[org.kiji.express.flow.KijiSource]]
+ * handles the creation of both KijiScheme and KijiTap in KijiExpress.
+ *
+ * @param timeRange to include from the Kiji table.
+ * @param timestampField is the optional name of a field containing the timestamp that all values
+ *     in a tuple should be written to.
+ *     Use None if all values should be written at the current time.
+ * @param inputColumns mapping tuple field names to requests for Kiji columns.
+ * @param outputColumns mapping tuple field names to specifications for Kiji columns to write out
+ *     to.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+class KijiScheme(
+    private[express] val timeRange: TimeRange,
+    private[express] val timestampField: Option[Symbol],
+    @transient private[express] val inputColumns: Map[String, ColumnInputSpec] = Map(),
+    @transient private[express] val outputColumns: Map[String, ColumnOutputSpec] = Map())
+    extends Scheme[JobConf, RecordReader[KijiKey, KijiValue], OutputCollector[_, _],
+        KijiSourceContext, KijiSinkContext] {
+  import KijiScheme._
+
+  // ColumnInputSpec and ColumnOutputSpec objects cannot be correctly serialized via
+  // java.io.Serializable.  Chiefly, Avro objects including Schema and all of the Generic types
+  // are not Serializable.  By making the inputColumns and outputColumns transient and wrapping
+  // them in KijiLocker objects (which handle serialization correctly),
+  // we can work around this limitation.  Thus, the following two lines should be the only to
+  // reference `inputColumns` and `outputColumns`, because they will be null after serialization.
+  // Everything else should instead use _inputColumns.get and _outputColumns.get.
+  private val _inputColumns = KijiLocker(inputColumns)
+  private val _outputColumns = KijiLocker(outputColumns)
+
+  // Including output column keys here because we might need to read back outputs during test
+  // TODO (EXP-250): Ideally we should include outputColumns.keys here only during tests.
+  setSourceFields(buildSourceFields(_inputColumns.get.keys ++ _outputColumns.get.keys))
+  setSinkFields(buildSinkFields(_outputColumns.get, timestampField))
+
+  /**
+   * Sets any configuration options that are required for running a MapReduce job
+   * that reads from a Kiji table. This method gets called on the client machine
+   * during job setup.
+   *
+   * @param flow being built.
+   * @param tap that is being used with this scheme.
+   * @param conf to which we will add our KijiDataRequest.
+   */
+  override def sourceConfInit(
+      flow: FlowProcess[JobConf],
+      tap: Tap[JobConf, RecordReader[KijiKey, KijiValue], OutputCollector[_, _]],
+      conf: JobConf) {
+    // Build a data request.
+    val request: KijiDataRequest = buildRequest(timeRange, _inputColumns.get.values)
+
+    // Write all the required values to the job's configuration object.
+    conf.setInputFormat(classOf[KijiInputFormat])
+    conf.set(
+        KijiConfKeys.KIJI_INPUT_DATA_REQUEST,
+        Base64.encodeBase64String(SerializationUtils.serialize(request)))
+    conf.set(SpecificCellSpecs.CELLSPEC_OVERRIDE_CONF_KEY,
+        SpecificCellSpecs.serializeOverrides(_inputColumns.get))
+  }
+
+  /**
+   * Sets up any resources required for the MapReduce job. This method is called
+   * on the cluster.
+   *
+   * @param flow is the current Cascading flow being run.
+   * @param sourceCall containing the context for this source.
+   */
+  override def sourcePrepare(
+      flow: FlowProcess[JobConf],
+      sourceCall: SourceCall[KijiSourceContext, RecordReader[KijiKey, KijiValue]]) {
+    val tableUriProperty = flow.getStringProperty(KijiConfKeys.KIJI_INPUT_TABLE_URI)
+    val uri: KijiURI = KijiURI.newBuilder(tableUriProperty).build()
+
+    // Set the context used when reading data from the source.
+    sourceCall.setContext(KijiSourceContext(
+        sourceCall.getInput.createValue(),
+        uri))
+  }
+
+  /**
+   * Reads and converts a row from a Kiji table to a Cascading Tuple. This method
+   * is called once for each row on the cluster.
+   *
+   * @param flow is the current Cascading flow being run.
+   * @param sourceCall containing the context for this source.
+   * @return `true` if another row was read and it was converted to a tuple,
+   *     `false` if there were no more rows to read.
+   */
+  override def source(
+      flow: FlowProcess[JobConf],
+      sourceCall: SourceCall[KijiSourceContext, RecordReader[KijiKey, KijiValue]]): Boolean = {
+    // Get the current key/value pair.
+    val KijiSourceContext(value, tableUri) = sourceCall.getContext
+
+    // Get the next row.
+    if (sourceCall.getInput.next(null, value)) {
+      val row: KijiRowData = value.get()
+
+      // Build a tuple from this row.
+      val result: Tuple = rowToTuple(
+          _inputColumns.get,
+          getSourceFields,
+          timestampField,
+          row,
+          tableUri,
+          flow.getConfigCopy
+      )
+
+      // If no fields were missing, set the result tuple and return from this method.
+      sourceCall.getIncomingEntry.setTuple(result)
+      flow.increment(counterGroupName, counterSuccess, 1)
+
+      // We set a result tuple, return true for success.
+      return true
+    } else {
+      return false // We reached the end of the RecordReader.
+    }
+  }
+
+  /**
+   * Cleans up any resources used during the MapReduce job. This method is called
+   * on the cluster.
+   *
+   * @param flow currently being run.
+   * @param sourceCall containing the context for this source.
+   */
+  override def sourceCleanup(
+      flow: FlowProcess[JobConf],
+      sourceCall: SourceCall[KijiSourceContext, RecordReader[KijiKey, KijiValue]]) {
+    sourceCall.setContext(null)
+  }
+
+  /**
+   * Sets any configuration options that are required for running a MapReduce job
+   * that writes to a Kiji table. This method gets called on the client machine
+   * during job setup.
+   *
+   * @param flow being built.
+   * @param tap that is being used with this scheme.
+   * @param conf to which we will add our KijiDataRequest.
+   */
+  override def sinkConfInit(
+      flow: FlowProcess[JobConf],
+      tap: Tap[JobConf, RecordReader[KijiKey, KijiValue], OutputCollector[_, _]],
+      conf: JobConf) {
+    // No-op since no configuration parameters need to be set to encode data for Kiji.
+  }
+
+  /**
+   * Sets up any resources required for the MapReduce job. This method is called
+   * on the cluster.
+   *
+   * @param flow is the current Cascading flow being run.
+   * @param sinkCall containing the context for this source.
+   */
+  override def sinkPrepare(
+      flow: FlowProcess[JobConf],
+      sinkCall: SinkCall[KijiSinkContext, OutputCollector[_, _]]) {
+    // Open a table writer.
+    val uriString: String = flow.getConfigCopy.get(KijiConfKeys.KIJI_OUTPUT_TABLE_URI)
+    val uri: KijiURI = KijiURI.newBuilder(uriString).build()
+
+    val kiji: Kiji = Kiji.Factory.open(uri, flow.getConfigCopy)
+    doAndRelease(kiji.openTable(uri.getTable)) { table: KijiTable =>
+      // Set the sink context to an opened KijiTableWriter.
+      sinkCall.setContext(
+          KijiSinkContext(table.openTableWriter(), uri, kiji, table.getLayout))
+    }
+  }
+
+  /**
+   * Converts and writes a Cascading Tuple to a Kiji table. This method is called once
+   * for each row on the cluster.
+   *
+   * @param flow is the current Cascading flow being run.
+   * @param sinkCall containing the context for this source.
+   */
+  override def sink(
+      flow: FlowProcess[JobConf],
+      sinkCall: SinkCall[KijiSinkContext, OutputCollector[_, _]]) {
+    // Retrieve writer from the scheme's context.
+    val KijiSinkContext(writer, tableUri, kiji, layout) = sinkCall.getContext
+
+    // Write the tuple out.
+    val output: TupleEntry = sinkCall.getOutgoingEntry
+    putTuple(
+        _outputColumns.get,
+        tableUri,
+        kiji,
+        timestampField,
+        output,
+        writer,
+        layout,
+        flow.getConfigCopy)
+  }
+
+  /**
+   * Cleans up any resources used during the MapReduce job. This method is called
+   * on the cluster.
+   *
+   * @param flow is the current Cascading flow being run.
+   * @param sinkCall containing the context for this source.
+   */
+  override def sinkCleanup(
+      flow: FlowProcess[JobConf],
+      sinkCall: SinkCall[KijiSinkContext, OutputCollector[_, _]]) {
+    sinkCall.getContext.kiji.release()
+    sinkCall.getContext.kijiTableWriter.close()
+    sinkCall.setContext(null)
+  }
+
+  override def equals(other: Any): Boolean = {
+    other match {
+      case scheme: KijiScheme => {
+        _inputColumns.get == scheme._inputColumns.get &&
+            _outputColumns.get == scheme._outputColumns.get &&
+            timestampField == scheme.timestampField &&
+            timeRange == scheme.timeRange
+      }
+      case _ => false
+    }
+  }
+
+  override def hashCode(): Int = Objects.hashCode(
+      _inputColumns.get,
+      _outputColumns.get,
+      timeRange,
+      timestampField)
+}
+
+/**
+ * Companion object for KijiScheme.
+ *
+ * Contains constants and helper methods for converting between Kiji rows and Cascading tuples,
+ * building Kiji data requests, and some utility methods for handling Cascading fields.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+object KijiScheme {
+  type HadoopScheme = Scheme[JobConf, RecordReader[_, _], OutputCollector[_, _], _, _]
+
+  private val logger: Logger = LoggerFactory.getLogger(classOf[KijiScheme])
+
+  /** Hadoop mapred counter group for KijiExpress. */
+  private[express] val counterGroupName = "kiji-express"
+  /** Counter name for the number of rows successfully read. */
+  private[express] val counterSuccess = "ROWS_SUCCESSFULLY_READ"
+  /** Field name containing a row's [[org.kiji.schema.EntityId]]. */
+  val entityIdField: String = "entityId"
+  /** Default number of qualifiers to retrieve when paging in a map type family.*/
+  private val qualifierPageSize: Int = 1000
+
+  /**
+   * Converts a KijiRowData to a Cascading tuple.
+   *
+   * If there is no data in a column in this row, the value of that field in the tuple will be an
+   * empty iterable of cells.
+   *
+   * This is used in the `source` method of KijiScheme, for reading rows from Kiji into
+   * KijiExpress tuples.
+   *
+   * @param columns Mapping from field name to column definition.
+   * @param fields Field names of desired tuple elements.
+   * @param timestampField is the optional name of a field containing the timestamp that all values
+   *     in a tuple should be written to.
+   *     Use None if all values should be written at the current time.
+   * @param row to convert to a tuple.
+   * @param tableUri is the URI of the Kiji table.
+   * @param configuration identifying the cluster to use when building EntityIds.
+   * @return a tuple containing the values contained in the specified row.
+   */
+  private[express] def rowToTuple(
+      columns: Map[String, ColumnInputSpec],
+      fields: Fields,
+      timestampField: Option[Symbol],
+      row: KijiRowData,
+      tableUri: KijiURI,
+      configuration: Configuration
+  ): Tuple = {
+    val result: Tuple = new Tuple()
+
+    // Add the row's EntityId to the tuple.
+    val entityId: EntityId = EntityId.fromJavaEntityId(row.getEntityId)
+    result.add(entityId)
+
+    def rowToTupleColumnFamily(cf: ColumnFamilyInputSpec): Unit = {
+      if (row.containsColumn(cf.family)) {
+        cf.paging match {
+          case PagingSpec.Off => {
+            val stream: Seq[FlowCell[_]] = row
+                .iterator(cf.family)
+                .asScala
+                .toList
+                .map { kijiCell: KijiCell[_] => FlowCell(kijiCell) }
+            result.add(stream)
+          }
+          case PagingSpec.Cells(pageSize) => {
+            def genItr(): Iterator[FlowCell[_]] = {
+              new MapFamilyVersionIterator(row, cf.family, qualifierPageSize, pageSize)
+                  .asScala
+                  .map { entry: MapFamilyVersionIterator.Entry[_] =>
+                    FlowCell(
+                        cf.family,
+                        entry.getQualifier,
+                        entry.getTimestamp,
+                        AvroUtil.avroToScala(entry.getValue))
+              }
+            }
+            result.add(new TransientSeq(genItr))
+          }
+        }
+      } else {
+        result.add(Seq())
+      }
+    }
+
+    def rowToTupleQualifiedColumn(qc: QualifiedColumnInputSpec): Unit = {
+      if (row.containsColumn(qc.family, qc.qualifier)) {
+        qc.paging match {
+          case PagingSpec.Off => {
+            val stream: Seq[FlowCell[_]] = row
+                .iterator(qc.family, qc.qualifier)
+                .asScala
+                .toList
+                .map { kijiCell: KijiCell[_] => FlowCell(kijiCell) }
+            result.add(stream)
+          }
+          case PagingSpec.Cells(pageSize) => {
+            def genItr(): Iterator[FlowCell[_]] = {
+              new ColumnVersionIterator(row, qc.family, qc.qualifier, pageSize)
+                  .asScala
+                  .map { entry: java.util.Map.Entry[java.lang.Long,_] =>
+                    FlowCell(
+                      qc.family,
+                      qc.qualifier,
+                      entry.getKey,
+                      AvroUtil.avroToScala(entry.getValue)
+                    )
+                  }
+            }
+            result.add(new TransientSeq(genItr))
+          }
+        }
+      } else {
+        result.add(Seq())
+      }
+    }
+
+    // Get rid of the entity id and timestamp fields, then map over each field to add a column
+    // to the tuple.
+    fields
+        .iterator()
+        .asScala
+        .filter { field => field.toString != entityIdField }
+        .filter { field => field.toString != timestampField.getOrElse("") }
+        .map { field => columns(field.toString) }
+        // Build the tuple, by adding each requested value into result.
+        .foreach {
+          case cf: ColumnFamilyInputSpec => rowToTupleColumnFamily(cf)
+          case qc: QualifiedColumnInputSpec => rowToTupleQualifiedColumn(qc)
+        }
+
+    return result
+  }
+
+  /**
+   * Writes a Cascading tuple to a Kiji table.
+   *
+   * This is used in KijiScheme's `sink` method.
+   *
+   * @param columns mapping field names to column definitions.
+   * @param tableUri of the Kiji table.
+   * @param kiji is the Kiji instance the table belongs to.
+   * @param timestampField is the optional name of a field containing the timestamp that all values
+   *     in a tuple should be written to.
+   *     Use None if all values should be written at the current time.
+   * @param output to write out to Kiji.
+   * @param writer to use for writing to Kiji.
+   * @param layout Kiji table layout.
+   * @param configuration identifying the cluster to use when building EntityIds.
+   */
+  private[express] def putTuple(
+      columns: Map[String, ColumnOutputSpec],
+      tableUri: KijiURI,
+      kiji: Kiji,
+      timestampField: Option[Symbol],
+      output: TupleEntry,
+      writer: KijiTableWriter,
+      layout: KijiTableLayout,
+      configuration: Configuration
+  ) {
+    // Get the entityId.
+    val entityId = output
+        .getObject(entityIdField)
+        .asInstanceOf[EntityId]
+        .toJavaEntityId(EntityIdFactory.getFactory(layout))
+
+    // Get a timestamp to write the values to, if it was specified by the user.
+    val timestamp: Long = timestampField match {
+      case Some(field) => output.getObject(field.name).asInstanceOf[Long]
+      case None => HConstants.LATEST_TIMESTAMP
+    }
+
+    columns.keys.iterator
+        .foreach { field =>
+          val value = output.getObject(field)
+          val col: ColumnOutputSpec = columns(field)
+
+          val qualifier = col match {
+            case qc: QualifiedColumnOutputSpec => qc.qualifier
+            case cf: ColumnFamilyOutputSpec => output.getString(cf.qualifierSelector.name)
+          }
+
+          writer.put(entityId, col.family, qualifier, timestamp, col.encode(value))
+      }
+  }
+
+  /**
+   * Gets a schema from the reader schema.
+   *
+   * @param readerSchema to find the schema for.
+   * @param schemaTable to look up IDs in.
+   * @return the resolved Schema.
+   */
+  private[express] def resolveSchemaFromJSONOrUid(
+      readerSchema: AvroSchema,
+      schemaTable: KijiSchemaTable
+  ): Schema = {
+    Option(readerSchema.getJson) match {
+      case None => schemaTable.getSchema(readerSchema.getUid)
+      case Some(json) => new Schema.Parser().parse(json)
+    }
+  }
+
+  /**
+   * Builds a data request out of the timerange and list of column requests.
+   *
+   * @param timeRange of cells to retrieve.
+   * @param columns to retrieve.
+   * @return data request configured with timeRange and columns.
+   */
+  private[express] def buildRequest(
+      timeRange: TimeRange,
+      columns: Iterable[ColumnInputSpec]
+  ): KijiDataRequest = {
+    def addColumn(
+        builder: KijiDataRequestBuilder,
+        column: ColumnInputSpec
+    ): KijiDataRequestBuilder.ColumnsDef = {
+      val kijiFilter: KijiColumnFilter = column
+          .filter
+          .map { _.toKijiColumnFilter }
+          .getOrElse(null)
+      builder.newColumnsDef()
+          .withMaxVersions(column.maxVersions)
+          .withFilter(kijiFilter)
+          .withPageSize(column.paging.cellsPerPage.getOrElse(0))
+          .add(column.columnName)
+    }
+
+    val requestBuilder: KijiDataRequestBuilder = KijiDataRequest.builder()
+        .withTimeRange(timeRange.begin, timeRange.end)
+
+    columns
+        .foldLeft(requestBuilder) { (builder, column) =>
+          addColumn(builder, column)
+          builder
+        }
+        .build()
+  }
+
+  /**
+   * Transforms a list of field names into a Cascading [[cascading.tuple.Fields]] object.
+   *
+   * @param fieldNames is a list of field names.
+   * @return a Fields object containing the names.
+   */
+  private def toField(fieldNames: Iterable[Comparable[_]]): Fields = {
+    new Fields(fieldNames.toArray:_*)
+  }
+
+  /**
+   * Builds the list of tuple fields being read by a scheme. The special field name
+   * "entityId" will be included to hold entity ids from the rows of Kiji tables.
+   *
+   * @param fieldNames is a list of field names that a scheme should read.
+   * @return is a collection of fields created from the names.
+   */
+  private[express] def buildSourceFields(fieldNames: Iterable[String]): Fields = {
+    toField(Set(entityIdField) ++ fieldNames)
+  }
+
+  /**
+   * Builds the list of tuple fields being written by a scheme. The special field name "entityId"
+   * will be included to hold entity ids that values should be written to. Any fields that are
+   * specified as qualifiers for a map-type column family will also be included. A timestamp field
+   * can also be included, identifying a timestamp that all values will be written to.
+   *
+   * @param columns is the column requests for this Scheme, with the names of each of the
+   *     fields that contain data to write to Kiji.
+   * @param timestampField is the optional name of a field containing the timestamp that all values
+   *     in a tuple should be written to.
+   *     Use None if all values should be written at the current time.
+   * @return a collection of fields created from the parameters.
+   */
+  private[express] def buildSinkFields(
+      columns: Map[String, ColumnOutputSpec],
+      timestampField: Option[Symbol]
+  ): Fields = {
+    toField(Set(entityIdField)
+        ++ columns.keys
+        ++ extractQualifierSelectors(columns)
+        ++ timestampField.map { _.name } )
+  }
+
+  /**
+   * Extracts the names of qualifier selectors from the column requests for a Scheme.
+   *
+   * @param columns is the column requests for a Scheme.
+   * @return the names of fields that are qualifier selectors.
+   */
+  private[express] def extractQualifierSelectors(
+      columns: Map[String, ColumnOutputSpec]
+  ): Iterator[String] = {
+    columns.valuesIterator.collect {
+      case x: ColumnFamilyOutputSpec => x.qualifierSelector.name
+    }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiSinkContext.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiSinkContext.scala
new file mode 100644
index 0000000000000000000000000000000000000000..9086769512b6087d2d804dcd5514420d2c422a4f
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiSinkContext.scala
@@ -0,0 +1,48 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.KijiTableWriter
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiURI
+import org.kiji.schema.layout.KijiTableLayout
+
+/**
+ * Container for the table writer and Kiji table layout required during a sink
+ * operation to write the output of a map reduce task back to a Kiji table.
+ * This is configured during the sink prepare operation.
+ *
+ * @param kijiTableWriter is a writer object for this Kiji table.
+ * @param tableUri of the Kiji table.
+ * @param kiji is the Kiji instance for this table, used to access the schema table.
+ * @param kijiLayout is the layout for this Kiji table.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] case class KijiSinkContext (
+    kijiTableWriter: KijiTableWriter,
+    tableUri: KijiURI,
+    kiji: Kiji,
+    kijiLayout: KijiTableLayout) {
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiSourceContext.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiSourceContext.scala
new file mode 100644
index 0000000000000000000000000000000000000000..adc122346f0d9a469bf0fa6897927eae72000cb9
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiSourceContext.scala
@@ -0,0 +1,40 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.KijiURI
+
+/**
+ * Container for a Kiji row data and Kiji table layout object that is required by a map reduce
+ * task while reading from a Kiji table.
+ *
+ * @param rowContainer is the representation of a Kiji row.
+ * @param tableUri is the URI of the Kiji table.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] case class KijiSourceContext (
+    rowContainer: KijiValue,
+    tableUri: KijiURI) {
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiTableSplit.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiTableSplit.scala
new file mode 100644
index 0000000000000000000000000000000000000000..61c9cc1c3516a87dc2bc38012d25f1daea8e6ec0
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiTableSplit.scala
@@ -0,0 +1,96 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import java.io.DataInput
+import java.io.DataOutput
+
+import org.apache.hadoop.hbase.mapreduce.TableSplit
+import org.apache.hadoop.mapred.InputSplit
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+
+/**
+ * An input split that specifies a region of rows from a Kiji table to be processed by a MapReduce
+ * task. This input split stores exactly the same data as `TableSplit`, but does a better job
+ * handling default split sizes.
+ *
+ * @param split to which functionality is delegated.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+final private[express] class KijiTableSplit(
+    split: TableSplit)
+    extends InputSplit {
+  /**
+   * No argument constructor for KijiTableSplit so that it can be constructed via reflection. This
+   * is required to be a separate constructor so that java has access to it.
+   */
+  // scalastyle:off public.methods.have.type
+  def this() = this(new TableSplit())
+  // scalastyle:on public.methods.have.type
+
+  /**
+   * Returns the length of the split.
+   * TODO: EXP-180: KijiTableSplit should return a better estimate in getLength
+   *
+   * @return the length of the split.
+   * @see org.apache.hadoop.mapreduce.InputSplit#getLength()
+   */
+  override def getLength(): Long = split.getLength()
+
+  /**
+   * Get the list of hostnames where the input split is located.
+   *
+   * @return a list of hostnames defining this input split.
+   */
+  override def getLocations(): Array[String] = split.getLocations()
+
+  /**
+   * Implementation of Hadoop's writable deserialization.
+   *
+   * @param input data stream containing data to populate this split with.
+   */
+  override def readFields(input: DataInput) {
+    split.readFields(input)
+  }
+
+  /**
+   * Implementation of Hadoop's writable serialization.
+   *
+   * @param output data stream to populate with the data this split contains.
+   */
+  override def write(output: DataOutput) {
+    split.write(output)
+  }
+
+  /**
+   * @return the HBase row key of the first row processed by this input split.
+   */
+  def getStartRow(): Array[Byte] = split.getStartRow()
+
+  /**
+   * @return the HBase row key of the last row processed by this input split.
+   */
+  def getEndRow(): Array[Byte] = split.getEndRow()
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiTap.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiTap.scala
new file mode 100644
index 0000000000000000000000000000000000000000..3c49b54c0699d17898a33ec15036bec82bb886d0
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiTap.scala
@@ -0,0 +1,303 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import java.util.UUID
+
+import cascading.flow.FlowProcess
+import cascading.flow.hadoop.HadoopFlowProcess
+import cascading.scheme.Scheme
+import cascading.tap.Tap
+import cascading.tap.hadoop.io.HadoopTupleEntrySchemeCollector
+import cascading.tap.hadoop.io.HadoopTupleEntrySchemeIterator
+import cascading.tuple.TupleEntryCollector
+import cascading.tuple.TupleEntryIterator
+import com.google.common.base.Objects
+
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.mapred.JobConf
+import org.apache.hadoop.mapred.OutputCollector
+import org.apache.hadoop.mapred.RecordReader
+import org.apache.hadoop.mapred.lib.NullOutputFormat
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.express.flow._
+import org.kiji.express.flow.util.Resources.doAndRelease
+import org.kiji.mapreduce.framework.KijiConfKeys
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiURI
+import org.kiji.schema.layout.KijiTableLayout
+
+/**
+ * A Kiji-specific implementation of a Cascading `Tap`, which defines the location of a Kiji table.
+ *
+ * KijiTap is responsible for configuring a MapReduce job with the correct input format for reading
+ * from a Kiji table.
+ *
+ * KijiTap must be used with [[org.kiji.express.flow.framework.KijiScheme]] to perform decoding of
+ * cells in a Kiji table. [[org.kiji.express.flow.KijiSource]] handles the creation of both
+ * KijiScheme and KijiTap in KijiExpress.
+ *
+ * @param uri of the Kiji table to read or write from.
+ * @param scheme that will convert data read from Kiji into Cascading's tuple model.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+class KijiTap(
+    // This is not a val because KijiTap needs to be serializable and KijiURI is not.
+    uri: KijiURI,
+    private val scheme: KijiScheme)
+    extends Tap[JobConf, RecordReader[KijiKey, KijiValue], OutputCollector[_, _]](
+        scheme.asInstanceOf[Scheme[JobConf, RecordReader[KijiKey, KijiValue],
+            OutputCollector[_, _], _, _]]) {
+
+  /** Address of the table to read from or write to. */
+  private val tableUri: String = uri.toString()
+
+  /** Unique identifier for this KijiTap instance. */
+  private val id: String = UUID.randomUUID().toString()
+
+  /**
+   * Sets any configuration options that are required for running a MapReduce job
+   * that reads from a Kiji table. This method gets called on the client machine
+   * during job setup.
+   *
+   * @param flow being built.
+   * @param conf to which we will add the table uri.
+   */
+  override def sourceConfInit(flow: FlowProcess[JobConf], conf: JobConf) {
+    // Configure the job's input format.
+    conf.setInputFormat(classOf[KijiInputFormat])
+
+    // Store the input table.
+    conf.set(KijiConfKeys.KIJI_INPUT_TABLE_URI, tableUri)
+
+    super.sourceConfInit(flow, conf)
+  }
+
+  /**
+   * Sets any configuration options that are required for running a MapReduce job
+   * that writes to a Kiji table. This method gets called on the client machine
+   * during job setup.
+   *
+   * @param flow being built.
+   * @param conf to which we will add the table uri.
+   */
+  override def sinkConfInit(flow: FlowProcess[JobConf], conf: JobConf) {
+    // TODO(CHOP-35): Use an output format that writes to HFiles.
+    // Configure the job's output format.
+    conf.setOutputFormat(classOf[NullOutputFormat[_, _]])
+
+    // Store the output table.
+    conf.set(KijiConfKeys.KIJI_OUTPUT_TABLE_URI, tableUri)
+
+    super.sinkConfInit(flow, conf)
+  }
+
+  /**
+   * Provides a string representing the resource this `Tap` instance represents.
+   *
+   * @return a java UUID representing this KijiTap instance. Note: This does not return the uri of
+   *     the Kiji table being used by this tap to allow jobs that read from or write to the same
+   *     table to have different data request options.
+   */
+  override def getIdentifier(): String = id
+
+  /**
+   * Opens any resources required to read from a Kiji table.
+   *
+   * @param flow being run.
+   * @param recordReader that will read from the desired Kiji table.
+   * @return an iterator that reads rows from the desired Kiji table.
+   */
+  override def openForRead(
+      flow: FlowProcess[JobConf],
+      recordReader: RecordReader[KijiKey, KijiValue]): TupleEntryIterator = {
+    val modifiedFlow = if (flow.getStringProperty(KijiConfKeys.KIJI_INPUT_TABLE_URI) == null) {
+      // TODO CHOP-71 Remove this hack which is introduced by a scalding bug:
+      // https://github.com/twitter/scalding/issues/369
+      // This hack is only required for testing (HadoopTest Mode)
+      val jconf = flow.getConfigCopy
+      val fp = new HadoopFlowProcess(jconf)
+      sourceConfInit(fp, jconf)
+      fp
+    } else {
+      flow
+    }
+    new HadoopTupleEntrySchemeIterator(
+        modifiedFlow,
+        this.asInstanceOf[Tap[JobConf, RecordReader[_, _], OutputCollector[_, _]]],
+        recordReader)
+  }
+
+  /**
+   * Opens any resources required to write from a Kiji table.
+   *
+   * @param flow being run.
+   * @param outputCollector that will write to the desired Kiji table. Note: This is ignored
+   *     currently since writing to a Kiji table is currently implemented without using an output
+   *     format by writing to the table directly from
+   *     [[org.kiji.express.flow.framework.KijiScheme]].
+   * @return a collector that writes tuples to the desired Kiji table.
+   */
+  override def openForWrite(
+      flow: FlowProcess[JobConf],
+      outputCollector: OutputCollector[_, _]): TupleEntryCollector = {
+    new HadoopTupleEntrySchemeCollector(
+        flow,
+        this.asInstanceOf[Tap[JobConf, RecordReader[_, _], OutputCollector[_, _]]],
+        outputCollector)
+  }
+
+  /**
+   * Builds any resources required to read from or write to a Kiji table.
+   *
+   * Note: KijiExpress currently does not support automatic creation of Kiji tables.
+   *
+   * @param conf containing settings for this flow.
+   * @return true if required resources were created successfully.
+   * @throws UnsupportedOperationException always.
+   */
+  override def createResource(conf: JobConf): Boolean = {
+    throw new UnsupportedOperationException("KijiTap does not support creating tables for you.")
+  }
+
+  /**
+   * Deletes any unnecessary resources used to read from or write to a Kiji table.
+   *
+   * Note: KijiExpress currently does not support automatic deletion of Kiji tables.
+   *
+   * @param conf containing settings for this flow.
+   * @return true if superfluous resources were deleted successfully.
+   * @throws UnsupportedOperationException always.
+   */
+  override def deleteResource(conf: JobConf): Boolean = {
+    throw new UnsupportedOperationException("KijiTap does not support deleting tables for you.")
+  }
+
+  /**
+   * Determines if the Kiji table this `Tap` instance points to exists.
+   *
+   * @param conf containing settings for this flow.
+   * @return true if the target Kiji table exists.
+   */
+  override def resourceExists(conf: JobConf): Boolean = {
+    val uri: KijiURI = KijiURI.newBuilder(tableUri).build()
+
+    doAndRelease(Kiji.Factory.open(uri, conf)) { kiji: Kiji =>
+      kiji.getTableNames().contains(uri.getTable())
+    }
+  }
+
+  /**
+   * Gets the time that the target Kiji table was last modified.
+   *
+   * Note: This will always return the current timestamp.
+   *
+   * @param conf containing settings for this flow.
+   * @return the current time.
+   */
+  override def getModifiedTime(conf: JobConf): Long = System.currentTimeMillis()
+
+  override def equals(other: Any): Boolean = {
+    other match {
+      case tap: KijiTap => (tableUri == tap.tableUri) && (scheme == tap.scheme) && (id == tap.id)
+      case _ => false
+    }
+  }
+
+  override def hashCode(): Int = Objects.hashCode(tableUri, scheme, id)
+
+  /**
+   * Checks whether the instance, tables, and columns this tap uses can be accessed.
+   *
+   * @throws KijiExpressValidationException if the tables and columns are not accessible when this
+   *    is called.
+   */
+  private[express] def validate(conf: JobConf): Unit = {
+    val kijiUri: KijiURI = KijiURI.newBuilder(tableUri).build()
+    KijiTap.validate(kijiUri, scheme.inputColumns, scheme.outputColumns, conf)
+  }
+}
+
+@ApiAudience.Framework
+@ApiStability.Experimental
+object KijiTap {
+  /**
+   * Checks whether the instance, tables, and columns specified can be accessed.
+   *
+   * @throws KijiExpressValidationException if the tables and columns are not accessible when this
+   *    is called.
+   */
+  private[express] def validate(
+      kijiUri: KijiURI,
+      inputColumns: Map[String, ColumnInputSpec],
+      outputColumns: Map[String, ColumnOutputSpec],
+      conf: Configuration) {
+    // Try to open the Kiji instance.
+    val kiji: Kiji =
+        try {
+          Kiji.Factory.open(kijiUri, conf)
+        } catch {
+          case e: Exception =>
+            throw new InvalidKijiTapException(
+                "Error opening Kiji instance: %s\n".format(kijiUri.getInstance()) + e.getMessage)
+        }
+
+    // Try to open the table.
+    val table: KijiTable =
+        try {
+          kiji.openTable(kijiUri.getTable())
+        } catch {
+          case e: Exception =>
+            throw new InvalidKijiTapException(
+                "Error opening Kiji table: %s\n".format(kijiUri.getTable()) + e.getMessage)
+        } finally {
+          kiji.release() // Release the Kiji instance.
+        }
+
+    // Check the columns are valid
+    val tableLayout: KijiTableLayout = table.getLayout
+    table.release() // Release the KijiTable.
+
+    // Get a list of columns that don't exist
+    val inputColumnNames: Seq[KijiColumnName] = inputColumns.values.map(_.columnName).toList
+    val outputColumnNames: Seq[KijiColumnName] = outputColumns.values.map(_.columnName).toList
+
+    val nonExistentColumnErrors = (inputColumnNames ++ outputColumnNames)
+        // Filter for columns that don't exist
+        .filter( { case colname => !tableLayout.exists(colname) } )
+        .map { column =>
+            "One or more columns does not exist in the table %s: %s\n".format(table.getName, column)
+        }
+
+    val allErrors = nonExistentColumnErrors
+
+    // Combine all error strings.
+    if (!allErrors.isEmpty) {
+      throw new InvalidKijiTapException(
+          "Errors found in validating Tap: %s".format(allErrors.mkString(", \n"))
+      )
+    }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiValue.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiValue.scala
new file mode 100644
index 0000000000000000000000000000000000000000..caa90506134daeea28a85ec900373f917fd75eba
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/KijiValue.scala
@@ -0,0 +1,59 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.KijiRowData
+
+/**
+ * A reusable container for [[org.kiji.schema.KijiRowData]].
+ *
+ * The MapReduce framework views a data set as a collection of key-value pairs,
+ * and likes to read those pairs into a reusable instance of the key or value class. When a row
+ * is read from a Kiji table, its [[org.kiji.schema.EntityId]] is emitted as the key, and
+ * [[org.kiji.schema.KijiRowData]] is emitted as the value. Because instances of
+ * [[org.kiji.schema.KijiRowData]] are not reusable, this class is provided to give the MapReduce
+ * framework a reusable container.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+final class KijiValue {
+  /** The row data being wrapped by this instance. */
+  private var currentValue: KijiRowData = null
+
+  /**
+   * Retrieves the [[org.kiji.schema.KijiRowData]] wrapped by this instance.
+   *
+   * @return the row data wrapped by this instance.
+   */
+  def get(): KijiRowData = currentValue
+
+  /**
+   * Sets the [[org.kiji.schema.KijiRowData]] wrapped by this instance.
+   *
+   * @param value that will be wrapped by this instance.
+   */
+  def set(value: KijiRowData) {
+    currentValue = value
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/LocalKijiScheme.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/LocalKijiScheme.scala
new file mode 100644
index 0000000000000000000000000000000000000000..f3401ab5ee54a939fe8496fe985f3ea39a1f7c4c
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/LocalKijiScheme.scala
@@ -0,0 +1,344 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import java.io.InputStream
+import java.io.OutputStream
+import java.util.HashMap
+import java.util.{Map => JMap}
+import java.util.Properties
+
+import scala.collection.JavaConverters.mapAsJavaMapConverter
+import scala.collection.JavaConverters.asScalaIteratorConverter
+
+import cascading.flow.FlowProcess
+import cascading.flow.hadoop.util.HadoopUtil
+import cascading.scheme.Scheme
+import cascading.scheme.SinkCall
+import cascading.scheme.SourceCall
+import cascading.tap.Tap
+import cascading.tuple.Tuple
+import cascading.tuple.TupleEntry
+import com.google.common.base.Objects
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.hbase.HBaseConfiguration
+import org.apache.hadoop.mapred.JobConf
+import org.slf4j.Logger
+import org.slf4j.LoggerFactory
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.ColumnInputSpec
+import org.kiji.express.flow.ColumnOutputSpec
+import org.kiji.express.flow.TimeRange
+import org.kiji.express.flow.util.GenericCellSpecs
+import org.kiji.express.flow.util.Resources._
+import org.kiji.express.flow.util.SpecificCellSpecs
+import org.kiji.mapreduce.framework.KijiConfKeys
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiRowData
+import org.kiji.schema.KijiRowScanner
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiTableReader
+import org.kiji.schema.KijiTableWriter
+import org.kiji.schema.KijiURI
+import org.kiji.schema.layout.CellSpec
+import org.kiji.schema.layout.KijiTableLayout
+
+/**
+ * Encapsulates the state required to read from a Kiji table locally, for use in
+ * [[org.kiji.express.flow.framework.LocalKijiScheme]].
+ *
+ * @param reader that has an open connection to the desired Kiji table.
+ * @param scanner that has an open connection to the desired Kiji table.
+ * @param iterator that maintains the current row pointer.
+ * @param tableUri of the kiji table.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] case class InputContext(
+    reader: KijiTableReader,
+    scanner: KijiRowScanner,
+    iterator: Iterator[KijiRowData],
+    tableUri: KijiURI,
+    configuration: Configuration)
+
+/**
+ * Encapsulates the state required to write to a Kiji table.
+ *
+ * @param writer that has an open connection to the desired Kiji table.
+ * @param tableUri of the Kiji table.
+ * @param layout of the kiji table.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] case class OutputContext(
+    writer: KijiTableWriter,
+    tableUri: KijiURI,
+    kiji: Kiji,
+    layout: KijiTableLayout,
+    configuration: Configuration)
+
+/**
+ * A local version of [[org.kiji.express.flow.framework.KijiScheme]] that is meant to be used with
+ * Cascading's local job runner. [[org.kiji.express.flow.framework.KijiScheme]] and
+ * LocalKijiScheme both define how to read and write the data stored in a Kiji table.
+ *
+ * This scheme is meant to be used with [[org.kiji.express.flow.framework.LocalKijiTap]] and
+ * Cascading's local job runner. Jobs run with Cascading's local job runner execute on
+ * your local machine instead of a cluster. This can be helpful for testing or quick jobs.
+ *
+ * In KijiExpress, LocalKijiScheme is used in tests.  See [[org.kiji.express.flow.KijiSource]]'s
+ * `TestLocalKijiScheme` class.
+ *
+ * This scheme is responsible for converting rows from a Kiji table that are input to a
+ * Cascading flow into Cascading tuples (see
+ * `source(cascading.flow.FlowProcess, cascading.scheme.SourceCall)`) and writing output
+ * data from a Cascading flow to a Kiji table
+ * (see `sink(cascading.flow.FlowProcess, cascading.scheme.SinkCall)`).
+ *
+ * Note: LocalKijiScheme logs every row that was skipped because of missing data in a column. It
+ * lacks the parameter `loggingInterval` in [[org.kiji.express.flow.framework.KijiScheme]] that
+ * configures how many skipped rows will be logged.
+ *
+ * Note: Warnings about a missing serialVersionUID are ignored here. When KijiScheme is
+ * serialized, the result is not persisted anywhere making serialVersionUID unnecessary.
+ *
+ * @see [[org.kiji.express.flow.framework.KijiScheme]]
+ *
+ * @param timeRange to include from the Kiji table.
+ * @param timestampField is the optional name of a field containing the timestamp that all values
+ *     in a tuple should be written to.
+ *     Use None if all values should be written at the current time.
+ * @param icolumns is a one-to-one mapping from field names to Kiji columns. The columns in the
+ *     map will be read into their associated tuple fields.
+ * @param ocolumns is a one-to-one mapping from field names to Kiji columns. Values from the
+ *     tuple fields will be written to their associated column.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+private[express] class LocalKijiScheme(
+    private[express] val timeRange: TimeRange,
+    private[express] val timestampField: Option[Symbol],
+    private[express] val icolumns: Map[String, ColumnInputSpec] = Map(),
+    private[express] val ocolumns: Map[String, ColumnOutputSpec] = Map())
+    extends Scheme[Properties, InputStream, OutputStream, InputContext, OutputContext] {
+  private val logger: Logger = LoggerFactory.getLogger(classOf[LocalKijiScheme])
+
+  /** Set the fields that should be in a tuple when this source is used for reading and writing. */
+  setSourceFields(KijiScheme.buildSourceFields(icolumns.keys ++ ocolumns.keys))
+  setSinkFields(KijiScheme.buildSinkFields(ocolumns, timestampField))
+
+  /**
+   * Sets any configuration options that are required for running a local job
+   * that reads from a Kiji table.
+   *
+   * @param process flow being built.
+   * @param tap that is being used with this scheme.
+   * @param conf is an unused Properties object that is a stand-in for a job configuration object.
+   */
+  override def sourceConfInit(
+      process: FlowProcess[Properties],
+      tap: Tap[Properties, InputStream, OutputStream],
+      conf: Properties) {
+    // No-op. Setting options in a java Properties object is not going to help us read from
+    // a Kiji table.
+  }
+
+  /**
+   * Sets up any resources required to read from a Kiji table.
+   *
+   * @param process currently being run.
+   * @param sourceCall containing the context for this source.
+   */
+  override def sourcePrepare(
+      process: FlowProcess[Properties],
+      sourceCall: SourceCall[InputContext, InputStream]) {
+    val conf: JobConf = HadoopUtil.createJobConf(
+        process.getConfigCopy,
+        new JobConf(HBaseConfiguration.create()))
+    val uriString: String = conf.get(KijiConfKeys.KIJI_INPUT_TABLE_URI)
+    val kijiUri: KijiURI = KijiURI.newBuilder(uriString).build()
+
+    // Build the input context.
+    withKijiTable(kijiUri, conf) { table: KijiTable =>
+      val request = KijiScheme.buildRequest(timeRange, icolumns.values)
+      val reader = {
+        val allCellSpecs: JMap[KijiColumnName, CellSpec] = new HashMap[KijiColumnName, CellSpec]()
+        allCellSpecs.putAll(GenericCellSpecs(table))
+        allCellSpecs.putAll(SpecificCellSpecs.buildCellSpecs(table.getLayout, icolumns).asJava)
+        table.getReaderFactory.openTableReader(allCellSpecs)
+      }
+      val scanner = reader.getScanner(request)
+      val tableUri = table.getURI
+      val context = InputContext(reader, scanner, scanner.iterator.asScala, tableUri, conf)
+
+      sourceCall.setContext(context)
+    }
+  }
+
+  /**
+   * Reads and converts a row from a Kiji table to a Cascading Tuple. This method
+   * is called once for each row in the table.
+   *
+   * @param process is the current Cascading flow being run.
+   * @param sourceCall containing the context for this source.
+   * @return <code>true</code> if another row was read and it was converted to a tuple,
+   *     <code>false</code> if there were no more rows to read.
+   */
+  override def source(
+      process: FlowProcess[Properties],
+      sourceCall: SourceCall[InputContext, InputStream]): Boolean = {
+    val context: InputContext = sourceCall.getContext
+    if (context.iterator.hasNext) {
+      // Get the current row.
+      val row: KijiRowData = context.iterator.next()
+      val result: Tuple =
+          KijiScheme.rowToTuple(
+              icolumns,
+              getSourceFields,
+              timestampField,
+              row,
+              context.tableUri,
+              context.configuration)
+
+      // Set the result tuple and return from this method.
+      sourceCall.getIncomingEntry.setTuple(result)
+      process.increment(KijiScheme.counterGroupName, KijiScheme.counterSuccess, 1)
+      return true // We set a result tuple, return true for success.
+    } else {
+      return false // We reached the end of the input.
+    }
+  }
+
+  /**
+   * Cleans up any resources used to read from a Kiji table.
+   *
+   * @param process Current Cascading flow being run.
+   * @param sourceCall Object containing the context for this source.
+   */
+  override def sourceCleanup(
+      process: FlowProcess[Properties],
+      sourceCall: SourceCall[InputContext, InputStream]) {
+    val context: InputContext = sourceCall.getContext
+    context.reader.close()
+    context.scanner.close()
+
+    // Set the context to null so that we no longer hold any references to it.
+    sourceCall.setContext(null)
+  }
+
+  /**
+   * Sets any configuration options that are required for running a local job
+   * that writes to a Kiji table.
+   *
+   * @param process Current Cascading flow being built.
+   * @param tap The tap that is being used with this scheme.
+   * @param conf The job configuration object.
+   */
+  override def sinkConfInit(
+      process: FlowProcess[Properties],
+      tap: Tap[Properties, InputStream, OutputStream],
+      conf: Properties) {
+    // No-op. Setting options in a java Properties object is not going to help us write to
+    // a Kiji table.
+  }
+
+  /**
+   * Sets up any resources required to write to a Kiji table.
+   *
+   * @param process Current Cascading flow being run.
+   * @param sinkCall Object containing the context for this source.
+   */
+  override def sinkPrepare(
+      process: FlowProcess[Properties],
+      sinkCall: SinkCall[OutputContext, OutputStream]) {
+    // Open a table writer.
+    val conf: JobConf = HadoopUtil.createJobConf(process.getConfigCopy,
+        new JobConf(HBaseConfiguration.create()))
+    val uriString: String = conf.get(KijiConfKeys.KIJI_OUTPUT_TABLE_URI)
+    val uri: KijiURI = KijiURI.newBuilder(uriString).build()
+
+    val kiji = Kiji.Factory.open(uri)
+    doAndRelease(kiji.openTable(uri.getTable)) { table: KijiTable =>
+      // Set the sink context to an opened KijiTableWriter.
+      sinkCall
+        .setContext(OutputContext(table.openTableWriter(), uri, kiji, table.getLayout, conf))
+    }
+  }
+
+  /**
+   * Converts and writes a Cascading Tuple to a Kiji table.
+   *
+   * @param process Current Cascading flow being run.
+   * @param sinkCall Object containing the context for this source.
+   */
+  override def sink(
+      process: FlowProcess[Properties],
+      sinkCall: SinkCall[OutputContext, OutputStream]) {
+    // Retrieve writer from the scheme's context.
+    val OutputContext(writer, tableUri, kiji, layout, configuration) = sinkCall.getContext
+
+    // Write the tuple out.
+    val output: TupleEntry = sinkCall.getOutgoingEntry
+    KijiScheme.putTuple(
+        ocolumns,
+        tableUri,
+        kiji,
+        timestampField,
+        output,
+        writer,
+        layout,
+        configuration)
+  }
+
+  /**
+   * Cleans up any resources used to write to a Kiji table.
+   *
+   * @param process Current Cascading flow being run.
+   * @param sinkCall Object containing the context for this source.
+   */
+  override def sinkCleanup(
+      process: FlowProcess[Properties],
+      sinkCall: SinkCall[OutputContext, OutputStream]) {
+    sinkCall.getContext.writer.close()
+    sinkCall.getContext.kiji.release()
+    // Set the context to null so that we no longer hold any references to it.
+    sinkCall.setContext(null)
+  }
+
+  override def equals(other: Any): Boolean = {
+    other match {
+      case scheme: LocalKijiScheme => {
+        icolumns == scheme.icolumns &&
+        ocolumns == scheme.ocolumns &&
+            timestampField == scheme.timestampField &&
+            timeRange == scheme.timeRange
+      }
+      case _ => false
+    }
+  }
+
+  override def hashCode(): Int = Objects.hashCode(icolumns, ocolumns, timestampField, timeRange)
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/LocalKijiTap.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/LocalKijiTap.scala
new file mode 100644
index 0000000000000000000000000000000000000000..9019027f20a98b45bf9a912f1f29ab8c4b6906f3
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/LocalKijiTap.scala
@@ -0,0 +1,228 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import java.io.ByteArrayInputStream
+import java.io.ByteArrayOutputStream
+import java.io.InputStream
+import java.io.OutputStream
+import java.util.Properties
+import java.util.UUID
+
+import cascading.flow.FlowProcess
+import cascading.flow.hadoop.util.HadoopUtil
+import cascading.scheme.Scheme
+import cascading.tap.Tap
+import cascading.tuple.TupleEntryCollector
+import cascading.tuple.TupleEntryIterator
+import cascading.tuple.TupleEntrySchemeCollector
+import cascading.tuple.TupleEntrySchemeIterator
+import com.google.common.base.Objects
+import org.apache.hadoop.hbase.HBaseConfiguration
+import org.apache.hadoop.mapred.JobConf
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.express.flow.util.Resources.doAndRelease
+import org.kiji.mapreduce.framework.KijiConfKeys
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiURI
+
+/**
+ * A Kiji-specific implementation of a Cascading `Tap`, which defines the location of a Kiji table.
+ *
+ * LocalKijiTap is responsible for configuring a local Cascading job with the settings necessary to
+ * read from a Kiji table.
+ *
+ * LocalKijiTap must be used with [[org.kiji.express.flow.framework.LocalKijiScheme]] to perform
+ * decoding of cells in a Kiji table. [[org.kiji.express.flow.KijiSource]] handles the creation
+ * of both LocalKijiScheme and LocalKijiTap in KijiExpress.
+ *
+ * @param uri of the Kiji table to read or write from.
+ * @param scheme that will convert data read from Kiji into Cascading's tuple model.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+private[express] class LocalKijiTap(
+    uri: KijiURI,
+    private val scheme: LocalKijiScheme)
+    extends Tap[Properties, InputStream, OutputStream](
+        scheme.asInstanceOf[Scheme[Properties, InputStream, OutputStream, _, _]]) {
+
+  /** The URI of the table to be read through this tap. */
+  private val tableUri: String = uri.toString()
+
+  /** A unique identifier for this tap instance. */
+  private val id: String = UUID.randomUUID().toString()
+
+  /**
+   * Sets any configuration options that are required for running a local job
+   * that reads from a Kiji table. This method gets called on the client machine
+   * during job setup.
+   *
+   * @param flow being built.
+   * @param conf to which we will add the table uri.
+   */
+  override def sourceConfInit(
+      flow: FlowProcess[Properties],
+      conf: Properties) {
+    // Store the input table.
+    conf.setProperty(KijiConfKeys.KIJI_INPUT_TABLE_URI, tableUri)
+
+    super.sourceConfInit(flow, conf)
+  }
+
+  /**
+   * Sets any configuration options that are required for running a local job
+   * that writes to a Kiji table. This method gets called on the client machine
+   * during job setup.
+   *
+   * @param flow being built.
+   * @param conf to which we will add the table uri.
+   */
+  override def sinkConfInit(
+      flow: FlowProcess[Properties],
+      conf: Properties) {
+    // Store the output table.
+    conf.setProperty(KijiConfKeys.KIJI_OUTPUT_TABLE_URI, tableUri)
+
+    super.sinkConfInit(flow, conf)
+  }
+
+  /**
+   * Provides a string representing the resource this `Tap` instance represents.
+   *
+   * @return a java UUID representing this KijiTap instance. Note: This does not return the uri of
+   *     the Kiji table being used by this tap to allow jobs that read from or write to the same
+   *     table to have different data request options.
+   */
+  override def getIdentifier(): String = id
+
+  /**
+   * Opens any resources required to read from a Kiji table.
+   *
+   * @param flow being run.
+   * @param input stream that will read from the desired Kiji table.
+   * @return an iterator that reads rows from the desired Kiji table.
+   */
+  override def openForRead(
+      flow: FlowProcess[Properties],
+      input: InputStream): TupleEntryIterator = {
+    return new TupleEntrySchemeIterator[Properties, InputStream](
+        flow,
+        getScheme(),
+        if (null == input) new ByteArrayInputStream(Array()) else input,
+        getIdentifier())
+  }
+
+  /**
+   * Opens any resources required to write from a Kiji table.
+   *
+   * @param flow being run.
+   * @param output stream that will write to the desired Kiji table. Note: This is ignored
+   *     currently since writing to a Kiji table is currently implemented without using an output
+   *     format by writing to the table directly from
+   *     [[org.kiji.express.flow.framework.KijiScheme]].
+   * @return a collector that writes tuples to the desired Kiji table.
+   */
+  override def openForWrite(
+      flow: FlowProcess[Properties],
+      output: OutputStream): TupleEntryCollector = {
+    return new TupleEntrySchemeCollector[Properties, OutputStream](
+        flow,
+        getScheme(),
+        if (null == output) new ByteArrayOutputStream() else output,
+        getIdentifier())
+  }
+
+  /**
+   * Builds any resources required to read from or write to a Kiji table.
+   *
+   * Note: KijiExpress currently does not support automatic creation of Kiji tables.
+   *
+   * @param conf containing settings for this flow.
+   * @return true if required resources were created successfully.
+   * @throws UnsupportedOperationException always.
+   */
+  override def createResource(conf: Properties): Boolean = {
+    throw new UnsupportedOperationException("KijiTap does not support creating tables for you.")
+  }
+
+  /**
+   * Deletes any unnecessary resources used to read from or write to a Kiji table.
+   *
+   * Note: KijiExpress currently does not support automatic deletion of Kiji tables.
+   *
+   * @param conf containing settings for this flow.
+   * @return true if superfluous resources were deleted successfully.
+   * @throws UnsupportedOperationException always.
+   */
+  override def deleteResource(conf: Properties): Boolean = {
+    throw new UnsupportedOperationException("KijiTap does not support deleting tables for you.")
+  }
+
+  /**
+   * Determines if the Kiji table this `Tap` instance points to exists.
+   *
+   * @param conf containing settings for this flow.
+   * @return true if the target Kiji table exists.
+   */
+  override def resourceExists(conf: Properties): Boolean = {
+    val uri: KijiURI = KijiURI.newBuilder(tableUri).build()
+    val jobConf: JobConf = HadoopUtil.createJobConf(conf,
+        new JobConf(HBaseConfiguration.create()))
+    doAndRelease(Kiji.Factory.open(uri, jobConf)) { kiji: Kiji =>
+      kiji.getTableNames().contains(uri.getTable())
+    }
+  }
+
+  /**
+   * Gets the time that the target Kiji table was last modified.
+   *
+   * Note: This will always return the current timestamp.
+   *
+   * @param conf containing settings for this flow.
+   * @return the current time.
+   */
+  override def getModifiedTime(conf: Properties): Long = System.currentTimeMillis()
+
+  override def equals(other: Any): Boolean = {
+    other match {
+      case tap: LocalKijiTap => ((tableUri == tap.tableUri)
+          && (scheme == tap.scheme)
+          && (id == tap.id))
+      case _ => false
+    }
+  }
+
+  override def hashCode(): Int = Objects.hashCode(tableUri, scheme, id)
+
+  /**
+   * Checks whether the instance, tables, and columns this tap uses can be accessed.
+   *
+   * @throws KijiExpressValidationException if the tables and columns are not accessible when this
+   *    is called.
+   */
+  private[express] def validate(conf: Properties): Unit = {
+    val kijiUri: KijiURI = KijiURI.newBuilder(tableUri).build()
+    KijiTap.validate(kijiUri, scheme.icolumns, scheme.ocolumns, HadoopUtil.createJobConf(conf,
+        new JobConf(HBaseConfiguration.create())))
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileFlowStepStrategy.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileFlowStepStrategy.scala
new file mode 100644
index 0000000000000000000000000000000000000000..07ac7fd83196e97b88317cb879a1e802124deae7
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileFlowStepStrategy.scala
@@ -0,0 +1,122 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.hfile
+
+import java.net.URI
+import java.util.List
+
+import cascading.flow.Flow
+import cascading.flow.FlowStep
+import cascading.flow.FlowStepStrategy
+import cascading.tap.hadoop.Hfs
+import cascading.tap.hadoop.util.TempHfs
+import cascading.tuple.Tuple
+import cascading.util.Util
+import org.apache.hadoop.filecache.DistributedCache
+import org.apache.hadoop.fs.Path
+import org.apache.hadoop.io.NullWritable
+import org.apache.hadoop.mapred.FileOutputFormat
+import org.apache.hadoop.mapred.JobConf
+import org.apache.hadoop.mapred.SequenceFileOutputFormat
+import org.apache.hadoop.mapred.lib.IdentityReducer
+import org.apache.hadoop.mapred.lib.KeyFieldBasedComparator
+import org.apache.hadoop.mapred.lib.TotalOrderPartitioner
+import org.apache.hadoop.mapreduce.JobContext
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.mapreduce.framework.HFileKeyValue
+import org.kiji.mapreduce.framework.HFileKeyValue.FastComparator
+import org.kiji.mapreduce.output.HFileMapReduceJobOutput
+import org.kiji.mapreduce.output.framework.KijiHFileOutputFormat
+import org.kiji.schema.KijiURI
+import org.kiji.schema.mapreduce.KijiConfKeys
+
+/**
+ * An implementation of a Cascading FlowStepStrategy used to alter the properties
+ * of the flow step corresponding to the sink to support writing directly to HFiles. This
+ * will only operate on FlowSteps where the sink's outputFormat is the KijiHFileOutputFormat.
+ *
+ * There are two situations that can happen when writing to HFiles:
+ * <ol>
+ *  <li> The Cascading sink step is a map-only flow (with no reducer). In this case, the Identity
+ *  reducer will be forced to be used and the correct partitioner configured so that the
+ *  tuples will be sinked to HFiles. </li>
+ *  <li> The sink step is part of an flow with a reducer in which case the output will be routed
+ *  to a temporary sequence file that a secondary M/R job will use to correct sort and store
+ *  the data into HFiles for bulk loading </li>
+ * </ol>
+ *
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+@Inheritance.Sealed
+class HFileFlowStepStrategy extends FlowStepStrategy[JobConf] {
+
+  override def apply(flow: Flow[JobConf],
+                     predecessorSteps: List[FlowStep[JobConf]],
+                     flowStep: FlowStep[JobConf]) {
+    val conf = flowStep.getConfig()
+    val outputFormat = conf.get("elephantbird.class.for.DeprecatedOutputFormatWrapper")
+    val numReducers = conf.getNumReduceTasks()
+    val hfOutputFormat = classOf[KijiHFileOutputFormat].getName()
+
+    if (outputFormat != null && outputFormat == hfOutputFormat) {
+      // If this is part of a map-only job, then we configure a reducer with the correct
+      // partitioner.
+      if (numReducers == 0) {
+        conf.setPartitionerClass(classOf[TotalOrderPartitioner[HFileKeyValue, NullWritable]])
+        conf.setReducerClass(classOf[IdentityReducer[HFileKeyValue, NullWritable]])
+
+        conf.setMapOutputKeyClass(classOf[HFileKeyValue])
+        conf.setMapOutputValueClass(classOf[NullWritable])
+        conf.setOutputKeyComparatorClass(classOf[FastComparator])
+
+        val outputURI = conf.get(KijiConfKeys.OUTPUT_KIJI_TABLE_URI)
+        val kijiURI = KijiURI.newBuilder(outputURI).build()
+        val splits = HFileMapReduceJobOutput.makeTableKeySplit(kijiURI, 0, conf)
+        conf.setNumReduceTasks(splits.size())
+
+        // Write the file that the TotalOrderPartitioner reads to determine where to partition
+        // records.
+        var partitionFilePath =
+          new Path(conf.getWorkingDirectory(), TotalOrderPartitioner.DEFAULT_PATH)
+
+        val fs = partitionFilePath.getFileSystem(conf)
+        partitionFilePath = fs.makeQualified(partitionFilePath)
+        HFileMapReduceJobOutput.writePartitionFile(conf, partitionFilePath, splits)
+        val cacheUri =
+          new URI(partitionFilePath.toString() + "#" + TotalOrderPartitioner.DEFAULT_PATH)
+        DistributedCache.addCacheFile(cacheUri, conf)
+        DistributedCache.createSymlink(conf)
+      } else {
+        // We use the temporary path that was configured by the job to dump the HFileKVs
+        // for the second M/R job to process and then properly produce HFiles.
+        val newOutputPath = new Path(conf.get(HFileKijiOutput.TEMP_HFILE_OUTPUT_KEY))
+        conf.setOutputKeyClass(classOf[HFileKeyValue])
+        conf.setOutputValueClass(classOf[NullWritable])
+
+        conf.setOutputFormat(classOf[SequenceFileOutputFormat[HFileKeyValue, NullWritable]])
+        FileOutputFormat.setOutputPath(conf, newOutputPath)
+      }
+    }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiJob.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiJob.scala
new file mode 100644
index 0000000000000000000000000000000000000000..2b39d243320c919fef02df5729fe4de8d03556f4
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiJob.scala
@@ -0,0 +1,168 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.hfile
+
+import scala.transient
+
+import cascading.flow.Flow
+import cascading.tap.hadoop.Hfs
+import cascading.util.Util
+import com.twitter.scalding.Args
+import com.twitter.scalding.HadoopTest
+import com.twitter.scalding.Hdfs
+import com.twitter.scalding.Job
+import com.twitter.scalding.Mode
+import com.twitter.scalding.WritableSequenceFile
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.fs.FileSystem
+import org.apache.hadoop.fs.Path
+import org.apache.hadoop.io.NullWritable
+import org.apache.hadoop.mapred.JobConf
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.KijiJob
+import org.kiji.mapreduce.framework.HFileKeyValue
+
+/**
+ * HFileKijiJob is an extension of KijiJob and users should extend it instead of KijiJob when
+ * writing jobs in KijiExpress that need to output HFiles that can eventually be bulk-loaded into
+ * HBase.
+ *
+ * In your HFileKijiJob, you need to write to a source constructed with
+ * [[org.kiji.express.flow.framework.hfile.HFileKijiOutput]].
+ *
+ * You can extend HFileKijiJob like this:
+ *
+ * {{{
+ * class MyKijiExpressClass(args) extends HFileKijiJob(args) {
+ *   // Your code here.
+ *   .write(HFileKijiOutput(tableUri = "kiji://localhost:2181/default/mytable",
+ *       hFileOutput = "my_hfiles",
+ *       timestampField = 'timestamps,
+ *       'column1 -> "info:column1",
+ *       'column2 -> "info:column2"))
+ * }
+ * }}}
+ *
+ *     NOTE: To properly work with dumping to HFiles, the argument --hfile-output must be provided.
+ *     This argument specifies the location where the HFiles will be written upon job completion.
+ *     Also required is the --output flag. This argument specifies the Kiji table to use to obtain
+ *     layout information to properly format the HFiles for bulk loading.
+ *
+ * @param args to the job. These get parsed in from the command line by Scalding.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Extensible
+class HFileKijiJob(args: Args) extends KijiJob(args) {
+  /** Name of the command-line argument that specifies the temporary HFile root directory. */
+  final val HFileOutputArgName: String = "hfile-output"
+
+  /** Name of the command-line argument that specifies the target output table. */
+  final val TableOutputArgName: String = "output"
+
+  // Force the check to ensure that a value has been provided for the hFileOutput
+  args(HFileOutputArgName)
+  args(TableOutputArgName)
+
+  @transient
+  lazy private val jobConf: Configuration = implicitly[Mode] match {
+    case Hdfs(_, configuration) => {
+      configuration
+    }
+    case HadoopTest(configuration, _) => {
+      configuration
+    }
+    case _ => new JobConf()
+  }
+
+  @transient
+  lazy val uniqTempFolder = makeTemporaryPathDirString("HFileDumper")
+
+  val tempPath = new Path(Hfs.getTempPath(jobConf.asInstanceOf[JobConf]), uniqTempFolder).toString
+
+  override def config(implicit mode: Mode): Map[AnyRef, AnyRef] = {
+    val baseConfig = super.config(mode)
+    baseConfig ++ Map(HFileKijiOutput.TEMP_HFILE_OUTPUT_KEY -> tempPath.toString())
+  }
+
+  override def buildFlow(implicit mode: Mode): Flow[_] = {
+    val flow = super.buildFlow
+    // Here we set the strategy to change the sink steps since we are dumping to HFiles.
+    flow.setFlowStepStrategy(new HFileFlowStepStrategy)
+    flow
+  }
+
+  override def next: Option[Job] = {
+    val fs = FileSystem.get(jobConf)
+    if(fs.exists(new Path(tempPath))) {
+      val newArgs = args + ("input" -> Some(tempPath))
+      val job = new HFileMapJob(newArgs)
+      Some(job)
+    } else {
+      None
+    }
+  }
+
+  // Borrowed from Hfs#makeTemporaryPathDirString
+  private def makeTemporaryPathDirString(name: String) = {
+    // _ is treated as a hidden file, so wipe them out
+    val name2 = name.replaceAll("^[_\\W\\s]+", "")
+
+    val name3 = if (name2.isEmpty()) {
+      "temp-path"
+    } else {
+      name2
+    }
+
+    name3.replaceAll("[\\W\\s]+", "_") + Util.createUniqueID()
+  }
+}
+
+/**
+ * Private job implementation that executes the conversion of the intermediary HFile key-value
+ * sequence files to the final HFiles. This is done only if the first job had a Cascading
+ * configured reducer.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+private final class HFileMapJob(args: Args) extends HFileKijiJob(args) {
+
+  override def next: Option[Job] = {
+    val conf: Configuration = implicitly[Mode] match {
+      case Hdfs(_, configuration) => {
+        configuration
+      }
+      case HadoopTest(configuration, _) => {
+        configuration
+      }
+      case _ => new JobConf()
+    }
+    val fs = FileSystem.get(conf)
+    val input = args("input")
+    fs.delete(new Path(input), true)
+    None
+  }
+
+  WritableSequenceFile[HFileKeyValue, NullWritable](args("input"), ('keyValue, 'bogus))
+      .write(new HFileSource(args(TableOutputArgName),args(HFileOutputArgName)))
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiOutput.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiOutput.scala
new file mode 100644
index 0000000000000000000000000000000000000000..14fe2cf2b2817e4be0e408ae7414b33a1c6c5135
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiOutput.scala
@@ -0,0 +1,174 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.hfile
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.All
+import org.kiji.express.flow.ColumnOutputSpec
+import org.kiji.express.flow.QualifiedColumnOutputSpec
+
+/**
+ * Factory methods for constructing [[org.kiji.express.flow.framework.hfile.HFileKijiSource]]s that
+ * will be used as outputs of a KijiExpress flow. Two basic APIs are provided with differing
+ * complexity. These are similar to the factory methods in [[org.kiji.express.flow.KijiOutput]]
+ * APIs except that an extra parameter for the HFile output location is required.
+ *
+ * Simple:
+ * {{{
+ *   // Create an HFileKijiOutput that writes to the table named `mytable` putting timestamps in the
+ *   // `'timestamps` field and writing the fields `'column1` and `'column2` to the columns
+ *   // `info:column1` and `info:column2`. The resulting HFiles will be written to the "my_hfiles"
+ *   // folder.
+ *   HFileKijiOutput(
+ *       tableUri = "kiji://localhost:2181/default/mytable",
+ *       hFileOutput = "my_hfiles",
+ *       timestampField = 'timestamps,
+ *       'column1 -> "info:column1",
+ *       'column2 -> "info:column2")
+ * }}}
+ *
+ * Verbose:
+ * {{{
+ *   // Create a KijiOutput that writes to the table named `mytable` putting timestamps in the
+ *   // `'timestamps` field and writing the fields `'column1` and `'column2` to the columns
+ *   // `info:column1` and `info:column2`. The resulting HFiles will be written to the "my_hfiles"
+ *   // folder.
+ *   HFileKijiOutput(
+ *       tableUri = "kiji://localhost:2181/default/mytable",
+ *       hFileOutput = "my_hfiles",
+ *       timestampField = 'timestamps,
+ *       columns = Map(
+ *           // Enable paging for `info:column1`.
+ *           'column1 -> QualifiedColumn("info", "column1").withPaging(cellsPerPage = 100),
+ *           'column2 -> QualifiedColumn("info", "column2")))
+ * }}}
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+object HFileKijiOutput {
+
+  val TEMP_HFILE_OUTPUT_KEY = "kiji.tempHFileOutput"
+
+  /**
+   * A factory method for instantiating [[org.kiji.express.flow.framework.hfile.HFileKijiSource]]s
+   * used as sinks. This method permits specifying the full range of read options for each column.
+   * Values written will be tagged with the current time at write.
+   *
+   * @param tableUri that addresses a table in a Kiji instance.
+   * @param hFileOutput is the location where the resulting HFiles will be placed.
+   * @param columns is a mapping specifying what column to write each field value to.
+   * @return a source that can write tuple field values to columns of a Kiji table.
+   */
+  def apply(
+      tableUri: String,
+      hFileOutput: String,
+      columns: Map[Symbol, _ <: ColumnOutputSpec]
+  ): HFileKijiSource = {
+    new HFileKijiSource(
+        tableAddress = tableUri,
+        hFileOutput,
+        timeRange = All,
+        timestampField = None,
+        loggingInterval = 1000,
+        columns = columns)
+  }
+
+  /**
+   * A factory method for instantiating [[org.kiji.express.flow.framework.hfile.HFileKijiSource]]s
+   * used as sinks. This method permits specifying the full range of read options for each column.
+   * Values written will be tagged with the current time at write.
+   *
+   * @param tableUri that addresses a table in a Kiji instance.
+   * @param hFileOutput is the location where the resulting HFiles will be placed.
+   * @param columns is a mapping specifying what column to write each field value to.
+   * @return a source that can write tuple field values to columns of a Kiji table.
+   */
+  def apply(
+      tableUri: String,
+      hFileOutput: String,
+      timestampField: Symbol,
+      columns: (Symbol, String)*
+  ): HFileKijiSource = {
+
+    val columnMap = columns
+        .toMap
+        .mapValues(QualifiedColumnOutputSpec(_))
+    new HFileKijiSource(
+        tableAddress = tableUri,
+        hFileOutput = hFileOutput,
+        timeRange = All,
+        Some(timestampField),
+        loggingInterval = 1000,
+        columns = columnMap)
+  }
+
+  /**
+   * A factory method for instantiating [[org.kiji.express.flow.framework.hfile.HFileKijiSource]]s
+   * used as sinks. This method permits specifying the full range of read options for each column.
+   *
+   * @param tableUri that addresses a table in a Kiji instance.
+   * @param hFileOutput is the location where the resulting HFiles will be placed.
+   * @param columns is a mapping specifying what column to write each field value to.
+   * @param timestampField is the name of a tuple field that will contain cell timestamps when the
+   *     source is used for writing.
+   * @return a source that can write tuple field values to columns of a Kiji table.
+   */
+  def apply(
+      tableUri: String,
+      hFileOutput: String,
+      timestampField: Symbol,
+      columns: Map[Symbol, _ <: ColumnOutputSpec]
+  ): HFileKijiSource = {
+    require(timestampField != null)
+
+    new HFileKijiSource(
+        tableAddress = tableUri,
+        hFileOutput,
+        timeRange = All,
+        timestampField = Some(timestampField),
+        loggingInterval = 1000,
+        columns = columns)
+  }
+
+  /**
+   * A factory method for instantiating [[org.kiji.express.flow.framework.hfile.HFileKijiSource]]s
+   * used as sinks. Values written will be tagged with the current time at write.
+   *
+   * @param tableUri that addresses a table in a Kiji instance.
+   * @param hFileOutput is the location where the resulting HFiles will be placed.
+   * @param columns are a series of pairs mapping tuple field names to Kiji column names. When
+   *     naming columns, use the format `"family:qualifier"`.
+   * @return a source that can write tuple field values to columns of a Kiji table.
+   */
+  def apply(
+      tableUri: String,
+      hFileOutput: String,
+      columns: (Symbol, String)*
+  ): HFileKijiSource = {
+    val columnMap = columns
+        .toMap
+        .mapValues(QualifiedColumnOutputSpec(_))
+
+    HFileKijiOutput(tableUri, hFileOutput, columnMap)
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiScheme.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiScheme.scala
new file mode 100644
index 0000000000000000000000000000000000000000..f1b8f42cb2f4a77efd741edc93957cbb018916a4
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiScheme.scala
@@ -0,0 +1,296 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.hfile
+
+import cascading.flow.FlowProcess
+import cascading.scheme.NullScheme
+import cascading.scheme.SinkCall
+import cascading.tap.Tap
+import cascading.tuple.TupleEntry
+import com.google.common.base.Objects
+import org.apache.hadoop.hbase.HConstants
+import org.apache.hadoop.io.NullWritable
+import org.apache.hadoop.mapred.JobConf
+import org.apache.hadoop.mapred.OutputCollector
+import org.apache.hadoop.mapred.RecordReader
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.ColumnFamilyOutputSpec
+import org.kiji.express.flow.ColumnOutputSpec
+import org.kiji.express.flow.EntityId
+import org.kiji.express.flow.QualifiedColumnOutputSpec
+import org.kiji.express.flow.TimeRange
+import org.kiji.express.flow.framework.KijiScheme
+import org.kiji.express.flow.framework.KijiSourceContext
+import org.kiji.express.flow.util.Resources.doAndRelease
+import org.kiji.mapreduce.framework.HFileKeyValue
+import org.kiji.mapreduce.framework.KijiConfKeys
+import org.kiji.schema.EntityIdFactory
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiURI
+import org.kiji.schema.impl.DefaultKijiCellEncoderFactory
+import org.kiji.schema.layout.KijiTableLayout
+import org.kiji.schema.layout.impl.CellEncoderProvider
+import org.kiji.schema.layout.impl.ColumnNameTranslator
+
+ /**
+ * A Kiji-specific implementation of a Cascading `Scheme` which defines how to write data
+ * to HFiles.
+ *
+ * HFileKijiScheme is responsible for converting rows from a Kiji table that are input to a
+ * Cascading flow into Cascading tuples
+ * (see `source(cascading.flow.FlowProcess, cascading.scheme.SourceCall)`) and writing output
+ * data from a Cascading flow to an HFile capable of being bulk loaded into HBase
+ * (see `sink(cascading.flow.FlowProcess, cascading.scheme.SinkCall)`).
+ *
+ * HFileKijiScheme must be used with [[org.kiji.express.flow.framework.hfile.HFileKijiTap]],
+ * since it expects the Tap to have access to a Kiji table.
+ * [[org.kiji.express.flow.framework.hfile.HFileKijiSource]] handles the creation of both
+ * HFileKijiScheme and KijiTap in KijiExpress.
+ *
+ * @param timeRange to include from the Kiji table.
+ * @param timestampField is the optional name of a field containing the timestamp that all values
+ *     in a tuple should be written to.
+ *     Use None if all values should be written at the current time.
+ * @param loggingInterval to log skipped rows on. For example, if loggingInterval is 1000,
+ *     then every 1000th skipped row will be logged.
+ * @param columns mapping tuple field names to requests for Kiji columns.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+private[express] class HFileKijiScheme(
+  private[express] val timeRange: TimeRange,
+  private[express] val timestampField: Option[Symbol],
+  private[express] val loggingInterval: Long,
+  private[express] val columns: Map[String, ColumnOutputSpec])
+    extends HFileKijiScheme.HFileScheme {
+
+  import KijiScheme._
+  import HFileKijiScheme._
+
+  setSinkFields(buildSinkFields(columns, timestampField))
+
+  /**
+   * Sets up any resources required for the MapReduce job. This method is called
+   * on the cluster.
+   *
+   * @param flow is the current Cascading flow being run.
+   * @param sinkCall containing the context for this source.
+   */
+  override def sinkPrepare(
+    flow: FlowProcess[JobConf],
+    sinkCall: SinkCall[HFileKijiSinkContext, OutputCollector[HFileKeyValue, NullWritable]]) {
+    val conf = flow.getConfigCopy()
+    val uri = conf.get(KijiConfKeys.KIJI_OUTPUT_TABLE_URI)
+    val kijiURI = KijiURI.newBuilder(uri).build()
+    val kiji = Kiji.Factory.open(kijiURI)
+
+    doAndRelease(kiji.openTable(kijiURI.getTable)) { table: KijiTable =>
+      // Set the sink context to an opened KijiTableWriter.
+      val ctx = HFileKijiSinkContext(kiji, kijiURI,
+        table.getLayout, new ColumnNameTranslator(table.getLayout))
+      sinkCall.setContext(ctx)
+    }
+  }
+
+  /**
+   * Converts and writes a Cascading Tuple to a Kiji table. This method is called once
+   * for each row on the cluster.
+   *
+   * @param flow is the current Cascading flow being run.
+   * @param sinkCall containing the context for this source.
+   */
+  override def sink(
+    flow: FlowProcess[JobConf],
+    sinkCall: SinkCall[HFileKijiSinkContext, OutputCollector[HFileKeyValue, NullWritable]]) {
+
+    // Write the tuple out.
+    val output: TupleEntry = sinkCall.getOutgoingEntry
+
+    val HFileKijiSinkContext(kiji, uri, layout, colTranslator) = sinkCall.getContext()
+    val eidFactory = EntityIdFactory.getFactory(layout)
+
+    val encoderProvider = new CellEncoderProvider(uri, layout, kiji.getSchemaTable(),
+        DefaultKijiCellEncoderFactory.get())
+
+    outputCells(output, timestampField, columns) { key: HFileCell =>
+      // Convert cell to an HFileKeyValue
+      val kijiColumn = new KijiColumnName(key.colRequest.family, key.colRequest.qualifier)
+      val hbaseColumn = colTranslator.toHBaseColumnName(kijiColumn)
+      val cellSpec = layout.getCellSpec(kijiColumn)
+        .setSchemaTable(kiji.getSchemaTable())
+
+      val encoder = encoderProvider.getEncoder(kijiColumn.getFamily(), kijiColumn.getQualifier())
+      val hFileKeyValue = new HFileKeyValue(
+        key.entityId.toJavaEntityId(eidFactory).getHBaseRowKey(),
+        hbaseColumn.getFamily(), hbaseColumn.getQualifier(), key.timestamp,
+        encoder.encode(key.datum))
+
+      sinkCall.getOutput().collect(hFileKeyValue, NullWritable.get())
+    }
+  }
+
+  /**
+   * Cleans up any resources used during the MapReduce job. This method is called
+   * on the cluster.
+   *
+   * @param flow is the current Cascading flow being run.
+   * @param sinkCall containing the context for this source.
+   */
+  override def sinkCleanup(
+    flow: FlowProcess[JobConf],
+    sinkCall: SinkCall[HFileKijiSinkContext, OutputCollector[HFileKeyValue, NullWritable]]) {
+
+    val HFileKijiSinkContext(kiji, _, _, _) = sinkCall.getContext()
+
+    kiji.release()
+    sinkCall.setContext(null)
+
+  }
+
+  /**
+   * Sets any configuration options that are required for running a MapReduce job
+   * that writes to a Kiji table. This method gets called on the client machine
+   * during job setup.
+   *
+   * @param flow being built.
+   * @param tap that is being used with this scheme.
+   * @param conf to which we will add our KijiDataRequest.
+   */
+  override def sinkConfInit(
+    flow: FlowProcess[JobConf],
+    tap: Tap[JobConf, RecordReader[_, _], OutputCollector[HFileKeyValue, NullWritable]],
+    conf: JobConf) {
+  }
+
+
+  override def equals(other: Any): Boolean = {
+    other match {
+      case scheme: HFileKijiScheme => {
+        columns == scheme.columns &&
+          timestampField == scheme.timestampField &&
+          timeRange == scheme.timeRange
+      }
+      case _ => false
+    }
+  }
+
+
+  override def hashCode(): Int =
+    Objects.hashCode(columns, timeRange, timestampField, loggingInterval: java.lang.Long)
+}
+
+/**
+ * Private scheme that is a subclass of Cascading's NullScheme that doesn't do anything but
+ * sinks data. This is used in the secondary M/R job that takes intermediate HFile Key/Values
+ * from a sequence files and outputs them to the KijiHFileOutputFormat ultimately going to HFiles.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+private[express] final class SemiNullScheme extends HFileKijiScheme.HFileScheme {
+  /**
+   * Converts and writes a Cascading Tuple to a Kiji table. This method is called once
+   * for each row on the cluster.
+   *
+   * @param flow is the current Cascading flow being run.
+   * @param sinkCall containing the context for this source.
+   */
+  override def sink(
+    flow: FlowProcess[JobConf],
+    sinkCall: SinkCall[HFileKijiSinkContext, OutputCollector[HFileKeyValue, NullWritable]]) {
+
+    // Write the tuple out.
+    val output: TupleEntry = sinkCall.getOutgoingEntry
+
+    val hFileKeyValue = output.getObject(0).asInstanceOf[HFileKeyValue]
+    sinkCall.getOutput().collect(hFileKeyValue, NullWritable.get())
+  }
+}
+
+/**
+ * Context housing information necessary for the scheme to interact
+ * with the Kiji table.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] case class HFileKijiSinkContext (
+  kiji: Kiji,
+  kijiUri: KijiURI,
+  layout: KijiTableLayout,
+  columnTranslator: ColumnNameTranslator
+)
+
+/**
+ * A cell from a Kiji table containing some datum, addressed by a family, qualifier,
+ * and version timestamp.
+ *
+ * @param entityId of the Kiji table cell.
+ * @param colRequest identifying the location of the Kiji table cell.
+ * @param timestamp  of the Kiji table cell.
+ * @param datum in the Kiji table cell.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] case class HFileCell private[express] (
+  entityId: EntityId,
+  colRequest: QualifiedColumnOutputSpec,
+  timestamp: Long,
+  datum: AnyRef)
+
+object HFileKijiScheme {
+  type HFileScheme = NullScheme[JobConf, RecordReader[_, _],
+    OutputCollector[HFileKeyValue, NullWritable], KijiSourceContext, HFileKijiSinkContext]
+
+  private[express] def outputCells(output: TupleEntry,
+                                   timestampField: Option[Symbol],
+                                   columns: Map[String, ColumnOutputSpec])(
+                                     cellHandler: HFileCell => Unit) {
+
+    // Get a timestamp to write the values to, if it was specified by the user.
+    val timestamp: Long = timestampField match {
+      case Some(field) => output.getObject(field.name).asInstanceOf[Long]
+      case None        => HConstants.LATEST_TIMESTAMP
+    }
+
+    // Get the entityId.
+    val entityId: EntityId =
+      output.getObject(KijiScheme.entityIdField).asInstanceOf[EntityId]
+
+    columns.foreach(kv => {
+      val (fieldName, colRequest) = kv
+      val colValue = output.getObject(fieldName)
+      val newColRequest = colRequest match {
+        case cf @ ColumnFamilyOutputSpec(family, qualField, schemaId) => {
+          val qualifier = output.getString(qualField.name)
+          QualifiedColumnOutputSpec(family, qualifier)
+        }
+        case qc @ QualifiedColumnOutputSpec(_, _, _) => qc
+      }
+      val cell = HFileCell(entityId, newColRequest, timestamp, colValue)
+      cellHandler(cell)
+    })
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiSource.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiSource.scala
new file mode 100644
index 0000000000000000000000000000000000000000..26932a5d445360bd44cb52558c696fc79a89d362
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiSource.scala
@@ -0,0 +1,163 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.hfile
+
+import java.lang.UnsupportedOperationException
+
+import cascading.tap.Tap
+import com.google.common.base.Objects
+import com.twitter.scalding.AccessMode
+import com.twitter.scalding.HadoopTest
+import com.twitter.scalding.Hdfs
+import com.twitter.scalding.Mode
+import com.twitter.scalding.Source
+import com.twitter.scalding.Write
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.express.flow.All
+import org.kiji.express.flow.ColumnOutputSpec
+import org.kiji.express.flow.TimeRange
+import org.kiji.express.flow.framework.KijiScheme
+
+/**
+ * A read or write view of a Kiji table.
+ *
+ * A Scalding `Source` provides a view of a data source that can be read as Scalding tuples. It
+ * is comprised of a Cascading tap [[cascading.tap.Tap]], which describes where the data is and how
+ * to access it, and a Cascading Scheme [[cascading.scheme.Scheme]], which describes how to read
+ * and interpret the data.
+ *
+ * An `HFileKijiSource` should never be used for reading.  It is intended to be used only in
+ * [[org.kiji.express.flow.framework.hfile.HFileKijiJob]], for writing out to HFiles formatted for
+ * bulk-loading into Kiji.
+ *
+ * When writing to a Kiji table, a `HFileKijiSource` views a Kiji table as a collection of tuples
+ * that correspond to cells from the Kiji table. Each tuple to be written must provide a cell
+ * address by specifying a Kiji `EntityID` in the tuple field `entityId`, a value to be written in a
+ * configurable field, and (optionally) a timestamp in a configurable field.
+ *
+ * End-users cannot directly obtain instances of `KijiSource`. Instead,
+ * they should use the factory methods provided as part of the
+ * [[org.kiji.express.flow.framework.hfile]] module.
+ *
+ * @param tableAddress is a Kiji URI addressing the Kiji table to read or write to.
+ * @param timeRange that cells read must belong to. Ignored when the source is used to write.
+ * @param timestampField is the name of a tuple field that will contain cell timestamp when the
+ *     source is used for writing. Specify `None` to write all
+ *     cells at the current time.
+ * @param loggingInterval The interval at which to log skipped rows.
+ * @param columns is a one-to-one mapping from field names to Kiji columns. When reading,
+ *     the columns in the map will be read into their associated tuple fields. When
+ *     writing, values from the tuple fields will be written to their associated column.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+class HFileKijiSource private[express] (
+    val tableAddress: String,
+    val hFileOutput: String,
+    val timeRange: TimeRange,
+    val timestampField: Option[Symbol],
+    val loggingInterval: Long,
+    val columns: Map[Symbol, ColumnOutputSpec]
+) extends Source {
+  import org.kiji.express.flow.KijiSource._
+
+  /**
+   * Creates a Scheme that writes to/reads from a Kiji table for usage with
+   * the hadoop runner.
+   */
+  override val hdfsScheme: KijiScheme.HadoopScheme =
+    new HFileKijiScheme(timeRange, timestampField, loggingInterval, convertKeysToStrings(columns))
+      // This cast is required due to Scheme being defined with invariant type parameters.
+      .asInstanceOf[KijiScheme.HadoopScheme]
+
+  /**
+   * Create a connection to the physical data source (also known as a Tap in Cascading)
+   * which, in this case, is a [[org.kiji.schema.KijiTable]].
+   *
+   * @param readOrWrite Specifies if this source is to be used for reading or writing.
+   * @param mode Specifies which job runner/flow planner is being used.
+   * @return A tap to use for this data source.
+   */
+  override def createTap(readOrWrite: AccessMode)(implicit mode: Mode): Tap[_, _, _] = {
+    val tap: Tap[_, _, _] = mode match {
+      // Production taps.
+      case Hdfs(_, _) => new HFileKijiTap(tableAddress, hdfsScheme, hFileOutput)
+
+      // Test taps.
+      case HadoopTest(conf, buffers) => {
+        readOrWrite match {
+          case Write => {
+            new HFileKijiTap(tableAddress, hdfsScheme, hFileOutput)
+          }
+          case _ => throw new UnsupportedOperationException("Read unsupported")
+        }
+      }
+      // Delegate any other tap types to Source's default behaviour.
+      case _ => super.createTap(readOrWrite)(mode)
+    }
+
+    return tap
+  }
+
+  override def toString: String = {
+    Objects
+        .toStringHelper(this)
+        .add("tableAddress", tableAddress)
+        .add("timeRange", timeRange)
+        .add("timestampField", timestampField)
+        .add("loggingInterval", loggingInterval)
+        .add("columns", columns)
+        .toString
+  }
+
+  override def equals(other: Any): Boolean = {
+    other match {
+      case source: HFileKijiSource => {
+        Objects.equal(tableAddress, source.tableAddress) &&
+        Objects.equal(hFileOutput, source.hFileOutput) &&
+        Objects.equal(columns, source.columns) &&
+        Objects.equal(timestampField, source.timestampField) &&
+        Objects.equal(timeRange, source.timeRange)
+      }
+      case _ => false
+    }
+  }
+
+  override def hashCode(): Int =
+      Objects.hashCode(tableAddress, hFileOutput, columns, timestampField, timeRange)
+}
+
+/**
+ * Private Scalding source implementation whose scheme is the SemiNullScheme that
+ * simply sinks tuples to an output for later writing to HFiles.
+ */
+private final class HFileSource(
+    override val tableAddress: String,
+    override val hFileOutput: String
+) extends HFileKijiSource(tableAddress, hFileOutput, All, None, 0, Map()) {
+  /**
+   * Creates a Scheme that writes to/reads from a Kiji table for usage with
+   * the hadoop runner.
+   */
+  override val hdfsScheme: KijiScheme.HadoopScheme =
+      new SemiNullScheme().asInstanceOf[KijiScheme.HadoopScheme]
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiTap.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiTap.scala
new file mode 100644
index 0000000000000000000000000000000000000000..541a0ff9d699931432e8ffbb351eb15b93a302fd
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiTap.scala
@@ -0,0 +1,179 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.hfile
+
+import java.util.UUID
+
+import cascading.flow.FlowProcess
+import cascading.scheme.Scheme
+import cascading.tap.Tap
+import cascading.tap.hadoop.io.HadoopTupleEntrySchemeCollector
+import cascading.tuple.TupleEntryCollector
+import cascading.tuple.TupleEntryIterator
+import com.twitter.elephantbird.mapred.output.DeprecatedOutputFormatWrapper
+import org.apache.hadoop.fs.Path
+import org.apache.hadoop.mapred.FileOutputFormat
+import org.apache.hadoop.mapred.JobConf
+import org.apache.hadoop.mapred.OutputCollector
+import org.apache.hadoop.mapred.RecordReader
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.express.flow.framework.KijiKey
+import org.kiji.express.flow.framework.KijiScheme
+import org.kiji.express.flow.framework.KijiValue
+import org.kiji.mapreduce.framework.KijiConfKeys
+import org.kiji.mapreduce.impl.HFileWriterContext
+import org.kiji.mapreduce.output.framework.KijiHFileOutputFormat
+
+/**
+ * A Kiji-specific implementation of a Cascading `Tap`, which defines how data is to be read from
+ * and written to a particular endpoint. This implementation only handles writing to Kiji
+ * formatted HFiles to be bulk loaded into HBase.
+ *
+ * HFileKijiTap must be used with [[org.kiji.express.flow.framework.hfile.HFileKijiScheme]]
+ * to perform decoding of cells in a Kiji table. [[org.kiji.express.flow.KijiSource]] handles
+ * the creation of both HFileKijiScheme and HFileKijiTap in KijiExpress.
+ *
+ * @param tableUri of the Kiji table to read or write from.
+ * @param scheme that will convert data read from Kiji into Cascading's tuple model.
+ * @param hFileOutput is the location where the HFiles will be written to.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+private[express] class HFileKijiTap(
+  private val tableUri: String,
+  private val scheme: KijiScheme.HadoopScheme,
+  private val hFileOutput: String)
+    extends Tap[JobConf, RecordReader[KijiKey, KijiValue], OutputCollector[_, _]](
+        scheme.asInstanceOf[Scheme[JobConf, RecordReader[KijiKey, KijiValue],
+            OutputCollector[_, _], _, _]]) {
+
+  /** Unique identifier for this KijiTap instance. */
+  private val id: String = UUID.randomUUID().toString()
+
+  /**
+   * Provides a string representing the resource this `Tap` instance represents.
+   *
+   * @return a java UUID representing this KijiTap instance. Note: This does not return the uri of
+   *     the Kiji table being used by this tap to allow jobs that read from or write to the same
+   *     table to have different data request options.
+   */
+  override def getIdentifier(): String = id
+
+
+  /**
+   * Opens any resources required to read from a Kiji table.
+   *
+   * @param flow being run.
+   * @param recordReader that will read from the desired Kiji table.
+   * @return an iterator that reads rows from the desired Kiji table.
+   */
+  override def openForRead(
+      flow: FlowProcess[JobConf],
+      recordReader: RecordReader[KijiKey, KijiValue]): TupleEntryIterator = {
+    null
+  }
+
+  /**
+   * Opens any resources required to write from a Kiji table.
+   *
+   * @param flow being run.
+   * @param outputCollector that will write to the desired Kiji table.
+   *
+   * @return a collector that writes tuples to the desired Kiji table.
+   */
+  override def openForWrite(
+      flow: FlowProcess[JobConf],
+      outputCollector: OutputCollector[_, _]): TupleEntryCollector = {
+
+    new HadoopTupleEntrySchemeCollector(
+        flow,
+        this.asInstanceOf[Tap[JobConf, RecordReader[_, _], OutputCollector[_, _]]],
+        outputCollector)
+  }
+  /**
+   * Builds any resources required to read from or write to a Kiji table.
+   *
+   * Note: KijiExpress currently does not support automatic creation of Kiji tables.
+   *
+   * @param conf containing settings for this flow.
+   * @return true if required resources were created successfully.
+   * @throws UnsupportedOperationException always.
+   */
+  override def createResource(conf: JobConf): Boolean = {
+    throw new UnsupportedOperationException("KijiTap does not support creating tables for you.")
+  }
+
+  /**
+   * Deletes any unnecessary resources used to read from or write to a Kiji table.
+   *
+   * Note: KijiExpress currently does not support automatic deletion of Kiji tables.
+   *
+   * @param conf containing settings for this flow.
+   * @return true if superfluous resources were deleted successfully.
+   * @throws UnsupportedOperationException always.
+   */
+  override def deleteResource(conf: JobConf): Boolean = {
+    throw new UnsupportedOperationException("KijiTap does not support deleting tables for you.")
+  }
+
+  /**
+   * Gets the time that the target Kiji table was last modified.
+   *
+   * Note: This will always return the current timestamp.
+   *
+   * @param conf containing settings for this flow.
+   * @return the current time.
+   */
+  override def getModifiedTime(conf: JobConf): Long = System.currentTimeMillis()
+
+  /**
+   * Determines if the Kiji table this `Tap` instance points to exists.
+   *
+   * @param conf containing settings for this flow.
+   * @return true if the target Kiji table exists.
+   */
+  override def resourceExists(conf: JobConf): Boolean = {
+    true
+  }
+
+  /**
+   * Sets any configuration options that are required for running a MapReduce job
+   * that writes to a Kiji table. This method gets called on the client machine
+   * during job setup.
+   *
+   * @param flow being built.
+   * @param conf to which we will add the table uri.
+   */
+  override def sinkConfInit(flow: FlowProcess[JobConf], conf: JobConf) {
+
+    FileOutputFormat.setOutputPath(conf, new Path(hFileOutput))
+    DeprecatedOutputFormatWrapper.setOutputFormat(classOf[KijiHFileOutputFormat], conf)
+    val hfContext = classOf[HFileWriterContext].getName()
+    conf.set(KijiConfKeys.KIJI_TABLE_CONTEXT_CLASS, hfContext)
+    // Store the output table.
+    conf.set(KijiConfKeys.KIJI_OUTPUT_TABLE_URI, tableUri)
+
+    super.sinkConfInit(flow, conf)
+  }
+}
+
+
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/package.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/package.scala
new file mode 100644
index 0000000000000000000000000000000000000000..00f6b37459919c687d3604ecc5095755267a4fd3
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/hfile/package.scala
@@ -0,0 +1,35 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+/**
+ * Provides the necessary implementation to allow Scalding jobs to write directly to HFiles. To
+ * support this, simply subclass the HFileKijiJob. The only requirement is that you provide
+ * a command line argument hFileOutput which references the location on HDFS where the final
+ * HFiles are to be stored.
+ *
+ * Under the covers, Scalding will be configured to write to HFiles either by manipulating the
+ * current Cascading flow if the defined flow has no reducers OR by redirecting output to a
+ * temporary location where a secondary job will re-order the results into something that
+ * HBase can load. The reason for this is because KeyValues must be sorted according to a
+ * TotalOrderPartitioner that relies on the region splits of HBase so that KeyValues are placed
+ * in the right region.
+ */
+package object hfile
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/package.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/package.scala
new file mode 100644
index 0000000000000000000000000000000000000000..9c490cb3582d360d835f4f5efdb38eb8642c78ff
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/package.scala
@@ -0,0 +1,27 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+/**
+ * Package containing supporting classes and objects for the KijiExpress flow API. Includes
+ * components required to integrate with Hadoop, Cascading, and Scalding. The classes in this
+ * package should only be used for developing other Kiji platform projects.
+ */
+package object framework
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/AvroSerializer.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/AvroSerializer.scala
new file mode 100644
index 0000000000000000000000000000000000000000..6495eaa5a6197ee3eee63f23e24b967521488c26
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/AvroSerializer.scala
@@ -0,0 +1,153 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.serialization
+
+import com.esotericsoftware.kryo.Kryo
+import com.esotericsoftware.kryo.Serializer
+import com.esotericsoftware.kryo.io.Input
+import com.esotericsoftware.kryo.io.Output
+import org.apache.avro.Schema
+import org.apache.avro.generic.GenericContainer
+import org.apache.avro.generic.GenericDatumReader
+import org.apache.avro.generic.GenericDatumWriter
+import org.apache.avro.io.DecoderFactory
+import org.apache.avro.io.EncoderFactory
+import org.apache.avro.specific.SpecificDatumReader
+import org.apache.avro.specific.SpecificDatumWriter
+import org.apache.avro.specific.SpecificRecord
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+
+/**
+ * Provides serialization for Avro schemas while using Kryo serialization.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+// TODO (EXP-295): Should these maybe be Framework?
+class AvroSchemaSerializer
+    extends Serializer[Schema] {
+  setAcceptsNull(false)
+
+  override def write(
+      kryo: Kryo,
+      output: Output,
+      schema: Schema
+  ) {
+    val encodedSchema = schema.toString(false)
+    output.writeString(encodedSchema)
+  }
+
+  override def read(
+      kryo: Kryo,
+      input: Input,
+      klazz: Class[Schema]
+  ): Schema = {
+    val encodedSchema = input.readString()
+    new Schema.Parser().parse(encodedSchema)
+  }
+}
+
+/**
+ * Provides serialization for Avro generic records while using Kryo serialization. Record schemas
+ * are prepended to the encoded generic record data.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+// TODO (EXP-295): Should these maybe be Framework?
+class AvroGenericSerializer
+    extends Serializer[GenericContainer] {
+  // TODO(EXP-269): Cache encoders per schema.
+
+  // We at least need an avro schema to perform serialization.
+  setAcceptsNull(false)
+
+  override def write(
+      kryo: Kryo,
+      output: Output,
+      avroObject: GenericContainer
+  ) {
+    // Serialize the schema.
+    new AvroSchemaSerializer().write(kryo, output, avroObject.getSchema)
+
+    // Serialize the data.
+    val writer = new GenericDatumWriter[GenericContainer](avroObject.getSchema)
+    val encoder = EncoderFactory
+        .get()
+        .directBinaryEncoder(output, null)
+    writer.write(avroObject, encoder)
+  }
+
+  override def read(
+      kryo: Kryo,
+      input: Input,
+      klazz: Class[GenericContainer]
+  ): GenericContainer = {
+    // Deserialize the schema.
+    val schema = new AvroSchemaSerializer().read(kryo, input, null)
+
+    // Deserialize the data.
+    val reader = new GenericDatumReader[GenericContainer](schema)
+    val decoder = DecoderFactory
+        .get()
+        .directBinaryDecoder(input, null)
+    reader.read(null.asInstanceOf[GenericContainer], decoder)
+  }
+}
+
+/**
+ * Provides serialization for Avro specific records while using Kryo serialization. Record schemas
+ * are not serialized as all clients interacting with this data are assumed to have the correct
+ * specific record class on their classpath.
+ */
+final class AvroSpecificSerializer
+    extends Serializer[SpecificRecord] {
+  // TODO(EXP-269) Cache encoders per class/schema.
+
+  setAcceptsNull(false)
+
+  override def write(
+      kryo: Kryo,
+      output: Output,
+      record: SpecificRecord
+  ) {
+    val writer =
+        new SpecificDatumWriter[SpecificRecord](record.getClass.asInstanceOf[Class[SpecificRecord]])
+    val encoder = EncoderFactory
+        .get()
+        .directBinaryEncoder(output, null)
+    writer.write(record, encoder)
+  }
+
+  override def read(
+      kryo: Kryo,
+      input: Input,
+      klazz: Class[SpecificRecord]
+  ): SpecificRecord = {
+    val reader = new SpecificDatumReader[SpecificRecord](klazz)
+    val decoder = DecoderFactory
+        .get()
+        .directBinaryDecoder(input, null)
+    reader.read(null.asInstanceOf[SpecificRecord], decoder)
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/KijiLocker.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/KijiLocker.scala
new file mode 100644
index 0000000000000000000000000000000000000000..48e3078d0f9e641852a8b274e3fe30141a0c60cd
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/KijiLocker.scala
@@ -0,0 +1,93 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.serialization
+
+import com.esotericsoftware.kryo.io.Output
+import com.twitter.bijection.Injection
+import com.twitter.chill.KryoBase
+import com.twitter.chill.KryoInjectionInstance
+import org.objenesis.strategy.StdInstantiatorStrategy
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+
+/**
+ * Provides a constructor function for a
+ * [[org.kiji.express.flow.framework.serialization.KijiLocker]].
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+// TODO (EXP-295): Should these maybe be Framework?
+object KijiLocker {
+  def apply[T <: AnyRef](t: T): KijiLocker[T] = new KijiLocker(t)
+}
+
+/**
+ * A clone of Chill's [[com.twitter.chill.MeatLocker]] with serialization provided by our custom
+ * [[org.kiji.express.flow.framework.serialization.KryoKiji]].
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+// TODO (EXP-295): Should these maybe be Framework?
+class KijiLocker[T <: AnyRef](@transient private var t: T) extends java.io.Serializable {
+
+  /**
+   * Creates an [[com.twitter.bijection.Injection]] for converting objects between their
+   * serialized byte for and back.
+   *
+   * @return an Object <-> Array[Byte] Injection.
+   */
+  private def injection: Injection[AnyRef, Array[Byte]] = {
+    val kryo = {
+      val kryo = new KryoBase
+      kryo.setRegistrationRequired(false)
+      kryo.setInstantiatorStrategy(new StdInstantiatorStrategy)
+      new KryoKiji().decorateKryo(kryo)
+      kryo
+    }
+    new KryoInjectionInstance(kryo, new Output( 1 << 10, 1 << 24))
+  }
+
+  /**
+   * Serialized value of t.
+   */
+  private val tBytes: Array[Byte] = injection(t)
+
+  /**
+   * Retrieve the value wrapped by this
+   * [[org.kiji.express.flow.framework.serialization.KijiLocker]].
+   *
+   * @return the value
+   */
+  def get: T = {
+    if(t == null) {
+      // we were serialized
+      t = injection
+          .invert(tBytes)
+          .getOrElse(throw new RuntimeException("Deserialization failed in KijiLocker."))
+          .asInstanceOf[T]
+    }
+    t
+  }
+}
+
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/KryoKiji.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/KryoKiji.scala
new file mode 100644
index 0000000000000000000000000000000000000000..becada6c16fd8156e5b1eb3355edbd1ed7030613
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/KryoKiji.scala
@@ -0,0 +1,47 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.serialization
+
+import com.esotericsoftware.kryo.Kryo
+import com.twitter.scalding.serialization.KryoHadoop
+import org.apache.avro.Schema
+import org.apache.avro.generic.GenericContainer
+import org.apache.avro.specific.SpecificRecord
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+
+/**
+ * Kryo specification that adds avro schema, generic record, and specific record serialization
+ * support. Used with [[org.kiji.express.flow.KijiJob]].
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+class KryoKiji extends KryoHadoop {
+  override def decorateKryo(kryo: Kryo) {
+    super.decorateKryo(kryo)
+
+    kryo.addDefaultSerializer(classOf[Schema], classOf[AvroSchemaSerializer])
+    kryo.addDefaultSerializer(classOf[GenericContainer], classOf[AvroGenericSerializer])
+    kryo.addDefaultSerializer(classOf[SpecificRecord], classOf[AvroSpecificSerializer])
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/package.scala b/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/package.scala
new file mode 100644
index 0000000000000000000000000000000000000000..4624b13d29eca1976172231617ded1d99a66c14d
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization/package.scala
@@ -0,0 +1,26 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+/**
+ * Package containing utility classes related to serialization of data during a KijiExpress flow.
+ * Includes components for integrating avro and kryo serialization.
+ */
+package object serialization
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/package.scala b/kiji-express/src/main/scala/org/kiji/express/flow/package.scala
new file mode 100644
index 0000000000000000000000000000000000000000..36892235e38b869841ec6c7c9be7b2ffab8d6c1c
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/package.scala
@@ -0,0 +1,186 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express
+
+import org.apache.hadoop.hbase.HConstants
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.schema.KConstants
+import org.kiji.schema.KijiInvalidNameException
+import org.kiji.schema.filter.KijiColumnFilter
+import org.kiji.schema.filter.RegexQualifierColumnFilter
+
+/**
+ * Module providing the ability to write Scalding using data stored in Kiji tables.
+ *
+ * KijiExpress users should import the members of this module to gain access to factory
+ * methods that produce [[org.kiji.express.flow.KijiSource]]s that can perform data processing
+ * operations that read from or write to Kiji tables.
+ * {{{
+ *   import org.kiji.express.flow._
+ * }}}
+ *
+ * === Reading from columns and map-type column families. ===
+ * Specify columns to read from a Kiji table using instances of the
+ * [[org.kiji.express.flow.QualifiedColumnInputSpec]] and
+ * [[org.kiji.express.flow.ColumnFamilyInputSpec]] classes, which contain fields for specifying
+ * the names of the columns to read, as well as what data to read back (e.g., only the latest
+ * version of a cell, or a certain number of recent versions) and how it is read back (e.g., using
+ * paging to limit the amount of data in memory).
+ *
+ * Specify a fully-qualified column with an instance of `QualifiedColumnInputSpec`.  Below are
+ * several examples for specifying the column `info:name`:
+ * {{{
+ *   // Request the latest cell.
+ *   val myInputColumn = QualifiedColumnInputSpec("info", "name")
+ *   val myInputColumn = QualifiedColumnInputSpec("info", "name", maxVersions = latest)
+ *   val myInputColumn = QualifiedColumnInputSpec("info", "name", maxVersions = 1)
+ *
+ *   // Request every cell.
+ *   val myInputColumn = QualifiedColumnInputSpec("info:name", maxVersions = all)
+ *
+ *   // Request the 10 most recent cells.
+ *   val myInputColumn = QualifiedColumnInputSpec("info:name", maxVersions = 10)
+ * }}}
+ *
+ * To request cells from all of the columns in a family, use the `ColumnFamilyInputSpec`
+ * class, which, like `QualifiedColumnInputSpec`, provides options on the input spec such as
+ * the maximum number of cell versions to return, filters to use, etc.  A user can
+ * specify a filter, for example, to specify a regular expression such that a column in the family
+ * will only be retrieved if its qualifier matches the regular expression:
+ * {{{
+ *   // Gets the most recent cell for all columns in the column family "searches".
+ *   var myFamilyInput = ColumnFamilyInputSpec("searches")
+ *
+ *   // Gets all cells for all columns in the column family "searches" whose
+ *   // qualifiers contain the word "penguin".
+ *   myFamilyInput = ColumnFamilyInputSpec(
+ *      "searches",
+ *      filter = Some(new RegexQualifierColumnFilter(""".*penguin.*""")),
+ *      maxVersions = all)
+ *
+ *   // Gets all cells for all columns in the column family "searches".
+ *   myFamilyInput = ColumnFamilyInputSpec("searches", maxVersions = all)
+ * }}}
+ *
+ * See [[org.kiji.express.flow.QualifiedColumnInputSpec]] and
+ * [[org.kiji.express.flow.ColumnFamilyInputSpec]] for a full list of options for column input
+ * specs.
+ *
+ * When specifying a column for writing, the user can likewise use the
+ * `QualifiedColumnOutputSpec` and `ColumnFamilyOutputSpec` classes to indicate the name of
+ * the column and any options.  The following, for example, specifies a column to use for writes
+ * with the default reader schema:
+ * {{{
+ *   // Create a column output spec for writing to "info:name" using the default reader schema
+ *   var myWriteReq = QualifiedColumnOutputSpec("info", "name", useDefaultReaderSchema = true)
+ * }}}
+ *
+ *
+ * When writing to a family, you specify a Scalding field that contains the name of the qualifier to
+ * use for your write.  For example, to use the value in the Scalding field ``'terms`` as the name
+ * of the column qualifier, use the following:
+ * {{{
+ *   var myOutputFamily = ColumnFamilyOutputSpec("searches", 'terms)
+ * }}}
+ *
+ * See [[org.kiji.express.flow.QualifiedColumnOutputSpec]] and
+ * [[org.kiji.express.flow.ColumnFamilyOutputSpec]] for a full list of options for column output
+ * specs.
+ *
+ * === Getting input from a Kiji table. ===
+ * The factory `KijiInput` can be used to obtain a
+ * [[org.kiji.express.flow.KijiSource]] to process rows from the table (represented as tuples)
+ * using various operations. When using `KijiInput`, users specify a table (using a Kiji URI) and
+ * use column specs and other options to control how data is read from Kiji into tuple fields.
+ * ``KijiInput`` contains different factories that allow for abbreviated column specifications,
+ * as illustrated in the examples below:
+ * {{{
+ *   // Read the most recent cells from columns "info:id" and "info:name" into tuple fields "id"
+ *   // and "name" (don't explicitly instantiate a QualifiedColumnInputSpec).
+ *   var myKijiSource =
+ *       KijiInput("kiji://.env/default/newsgroup_users", "info:id" -> 'id, "info:name" -> 'name)
+ *
+ *   // Read only cells from "info:id" that occurred before Unix time 100000.
+ *   // (Don't explicitly instantiate a QualifiedColumnInputSpec)
+ *   myKijiSource =
+ *       KijiInput("kiji://.env/default/newsgroup_users", Before(100000), "info:id" -> 'id)
+ *
+ *   // Read all versions from "info:posts"
+ *   myKijiSource = KijiInput(
+ *       "kiji://.env/default/newsgroup_users",
+ *       Map(QualifiedColumnOutputSpec("info", "id", maxVersions = all) -> 'id))
+ * }}}
+ *
+ * See [[org.kiji.express.flow.KijiInput]] and [[org.kiji.express.flow.ColumnInputSpec]] for more
+ * information on how to create and use time ranges for requesting data.
+ *
+ * === Writing to a Kiji table. ===
+ * Data from any Cascading `Source` can be written to a Kiji table. Tuples to be written to a
+ * Kiji table must have a field named `entityId` which contains an entity id for a row in a Kiji
+ * table. The contents of a tuple field can be written as a cell at the most current timestamp to
+ * a column in a Kiji table. To do so, you specify a mapping from tuple field names to qualified
+ * Kiji table column names.
+ * {{{
+ *   // Write from the tuple field "average" to the column "stats:average" of the Kiji table
+ *   // "newsgroup_users".
+ *   mySource.write("kiji://.env/default/newsgroup_users", 'average -> "stats:average")
+ *
+ *   // Create a KijiSource to write the data in tuple field "results" to column family
+ *   // "searches" with the string in tuple field "terms" as the column qualifier.
+ *   myOutput = KijiOutput(
+ *       "kiji://.env/default/searchstuff",
+ *       'results -> ColumnFamilyOutputSpec("searches", "terms"))
+ * }}}
+ *
+ * === Specifying ranges of time. ===
+ * Instances of [[org.kiji.express.flow.TimeRange]] are used to specify a range of timestamps
+ * that should be retrieved when reading data from Kiji. There are five implementations of
+ * `TimeRange` that can be used when requesting data.
+ *
+ * <ul>
+ *   <li>All</li>
+ *   <li>At(timestamp: Long)</li>
+ *   <li>After(begin: Long)</li>
+ *   <li>Before(end: Long)</li>
+ *   <li>Between(begin: Long, end: Long)</li>
+ * </ul>
+ *
+ * These implementations can be used with [[org.kiji.express.flow.KijiInput]] to specify a range
+ * that a Kiji cell's timestamp must be in to be retrieved. For example,
+ * to read cells from the column `info:word` that have timestamps between `0L` and `10L`,
+ * you can do the following.
+ *
+ * @example {{{
+ *     KijiInput("kiji://.env/default/words", timeRange=Between(0L, 10L), "info:word" -> 'word)
+ * }}}
+ */
+package object flow {
+
+  /** Used with a column input spec to indicate that all cells of a column should be retrieved. */
+  val all = HConstants.ALL_VERSIONS
+
+  /**
+   * Used with a column input spec to indicate that only the latest cell of a column should be
+   * retrieved.
+   */
+  val latest = 1
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/tool/TmpJarsTool.scala b/kiji-express/src/main/scala/org/kiji/express/flow/tool/TmpJarsTool.scala
new file mode 100644
index 0000000000000000000000000000000000000000..21943c07136a6864b6f1d5a3900f150009d34a5f
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/tool/TmpJarsTool.scala
@@ -0,0 +1,149 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.tool
+
+import java.io.File
+
+import org.apache.hadoop.hbase.HBaseConfiguration
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.mapreduce.util.Jars
+
+/**
+ * Reads a colon-separated list of classpath entries and outputs comma-separated list of
+ * URIs to jar files accessible from that classpath, as well as the HBase library jars.
+ *
+ * This tool is meant to be used to specify a collection of jars that should be included on the
+ * distributed cache of Hadoop jobs run through KijiExpress. It should be used from the `express`
+ * script and should be passed a classpath entry for the KijiExpress `lib` directory as well
+ * as any classpath entries specified by the user through `EXPRESS_CLASSPATH`.
+ *
+ * Classpath entries can come in one of three forms:
+ *
+ * 1. A path to a specific file. In this case we send the file through the distributed cache if
+ * it is a jar.
+ * 2. A path to a directory. Since directory entries do not include jars on the classpath,
+ * we send no jars through the distributed cache.
+ * 3. A path to a directory with the wildcard * appended. In this case all jars under the
+ * specified directory are sent through the distributed cache.
+ *
+ * This tool accepts one command line argument: a string containing a colon-separated list of
+ * classpath entries.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+@Inheritance.Sealed
+object TmpJarsTool {
+
+  /**
+   * Constructs a file from a classpath entry.
+   *
+   * @param entry the file will be constructed from.
+   * @return a file for the classpath entry.
+   */
+  private[tool] def entryToFile(entry: String): File = new File(entry)
+
+  /**
+   * If a file is a glob entry, transform it into all files present in the globed directory.
+   *
+   * @param maybeGlob is a file that might be a glob entry from the classpath.
+   * @return the original file if it was not a glob, the files present in the globed directory
+   *     otherwise.
+   */
+  private[tool] def globToFiles(maybeGlob: File): Array[File] = maybeGlob.getName match {
+    case "*" => maybeGlob.getParentFile
+                .listFiles()
+                .filterNot { _.isDirectory }
+    case _ => Array(maybeGlob)
+  }
+
+  /**
+   * Determines if a file has the `.jar` or `.JAR` extensions.
+   *
+   * @param maybeJar is a file that may, in fact, be a jar.
+   * @return `true` if the file is jar, `false` otherwise.
+   */
+  private[tool] def isJar(maybeJar: File): Boolean = {
+    maybeJar.getName.endsWith(".jar") || maybeJar.getName.endsWith(".JAR")
+  }
+
+  /**
+   * Transforms a colon-separated list of classpath entries into the jar files accessible from
+   * the classpath entries.
+   *
+   * @param classpath is a colon-separated list of classpath entries.
+   * @return the jar files accessible from the classpath entries.
+   */
+  private[tool] def getJarsFromClasspath(classpath: String): Array[File] = {
+    classpath
+        .split(':')
+        .filterNot { _.isEmpty }
+        .map { entryToFile }
+        .filterNot { _.isDirectory }
+        .flatMap { globToFiles }
+        .filter { isJar }
+  }
+
+  /**
+   * Retrieves all jars located in the directory containing the HBase library jar.
+   *
+   * @return the jar files contained in the HBase lib directory.
+   */
+  private def getJarsForHBase: Array[File] = {
+    val hbaseJar: File = new File(Jars.getJarPathForClass(classOf[HBaseConfiguration]))
+    hbaseJar.getParentFile.listFiles()
+        .filterNot { _.isDirectory }
+        .filter { isJar }
+  }
+
+  /**
+   * Formats the paths to jar files as a comma-separated list of URIs to files on the local file
+   * system.
+   *
+   * @param jars that should be added to the generated comma-separated list.
+   * @return a comma-separated list of URIs to jar files.
+   */
+  private[tool] def toJarURIList(jars: Array[File]): String = {
+    val pathToPathWithScheme = (path: String) => "file://" + path
+    val joinPaths = (path1: String, path2: String) => path1 + "," + path2
+    jars
+        .map { _.getCanonicalPath }
+        .map { pathToPathWithScheme }
+        .reduce { joinPaths }
+  }
+
+  /**
+   * Transforms a classpath into a comma-separated list of URIs to jar files accessible from the
+   * classpath.
+   *
+   * @param args from the command line, which should only include a colon-separated classpath.
+   */
+  def main(args: Array[String]) {
+    if (args.length != 1) {
+      println("Usage: TmpJarsTool <classpath>")
+    }
+    val jarsFromClasspath = getJarsFromClasspath(args(0))
+    val jarsFromHBase = getJarsForHBase
+    val jarsForDCache = jarsFromClasspath ++ jarsFromHBase
+    println(toJarURIList(jarsForDCache))
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/tool/package.scala b/kiji-express/src/main/scala/org/kiji/express/flow/tool/package.scala
new file mode 100644
index 0000000000000000000000000000000000000000..3a1d57eb2ee2f5f464d92ccba75687b65d0e7517
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/tool/package.scala
@@ -0,0 +1,25 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+/**
+ * Package containing supporting classes for KijiExpress' command-line interface.
+ */
+package object tool
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/AvroTupleConversions.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/AvroTupleConversions.scala
new file mode 100644
index 0000000000000000000000000000000000000000..d1fd505d7449785577122d847fd1e8928accc983
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/AvroTupleConversions.scala
@@ -0,0 +1,201 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import java.lang.reflect.Method
+
+import scala.collection.JavaConversions.asScalaIterator
+import scala.reflect.Manifest
+
+import cascading.tuple.Fields
+import cascading.tuple.Tuple
+import cascading.tuple.TupleEntry
+import com.twitter.scalding.TupleConversions
+import com.twitter.scalding.TupleConverter
+import com.twitter.scalding.TuplePacker
+import com.twitter.scalding.TupleSetter
+import com.twitter.scalding.TupleUnpacker
+import org.apache.avro.Schema
+import org.apache.avro.generic.GenericRecord
+import org.apache.avro.generic.GenericRecordBuilder
+import org.apache.avro.specific.SpecificRecord
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.framework.serialization.KijiLocker
+
+/**
+ * Provides implementations of Scalding abstract classes to enable packing and unpacking Avro
+ * specific and generic records.  Also provides implicit definitions to support implicitly using
+ * these classes, where necessary.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+trait AvroTupleConversions {
+  /**
+   * [[com.twitter.scalding.TuplePacker]] implementation provides instances of the
+   * [[org.kiji.express.flow.util.AvroSpecificTupleConverter]] for converting fields
+   * in a Scalding flow into specific Avro records of the parameterized type.  An instance of this
+   * class must be in implicit scope, or passed in explicitly to
+   * [[com.twitter.scalding.RichPipe.pack]].
+   *
+   * @tparam T type of compiled Avro class to be packed (created).
+   * @param m [[scala.reflect.Manifest]] of T.  Provided implicitly by a built-in conversion.
+   */
+  private[express] class AvroSpecificTuplePacker[T <: SpecificRecord](implicit m: Manifest[T])
+      extends TuplePacker[T] {
+    override def newConverter(fields: Fields): TupleConverter[T] = {
+      new AvroSpecificTupleConverter(fields, m)
+    }
+  }
+
+  /**
+   * Provides a [[com.twitter.scalding.TupleSetter]] for unpacking an Avro
+   * [[org.apache.avro.generic.GenericRecord]] into a [[cascading.tuple.Tuple]].
+   */
+  private[express] class AvroGenericTupleUnpacker extends TupleUnpacker[GenericRecord] {
+    override def newSetter(fields: Fields): TupleSetter[GenericRecord] = {
+      new AvroGenericTupleSetter(fields)
+    }
+  }
+
+  /**
+   * Takes an Avro [[org.apache.avro.generic.GenericRecord]] and unpacks the specified fields
+   * into a new [[cascading.tuple.Tuple]].
+   *
+   * @param fs the fields to be unpacked from the [[org.apache.avro.generic.GenericRecord]].
+   */
+  private[express] class AvroGenericTupleSetter(fs: Fields) extends TupleSetter[GenericRecord] {
+    override def arity: Int = fs.size
+
+    private val fields: List[String] = fs.iterator.map(_.toString).toList
+
+    override def apply(arg: GenericRecord): Tuple = {
+      new Tuple(fields.map(arg.get): _*)
+    }
+  }
+
+  /**
+   * Provides an [[org.kiji.express.flow.util.AvroTupleConversions.AvroSpecificTuplePacker]] to the
+   * implicit scope.
+   *
+   * @tparam T Avro compiled [[org.apache.avro.specific.SpecificRecord]] class.
+   * @param mf implicitly provided [[scala.reflect.Manifest]] of provided Avro type.
+   * @return [[org.kiji.express.flow.util.AvroTupleConversions.AvroSpecificTuplePacker]] for given
+   *         Avro specific record type
+   */
+  private[express] implicit def avroSpecificTuplePacker[T <: SpecificRecord]
+      (implicit mf: Manifest[T]): AvroSpecificTuplePacker[T] = {
+    new AvroSpecificTuplePacker[T]
+  }
+
+  /**
+   * Provides an [[org.kiji.express.flow.util.AvroTupleConversions.AvroGenericTupleUnpacker]] to
+   * implicit scope.
+   *
+   * @return an [[org.kiji.express.flow.util.AvroTupleConversions.AvroGenericTupleUnpacker]].
+   */
+  private[express] implicit def avroGenericTupleUnpacker: AvroGenericTupleUnpacker = {
+    new AvroGenericTupleUnpacker
+  }
+}
+
+/**
+ * Converts [[cascading.tuple.TupleEntry]]s with the given fields into an Avro
+ * [[org.apache.avro.specific.SpecificRecord]] instance of the parameterized type.  This
+ * converter will fill in default values of the record if not specified by tuple fields.
+ *
+ * @tparam T Type of the target specific Avro record.
+ * @param fs The fields to convert into an Avro record.  The field names must match the field
+ *           names of the Avro record type.  There must be only one result field.
+ * @param m [[scala.reflect.Manifest]] of the target type.  Implicitly provided.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] case class AvroSpecificTupleConverter[T](fs: Fields, m: Manifest[T])
+    extends TupleConverter[T] {
+
+  override def arity: Int = -1
+
+  /**
+   * Attempts to translate an avro field name (e.g. count, my_count, should_be_snake_case) to an
+   * Avro record setter name (e.g., setCount, setMyCount, setShouldBeSnakeCase).
+   * @param field to translate to setter format.
+   * @return setter of given field.
+   */
+  private def fieldToSetter(field: String): String = {
+    field.split('_').map(_.capitalize).mkString("set", "", "")
+  }
+
+  // Precompute as much of the reflection business as possible.  Method objects do not serialize,
+  // so any val containing a method in it must be lazy.
+  private val avroClass: Class[_] = m.erasure
+  private val builderClass: Class[_] =
+      avroClass.getDeclaredClasses.find(_.getSimpleName == "Builder").get
+  lazy private val newBuilderMethod: Method = avroClass.getMethod("newBuilder")
+  lazy private val buildMethod: Method = builderClass.getMethod("build")
+
+  /**
+   * Map of field name to setter method.
+   */
+  lazy private val fieldSetters: Map[String, Method] = {
+    val fields: List[String] = fs.iterator.map(_.toString).toList
+    val setters: Map[String, Method] = builderClass
+        .getDeclaredMethods
+        .map { m => (m.getName, m) }
+        .toMap
+
+    fields
+        .zip(fields.map(fieldToSetter))
+        .toMap
+        .mapValues(setters)
+  }
+
+  override def apply(entry: TupleEntry): T = {
+    val builder = newBuilderMethod.invoke(avroClass)
+    fieldSetters.foreach { case (field, setter) => setter.invoke(builder, entry.getObject(field)) }
+    buildMethod.invoke(builder).asInstanceOf[T]
+  }
+}
+
+/**
+ * Converts [[cascading.tuple.TupleEntry]]s into an Avro [[org.apache.avro.generic.GenericRecord]]
+ * object with the provided schema.  This converter will fill in default values of the schema if
+ * they are not specified through fields.
+ *
+ * @param schemaLocker wrapping the schema of the target record
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] class AvroGenericTupleConverter(schemaLocker: KijiLocker[Schema])
+    extends TupleConverter[GenericRecord] with TupleConversions {
+
+  override def arity: Int = -1
+
+  override def apply(entry: TupleEntry): GenericRecord = {
+    val builder = new GenericRecordBuilder(schemaLocker.get)
+    toMap(entry).foreach { kv => builder.set(kv._1, kv._2) }
+    builder.build()
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/AvroUtil.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/AvroUtil.scala
new file mode 100644
index 0000000000000000000000000000000000000000..8718c923fcb7e593a99aecd3b7d07cfadc4776c1
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/AvroUtil.scala
@@ -0,0 +1,194 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import java.nio.ByteBuffer
+
+import scala.collection.JavaConverters.asScalaBufferConverter
+import scala.collection.JavaConverters.mapAsScalaMapConverter
+import scala.collection.JavaConverters.seqAsJavaListConverter
+
+import org.apache.avro.Schema
+import org.apache.avro.generic.GenericData
+import org.apache.avro.generic.GenericData.Fixed
+import org.apache.avro.generic.GenericEnumSymbol
+import org.apache.avro.generic.GenericFixed
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+
+/**
+ * A module with functions that can convert Java values (including Java API Avro values)
+ * read from a Kiji table to Scala values (including our Express-specific generic API Avro values)
+ * for use in KijiExpress, and vice versa for writing back to a Kiji table.
+ *
+ * When reading from Kiji, using the specific API, use the method `decodeSpecificFromJava`.
+ * When reading from Kiji, using the generic API, use the method `decodeGenericFromJava`.
+ * When writing to Kiji, use the method `encodeToJava`.
+ *
+ * When decoding using the generic API, primitives are converted to Scala primitives, and records
+ * are converted to AvroValues. The contents of a record are also AvroValues, even if they
+ * represent primitives.  These primitives can be accessed with `asInt`, `asLong`, etc methods.
+ * For example: `myRecord("fieldname").asInt` gets an integer field with name `fieldname` from
+ * `myRecord`.
+ *
+ * When decoding using the specific API, primitives are converted to Scala primitives, and records
+ * are converted to their corresponding Java classes.
+ *
+ * Certain AvroValues are used in both the specific and the generic API:
+ * <ul>
+ *   <li>AvroEnum, which always wraps an enum from Avro.  This is because there is not an easy
+ *       Scala equivalent of a Java enum.</li>
+ *   <li>AvroFixed, which always wraps a fixed-length byte array from Avro.  This is to distinguish
+ *       between a regular byte array, which gets converted to an Array[Byte], and a fixed-length
+ *       byte array.</li>
+ * <ul>
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+object AvroUtil {
+
+  /**
+   * Attempts to convert complex Avro types to their Scala equivalents.  If no such conversion
+   * exists, passes the value through.  In particular, four conversion are attempted:
+   *
+   * <ul>
+   *   <li>Java [[java.util.Map]]s will be converted to Scala [[scala.collection.Map]].</li>
+   *   <li>Java [[java.util.List]]s (including [[org.apache.avro.generic.GenericArray]]s) will be
+   *       converted to a [[scala.collection.mutable.Buffer]]s.</li>
+   *   <li>Avro [[org.apache.avro.generic.GenericFixed]] instances will be converted to Scala
+   *       [[scala.Array[Byte]]]s.</li>
+   *   <li>Avro [[org.apache.avro.generic.GenericEnumSymbol]]s will be converted to their
+   *       [[java.lang.String]] equivalent.</li>
+   * </ul>
+   *
+   * @param value to be converted to Scala equivalent
+   * @return Scala equivalent, or original object if no such equivalent exists
+   */
+  private[express] def avroToScala(value: Any): Any = value match {
+    case m: java.util.Map[_, _] => m.asScala.mapValues(avroToScala)
+    case l: java.util.List[_] => l.asScala.map(avroToScala)
+    case f: GenericFixed => f.bytes
+    case e: GenericEnumSymbol => e.toString
+    case other => other
+  }
+
+  /**
+   * Provides an encoder function that will coerce a [[scala.collection.TraversableOnce]] to an
+   * [[org.apache.avro.generic.GenericData.Array]].
+   *
+   * @param schema of the array
+   * @return an encoding function for Avro array types.
+   */
+  private[express] def arrayEncoder(schema: Schema): Any => Any = {
+    require(schema.getType == Schema.Type.ARRAY)
+    return {
+      case tr: TraversableOnce[_] =>
+        new GenericData.Array(schema, tr.map(avroEncoder(schema.getElementType)).toList.asJava)
+      case other => other
+    }
+  }
+
+  /**
+   * Provides an encoder function that will coerce values to an
+   * [[org.apache.avro.generic.GenericData.EnumSymbol]] if possible.
+   *
+   * @param schema of enum type
+   * @return an encoder function for enum values.
+   */
+  private[express] def enumEncoder(schema: Schema): Any => Any = {
+    require(schema.getType == Schema.Type.ENUM)
+    val genericData = new GenericData()
+
+    return {
+      case e: Enum[_] =>
+        // Perhaps useful for the case where a user defines their own enum with the same members
+        // as defined in the schema.
+        genericData.createEnum(e.name, schema)
+      case s: String => genericData.createEnum(s, schema)
+      case other => other
+    }
+  }
+
+  /**
+   * Provides an encoder function that will coerce values into an Avro compatible bytes format.
+   *
+   * @param schema of bytes type
+   * @return an encoder function for bytes values
+   */
+  private[express] def bytesEncoder(schema: Schema): Any => Any = {
+    require(schema.getType == Schema.Type.BYTES)
+
+    return {
+      case bs: Array[Byte] => ByteBuffer.wrap(bs)
+      case other => other
+    }
+  }
+
+  /**
+   * Provides an encoder function that will coerce values into an Avro generic fixed.
+   *
+   * @param schema of fixed type.
+   * @return an encoder function for fixed values.
+   */
+  private[express] def fixedEncoder(schema: Schema): Any => Any = {
+    require(schema.getType == Schema.Type.FIXED)
+
+    def getBytes(bb: ByteBuffer): Array[Byte] = {
+      val backing = bb.array()
+      val remaining = bb.remaining()
+      if (remaining == backing.length) { // no need to copy
+        backing
+      } else {
+        val copy = Array.ofDim[Byte](remaining)
+        bb.get(copy)
+        copy
+      }
+    }
+
+    return {
+      case bs: Array[Byte] => new Fixed(schema, bs)
+      case bb: ByteBuffer if bb.hasArray => new Fixed(schema, getBytes(bb))
+      case other => other
+    }
+  }
+
+  /**
+   * Creates an encoder function for a given schema.  For most cases no encoding needs to be done,
+   * so the encoder is the identity function.  For the collection types that have generic
+   * implementations we can convert a scala version of the type to the generic avro version.  We
+   * don't try to catch type mismatches here; instead Schema will check if the value is compatible
+   * with the given schema.
+   *
+   * @param schema of the values that the encoder function will take.
+   * @return an encoder function that will make a best effort to encode values to a type
+   *    compatible with the given schema.
+   */
+  private[express] def avroEncoder(schema: Schema): Any => Any = {
+    schema.getType match {
+      case Schema.Type.ARRAY => arrayEncoder(schema)
+      case Schema.Type.ENUM => enumEncoder(schema)
+      case Schema.Type.BYTES => bytesEncoder(schema)
+      case Schema.Type.FIXED => fixedEncoder(schema)
+      case _ => identity
+    }
+  }
+
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/CellMathUtil.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/CellMathUtil.scala
new file mode 100644
index 0000000000000000000000000000000000000000..4d449bba47a1e768dfe88ce7c34fd45f8e0e5802
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/CellMathUtil.scala
@@ -0,0 +1,181 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import scala.annotation.implicitNotFound
+
+import org.slf4j.Logger
+import org.slf4j.LoggerFactory
+
+import org.kiji.express.flow.FlowCell
+
+/**
+ * Provides aggregator functions for sequences of [[org.kiji.express.flow.FlowCell]]s.
+ */
+object CellMathUtil {
+  private val logger: Logger = LoggerFactory.getLogger(CellMathUtil.getClass)
+
+  /**
+   * Computes the sum of the values stored within the [[org.kiji.express.flow.FlowCell]]s in the
+   * provided `Seq`.
+   *
+   * <b>Note:</b> This method will not compile unless the cells contained within this collection
+   * contain values of a type that has an implicit implementation of the `scala.Numeric` trait.
+   *
+   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
+   * @param slice is the collection of [[org.kiji.express.flow.FlowCell]]s to sum.
+   * @return the sum of the values within the [[org.kiji.express.flow.FlowCell]]s in the provided
+   *     slice.
+   */
+  @implicitNotFound("The type of data contained within the provided cells does not support" +
+      " numeric operations through the scala.Numeric trait.")
+  def sum[T](slice: Seq[FlowCell[T]])(implicit num: Numeric[T]): T = {
+    slice.foldLeft(num.zero) { (sum: T, cell: FlowCell[T]) =>
+      num.plus(sum, cell.datum)
+    }
+  }
+
+  /**
+   * Computes the mean of the values stored within the [[org.kiji.express.flow.FlowCell]]s in the
+   * provided slice.
+   *
+   * <b>Note:</b> This method will not compile unless the cells contained within this collection
+   * contain values of a type that has an implicit implementation of the `scala.Numeric` trait.
+   *
+   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
+   * @param slice is the collection of [[org.kiji.express.flow.FlowCell]]s to compute the mean of.
+   * @return the mean of the values within the provided [[org.kiji.express.flow.FlowCell]]s.
+   */
+  @implicitNotFound("The type of data contained within the provided cells does not support" +
+      " numeric operations through the scala.Numeric trait.")
+  def mean[T](slice: Seq[FlowCell[T]])(implicit num: Numeric[T]): Double = {
+    val n = slice.size
+    slice.foldLeft(0.0) { (mean: Double, cell: FlowCell[T]) =>
+      mean + (num.toDouble(cell.datum) / n)
+    }
+  }
+
+  /**
+   * Finds the minimum of the values stored within the [[org.kiji.express.flow.FlowCell]]s in the
+   * provided slice.
+   *
+   * <b>Note:</b> This method will not compile unless the cells contained within this collection
+   * contain values of a type that has an implicit implementation of the `scala.Ordering` trait.
+   *
+   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
+   * @param slice is the collection of [[org.kiji.express.flow.FlowCell]]s to find the minimum of.
+   * @return the minimum value contained in the `Seq` of [[org.kiji.express.flow.FlowCell]]s.
+   */
+  @implicitNotFound("The type of data contained within the provided cells does not support" +
+      " ordering operations through the scala.Ordering trait.")
+  def min[T](slice: Seq[FlowCell[T]])(implicit cmp: Ordering[T]): T = {
+    slice.min(Ordering.by { cell: FlowCell[T] => cell.datum }).datum
+  }
+
+  /**
+   * Finds the maximum of the values stored within the [[org.kiji.express.flow.FlowCell]]s in the
+   * provided slice.
+   *
+   * <b>Note:</b> This method will not compile unless the cells contained within this collection
+   * contain values of a type that has an implicit implementation of the `scala.Ordering` trait.
+   *
+   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
+   * @param slice is the collection of [[org.kiji.express.flow.FlowCell]]s to find the maximum of.
+   * @return the maximum of the value within the provided [[org.kiji.express.flow.FlowCell]]s.
+   */
+  @implicitNotFound("The type of data contained within the provided cells does not support" +
+      " ordering operations through the scala.Ordering trait.")
+  def max[T](slice: Seq[FlowCell[T]])(implicit cmp: Ordering[T]): T = {
+    slice.max(Ordering.by { cell: FlowCell[T] => cell.datum }).datum
+  }
+
+  /**
+   * Computes the standard deviation of the values stored within the
+   * [[org.kiji.express.flow.FlowCell]]s in the provided slice.
+   *
+   * <b>Note:</b> This method will not compile unless the cells contained within this collection
+   * contain values of a type that has an implicit implementation of the `scala.Numeric` trait.
+   *
+   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
+   * @param slice the `Seq` of [[org.kiji.express.flow.FlowCell]]s to compute the standard
+   *     deviation of.
+   * @return the standard deviation of the values within the [[org.kiji.express.flow.FlowCell]]s in
+   *     the provided slice.
+   */
+  @implicitNotFound("The type of data contained within the provided cells does not support" +
+      " numeric operations through the scala.Numeric trait.")
+  def stddev[T](slice:Seq[FlowCell[T]])(implicit num: Numeric[T]): Double = {
+    scala.math.sqrt(variance(slice))
+  }
+
+  /**
+   * Computes the squared sum of the values stored within the [[org.kiji.express.flow.FlowCell]]s in
+   * the provided slice.
+   *
+   * <b>Note:</b> This method will not compile unless the cells contained within this collection
+   * contain values of a type that has an implicit implementation of the `scala.Numeric` trait.
+   *
+   * @tparam T the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
+   * @param slice the collection of [[org.kiji.express.flow.FlowCell]]s to compute the squared
+   *     sum of.
+   * @return the squared sum of the values in the provided [[org.kiji.express.flow.FlowCell]]s.
+   */
+  @implicitNotFound("The type of data contained within the provided cells does not support" +
+      " numeric operations through the scala.Numeric trait.")
+  def sumSquares[T](slice: Seq[FlowCell[T]])(implicit num: Numeric[T]): T = {
+    slice.foldLeft(num.zero) { (sumSquares: T, cell: FlowCell[T]) =>
+      num.plus(sumSquares, num.times(cell.datum, cell.datum))
+    }
+  }
+
+  /**
+   * Computes the variance of the values stored within the [[org.kiji.express.flow.FlowCell]]s in
+   * the provided slice.
+   *
+   * <b>Note:</b> This method will not compile unless the cells contained within this collection
+   * contain values of a type that has an implicit implementation of the `scala.Numeric` trait.
+   *
+   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
+   * @param slice is the collection of [[org.kiji.express.flow.FlowCell]]s to compute the
+   *     variance of.
+   * @return the variance of the values within the [[org.kiji.express.flow.FlowCell]]s in the
+   *     provided slice.
+   */
+  @implicitNotFound("The type of data contained within the provided cells does not support" +
+      " numeric operations through the scala.Numeric trait.")
+  def variance[T](slice: Seq[FlowCell[T]])(implicit num: Numeric[T]): Double = {
+    val (n, _, m2) = slice
+        .foldLeft((0L, 0.0, 0.0)) { (acc: (Long, Double, Double), cell: FlowCell[T]) =>
+          val (n, mean, m2) = acc
+          logger.debug("cell: %s".format(cell.datum))
+          logger.debug("acc: %s".format(acc))
+          val x = num.toDouble(cell.datum)
+
+          val nPrime = n + 1
+          val delta = x - mean
+          val meanPrime = mean + delta / nPrime
+          val m2Prime = m2 + delta * (x - meanPrime)
+
+          (nPrime, meanPrime, m2Prime)
+        }
+
+    m2 / n
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/EntityIdFactoryCache.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/EntityIdFactoryCache.scala
new file mode 100644
index 0000000000000000000000000000000000000000..ce93653c25720ba1766c8af4f29e6cda7ad7c718
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/EntityIdFactoryCache.scala
@@ -0,0 +1,126 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import java.io.ByteArrayInputStream
+import java.io.ByteArrayOutputStream
+import java.io.DataInputStream
+import java.io.DataOutputStream
+import java.nio.ByteBuffer
+
+import org.apache.hadoop.conf.Configuration
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.express.flow.util.Resources._
+import org.kiji.schema.EntityIdFactory
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiURI
+
+/**
+ * EntityIdFactoryCache performs the operation of getting the EntityIdFactory for a Kiji
+ * table in a memoized way. If the required EntityIdFactory is not present in its cache, this
+ * will open a connection to a Kiji table, get the factory from it and cache it for
+ * subsequent calls. There will be one such cache per JVM.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+private[express] object EntityIdFactoryCache {
+  /**
+   * Memoizes construction of EntityId factories. The conf is represented as a ByteBuffer for proper
+   * comparison.
+   */
+  private val factoryCache: Memoize[(KijiURI, ByteBuffer), EntityIdFactory] =
+      Memoize { entry: (KijiURI, ByteBuffer) =>
+        val (tableUri, serializedConf) = entry
+        val conf = deserializeConf(serializedConf)
+
+        val tableLayout = doAndRelease(Kiji.Factory.open(tableUri, conf)) { kiji: Kiji =>
+          doAndRelease(kiji.openTable(tableUri.getTable)) { table: KijiTable =>
+            table.getLayout
+          }
+        }
+        EntityIdFactory.getFactory(tableLayout)
+      }
+
+  /** Memoizes construction of KijiURIs. */
+  private val uriCache: Memoize[String, KijiURI] =
+      Memoize { tableUri: String =>
+        KijiURI.newBuilder(tableUri).build()
+      }
+
+  /**
+   * Get an EntityIdFactory for the table specified. This method memoizes EntityId factory
+   * construction and will not fetch the most up-to-date factory from the addressed table.
+   *
+   * @param tableUri of the Kiji table to fetch an EntityId factory from.
+   * @param conf identifying the cluster to use when building EntityIds.
+   * @return an EntityIdFactory associated with the addressed table.
+   */
+  private[express] def getFactory(
+      tableUri: String,
+      conf: Configuration): EntityIdFactory = {
+    val uri: KijiURI = uriCache(tableUri)
+    getFactory(uri, conf)
+  }
+
+  /**
+   * Get an EntityIdFactory for the table specified. This method memoizes EntityId factory
+   * construction and will not fetch the most up-to-date factory from the addressed table.
+   *
+   * @param tableUri of the Kiji table to fetch an EntityId factory from.
+   * @param conf identifying the cluster to use when building EntityIds.
+   * @return an EntityIdFactory associated with the addressed table.
+   */
+  private[express] def getFactory(
+      tableUri: KijiURI,
+      conf: Configuration): EntityIdFactory = {
+    factoryCache(tableUri, serializeConf(conf))
+  }
+
+  /**
+   * Serializes a configuration into a string.
+   *
+   * @param conf to serialize.
+   * @return the serialized configuration.
+   */
+  private[express] def serializeConf(conf: Configuration): ByteBuffer = {
+    val confOutputStreamWriter = new ByteArrayOutputStream()
+    val dataOutputStream = new DataOutputStream(confOutputStreamWriter)
+    conf.write(dataOutputStream)
+    dataOutputStream.close
+    return ByteBuffer.wrap(confOutputStreamWriter.toByteArray)
+  }
+
+  /**
+   * Deserializes a conf from a string.
+   *
+   * @param serializedConf to deserialize
+   * @return A configuration deserialized from `serializedConf`.
+   */
+  private[express] def deserializeConf(serializedConf: ByteBuffer): Configuration = {
+    val in = new ByteArrayInputStream(serializedConf.array())
+    val conf = new Configuration()
+    conf.readFields(new DataInputStream(in))
+    in.close()
+    return conf
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/GenericCellSpecs.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/GenericCellSpecs.scala
new file mode 100644
index 0000000000000000000000000000000000000000..109264f6d29db463f9723b08977e9b4a52b1eea6
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/GenericCellSpecs.scala
@@ -0,0 +1,93 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import java.util.{Map => JMap}
+
+import scala.collection.JavaConverters.asScalaSetConverter
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.schema.GenericCellDecoderFactory
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiTable
+import org.kiji.schema.layout.CellSpec
+import org.kiji.schema.layout.KijiTableLayout
+
+/**
+ * A factory for mappings from columns in a Kiji table to generic cell specifications for those
+ * columns. This factory can be used to easily construct generic cell specifications for a table,
+ * which can then be passed to a Kiji table reader to allow for reading data generically. See
+ * [[org.kiji.schema.KijiReaderFactory]] for more information on how to use these cell specs to
+ * obtain a Kiji table reader that decodes data using the Avro GenericData API.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+object GenericCellSpecs {
+
+  /**
+   * Gets a generic cell specification mapping for a Kiji table.
+   *
+   * @param table used to get the generic cell specification mapping.
+   * @return a map from names of columns in the table to generic cell specifications for the
+   *     columns.
+   */
+  def apply(table: KijiTable): JMap[KijiColumnName, CellSpec] = {
+    apply(table.getLayout())
+  }
+
+  /**
+   * Gets a generic cell specification mapping for a layout of a Kiji table.
+   *
+   * @param layout used to get the generic cell specification mapping.
+   * @return a map from names of columns in the layout to generic cell specifications for the
+   *     columns.
+   */
+  def apply(layout: KijiTableLayout): JMap[KijiColumnName, CellSpec] = {
+    return createCellSpecMap(layout)
+  }
+
+  /**
+   * Creates a mapping from columns in a table layout to generic cell specifications for those
+   * columns.
+   *
+   * @param layout used to create the generic cell specifications.
+   * @return a map from the names of columns in the layout to generic cell specifications for
+   *     those columns.
+   */
+  private def createCellSpecMap(layout: KijiTableLayout): JMap[KijiColumnName, CellSpec] = {
+    // Fold the column names in the layout into a map from column name to a generic cell
+    // specification for the column.
+    val specMap = new java.util.HashMap[KijiColumnName, CellSpec]()
+    layout
+        .getColumnNames
+        .asScala
+        .foreach { columnName: KijiColumnName =>
+          val cellSpec = layout
+              .getCellSpec(columnName)
+              .setDecoderFactory(GenericCellDecoderFactory.get())
+          if (cellSpec.isAvro) {
+            cellSpec.setUseWriterSchema()
+          }
+          specMap.put(columnName, cellSpec)
+        }
+    specMap
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/GenericRowDataConverter.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/GenericRowDataConverter.scala
new file mode 100644
index 0000000000000000000000000000000000000000..ed841b1a03c53f192d67226b3e9600b42addf92c
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/GenericRowDataConverter.scala
@@ -0,0 +1,142 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import java.util.HashMap
+import java.util.{Map => JMap}
+
+import scala.collection.JavaConverters.collectionAsScalaIterableConverter
+import scala.collection.mutable.{Map => MMap}
+
+import org.apache.hadoop.conf.Configuration
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.GenericCellDecoderFactory
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiRowData
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiURI
+import org.kiji.schema.impl.HBaseKijiRowData
+import org.kiji.schema.layout.CellSpec
+import org.kiji.schema.layout.KijiTableLayout
+import org.kiji.schema.layout.impl.CellDecoderProvider
+import org.kiji.schema.util.ResourceUtils
+
+/**
+ * A factory for [[org.kiji.schema.KijiRowData]] that use the Avro GenericData API to decode
+ * their data.
+ *
+ * @param uri to a Kiji instance for rows that will be converted.
+ * @param conf is the hadoop configuration used when accessing Kiji.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+@Inheritance.Sealed
+final class GenericRowDataConverter(uri: KijiURI, conf: Configuration) {
+
+  /**
+   * The Kiji instance containing the tables whose rows will be converted.
+   */
+  private val kiji = Kiji.Factory.open(uri, conf)
+
+  /**
+   * A schema table that will be given to decoders.
+   */
+  private val schemaTable = kiji.getSchemaTable()
+
+  /**
+   * A cache of decoder providers for table layouts, that provide generic decoders that decode
+   * data without a reader schema.
+   */
+  private val decoderProviderCache = MMap[KijiTableLayout, CellDecoderProvider]()
+
+  /**
+   * Gets a row data that is a copy of the original, but configured to decode data using the Avro
+   * GenericData API.
+   *
+   * In practice this is used to enable the use of the dynamic Avro API throughout KijiExpress.
+   * The row data copy performed is shallow and thus reasonably efficient,
+   * and care should be taken to maintain this.
+   *
+   * @param original row data to reconfigure.
+   * @return a copy of the row data, configured to decode data generically.
+   */
+  def apply(original: KijiRowData): KijiRowData = {
+    // Downcast the original data so we can access its guts.
+    val hbaseRowData = original.asInstanceOf[HBaseKijiRowData]
+    // Create a provider for generic data cell decoders.
+    val decoderProvider = getDecoderProvider(hbaseRowData.getTable())
+    // Create a new original data instance configured with the new decoder.
+    new HBaseKijiRowData(hbaseRowData.getTable(), hbaseRowData.getDataRequest(),
+        hbaseRowData.getEntityId(), hbaseRowData.getHBaseResult(), decoderProvider)
+  }
+
+  /**
+   * Closes the resources associated with this converter.
+   */
+  def close() {
+    // Since we are the only ones retaining this kiji, releasing it will close it,
+    // which will in turn close the schema table.
+    ResourceUtils.releaseOrLog(kiji)
+  }
+
+  /**
+   * Retrieves a cell decoder provider for a Kiji table, that provides decoders that decode data
+   * generically and without reader schema.
+   *
+   * @param table to retrieve the cell decoder provider for.
+   * @return the cell decoder provider that decodes data generically and without reader schema.
+   */
+  private def getDecoderProvider(table: KijiTable): CellDecoderProvider = {
+    // Retrieve a provider from the cache, or generate and cache a new one for the table.
+    // Generating a new decoder provider involves creating a map of cell specifications used to
+    // "overwrite" the reader schemas in the layout to null, so that no reader schema is used
+    // when decoding data.
+    val layout = table.getLayout()
+    decoderProviderCache.get(layout).getOrElse {
+      val provider = new CellDecoderProvider(layout, schemaTable,
+          GenericCellDecoderFactory.get(), createCellSpecMap(layout))
+      decoderProviderCache.put(layout, provider)
+      provider
+    }
+  }
+
+  /**
+   * Creates a map from the names of columns in the layout into cell specifications without
+   * reader schemas.
+   *
+   * @param layout to use when generation the cell spec map.
+   * @return a map from columns in the layout to cell specifications without reader schema.
+   */
+  private def createCellSpecMap(layout: KijiTableLayout): JMap[KijiColumnName, CellSpec] = {
+    // Fold the columns in the layout into a map from columns to cell specifications without a
+    // reader schema.
+    layout
+        .getColumnNames()
+        .asScala
+        .foldLeft(new HashMap[KijiColumnName, CellSpec]()) { (specMap, columnName) =>
+          specMap.put(columnName, layout.getCellSpec(columnName).setUseWriterSchema())
+          specMap
+        }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/Memoize.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/Memoize.scala
new file mode 100644
index 0000000000000000000000000000000000000000..822658d3cfdac74d0636b8897dd3c1a3dd04a32f
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/Memoize.scala
@@ -0,0 +1,103 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import scala.collection.mutable
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+
+/**
+ * Class used to wrap a function with a single parameter, caching parameter/return value
+ * combinations. The wrapped function will only execute when encountering new parameters. This
+ * should only be used to wrap pure functions.
+ *
+ * For example, lets define a function that has a potentially long execution time and memoize it:
+ * {{{
+ * // A slow function:
+ * def factorial(n: Int): Int = {
+ *   if (n == 0) {
+ *     1
+ *   } else {
+ *     n * factorial(n - 1)
+ *   }
+ * }
+ *
+ * // This will be very fast for each cached input.
+ * val cachedFactorial = Memoize(factorial)
+ *
+ * // This will take some time to run
+ * cachedFactorial(100)
+ * // This will be very fast now that the factorial of 100 has been cached already.
+ * cachedFactorial(100)
+ * }}}
+ *
+ * Note: Functions with multiple parameters can also be used with Memoize by first converting them
+ * to a single parameter function where the parameter to the function is a tuple containing the
+ * original parameters. This can be achieved with the scala standard library's `tupled` method on
+ * functions.
+ *
+ * @param f is the function to wrap.
+ * @tparam T is the input type of the wrapped function.
+ * @tparam R is the return type of the wrapped function.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+class Memoize[-T, +R](f: T => R) extends (T => R) {
+  /** Mutable map used to store already computed parameter/return value combinations. */
+  private[this] val cache = mutable.Map.empty[T, R]
+
+  /**
+   * Fetches the return value associated with the provided parameter. This function will return a
+   * cached value first if possible.
+   *
+   * @param x is the parameter to the function.
+   * @return the value associated with the provided parameter.
+   */
+  def apply(x: T): R = {
+    // Check to see if 'x' is already in the cache.
+    if (cache.contains(x)) {
+      // Return the cached value.
+      cache(x)
+    } else {
+      // Compute the return value associated with 'x', add it to the cache, and return it.
+      val y = f(x)
+      cache += ((x, y))
+      y
+    }
+  }
+}
+
+/**
+ * Companion object for memoization wrapper that contains factory methods.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+object Memoize {
+  /**
+   * Wraps a function with Memoize.
+   *
+   * @param f is a function to wrap.
+   * @return the wrapped function that will cache parameter/return value pairs.
+   */
+  def apply[T, R](f: T => R): Memoize[T, R] = new Memoize(f)
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/PipeConversions.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/PipeConversions.scala
new file mode 100644
index 0000000000000000000000000000000000000000..aeb6e3b21c8b482fad780413f9db251ff8be0d82
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/PipeConversions.scala
@@ -0,0 +1,81 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import cascading.flow.FlowDef
+import cascading.pipe.Pipe
+import com.twitter.scalding.Mode
+import com.twitter.scalding.RichPipe
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.KijiPipe
+import org.kiji.express.flow.KijiSource
+
+/**
+ * PipeConversions contains implicit conversions necessary for KijiExpress that are not included in
+ * Scalding's `Job`.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] trait PipeConversions {
+  /**
+   * Converts a Cascading Pipe to a KijiExpress KijiPipe. This method permits implicit conversions
+   * from Pipe to KijiPipe.
+   *
+   * @param pipe to convert to a KijiPipe.
+   * @return a KijiPipe wrapping the specified Pipe.
+   */
+  implicit def pipe2KijiPipe(pipe: Pipe): KijiPipe = new KijiPipe(pipe)
+
+  /**
+   * Converts a [[org.kiji.express.flow.KijiPipe]] to a [[cascading.pipe.Pipe]].  This
+   * method permits implicit conversion from KijiPipe to Pipe.
+   *
+   * @param kijiPipe to convert to [[cascading.pipe.Pipe]].
+   * @return Pipe instance wrapped by the input KijiPipe.
+   */
+  implicit def kijiPipe2Pipe(kijiPipe: KijiPipe): Pipe = kijiPipe.pipe
+
+  /**
+   * Converts a [[org.kiji.express.flow.KijiPipe]] to a [[com.twitter.scalding.RichPipe]].  This
+   * method permits implicit conversion from KijiPipe to RichPipe.
+   * @param kijiPipe to convert to [[com.twitter.scalding.RichPipe]].
+   * @return RichPipe instance of Pipe wrapped by input KijiPipe.
+   */
+  implicit def kijiPipe2RichPipe(kijiPipe: KijiPipe): RichPipe = new RichPipe(kijiPipe.pipe)
+
+  /**
+   * Converts a KijiSource to a KijiExpress KijiPipe. This method permits implicit conversions
+   * from Source to KijiPipe.
+   *
+   * We expect flowDef and mode implicits to be in scope.  This should be true in the context of a
+   * Job, KijiJob, or inside the ShellRunner.
+   *
+   * @param source to convert to a KijiPipe
+   * @return a KijiPipe read from the specified source.
+   */
+  implicit def source2RichPipe(
+      source: KijiSource)(
+      implicit flowDef: FlowDef,
+      mode: Mode): KijiPipe = new KijiPipe(source.read(flowDef, mode))
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/Resources.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/Resources.scala
new file mode 100644
index 0000000000000000000000000000000000000000..ace51751db6e25ebb017b12cd98d4acc2bdcad10
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/Resources.scala
@@ -0,0 +1,257 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import scala.io.Source
+
+import org.apache.hadoop.conf.Configuration
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.util.ReferenceCountable
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiTableReader
+import org.kiji.schema.KijiTableWriter
+import org.kiji.schema.KijiURI
+
+/**
+ * The Resources object contains various convenience functions while dealing with
+ * resources such as Kiji instances or Kiji tables, particularly around releasing
+ * or closing them and handling exceptions.
+ */
+@ApiAudience.Public
+@ApiStability.Experimental
+object Resources {
+  /**
+   * Exception that contains multiple exceptions. Typically used in the case where
+   * the user gets an exception within their function and further gets an exception
+   * during cleanup in finally.
+   *
+   * @param msg is the message to include with the exception.
+   * @param errors causing this exception.
+   */
+  @Inheritance.Sealed
+  final case class CompoundException(msg: String, errors: Seq[Exception]) extends Exception
+
+  /**
+   * Performs an operation with a resource that requires post processing. This method will throw a
+   * [[org.kiji.express.util.Resources.CompoundException]] when exceptions get thrown
+   * during the operation and while resources are being closed.
+   *
+   * @tparam T is the return type of the operation.
+   * @tparam R is the type of resource such as a Kiji instance or table.
+   * @param resource required by the operation.
+   * @param after is a function for any post processing on the resource, such as close or release.
+   * @param fn is the operation to perform using the resource, like getting the layout of a table.
+   * @return the result of the operation.
+   * @throws CompoundException if your function crashes as well as the close operation.
+   */
+  def doAnd[T, R](resource: => R, after: R => Unit)(fn: R => T): T = {
+    var error: Option[Exception] = None
+
+    // Build the resource.
+    val res: R = resource
+    try {
+      // Perform the operation.
+      fn(res)
+    } catch {
+      // Store the exception in case close fails.
+      case e: Exception => {
+        error = Some(e)
+        throw e
+      }
+    } finally {
+      try {
+        // Cleanup resources.
+        after(res)
+      } catch {
+        // Throw the exception(s).
+        case e: Exception => {
+          error match {
+            case Some(firstErr) => throw CompoundException("Exception was thrown while cleaning up "
+                + "resources after another exception was thrown.", Seq(firstErr, e))
+            case None => throw e
+          }
+        }
+      }
+    }
+  }
+
+  /**
+   * Performs an operation with a releaseable resource by first retaining the resource and releasing
+   * it upon completion of the operation.
+   *
+   * @tparam T is the return type of the operation.
+   * @tparam R is the type of resource, such as a Kiji table or instance.
+   * @param resource is the retainable resource object used by the operation.
+   * @param fn is the operation to perform using the releasable resource.
+   * @return the result of the operation.
+   */
+  def retainAnd[T, R <: ReferenceCountable[R]](
+      resource: => ReferenceCountable[R])(fn: R => T): T = {
+    doAndRelease[T, R](resource.retain())(fn)
+  }
+
+  /**
+   * Performs an operation with an already retained releaseable resource releasing it upon
+   * completion of the operation.
+   *
+   * @tparam T is the return type of the operation.
+   * @tparam R is the type of resource, such as a Kiji table or instance.
+   * @param resource is the retainable resource object used by the operation.
+   * @param fn is the operation to perform using the resource.
+   * @return the result of the operation.
+   */
+  def doAndRelease[T, R <: ReferenceCountable[R]](resource: => R)(fn: R => T): T = {
+    def after(r: R) { r.release() }
+    doAnd(resource, after)(fn)
+  }
+
+  /**
+   * Performs an operation with a closeable resource closing it upon completion of the operation.
+   *
+   * @tparam T is the return type of the operation.
+   * @tparam C is the type of resource, such as a Kiji table or instance.
+   * @param resource is the closeable resource used by the operation.
+   * @param fn is the operation to perform using the resource.
+   * @return the result of the operation.
+   */
+  def doAndClose[T, C <: { def close(): Unit }](resource: => C)(fn: C => T): T = {
+    def after(c: C) { c.close() }
+    doAnd(resource, after)(fn)
+  }
+
+  /**
+   * Performs an operation that uses a Kiji instance.
+   *
+   * @tparam T is the return type of the operation.
+   * @param uri of the Kiji instance to open.
+   * @param configuration identifying the cluster running Kiji.
+   * @param fn is the operation to perform.
+   * @return the result of the operation.
+   */
+  def withKiji[T](uri: KijiURI, configuration: Configuration)(fn: Kiji => T): T = {
+    doAndRelease(Kiji.Factory.open(uri, configuration))(fn)
+  }
+
+  /**
+   * Performs an operation that uses a Kiji table.
+   *
+   * @tparam T is the return type of the operation.
+   * @param kiji instance the desired table belongs to.
+   * @param table name of the Kiji table to open.
+   * @param fn is the operation to perform.
+   * @return the result of the operation.
+   */
+  def withKijiTable[T](kiji: Kiji, table: String)(fn: KijiTable => T): T = {
+    doAndRelease(kiji.openTable(table))(fn)
+  }
+
+  /**
+   * Performs an operation that uses a Kiji table.
+   *
+   * @tparam T is the return type of the operation.
+   * @param tableUri addressing the Kiji table to open.
+   * @param configuration identifying the cluster running Kiji.
+   * @param fn is the operation to perform.
+   * @return the result of the operation.
+   */
+  def withKijiTable[T](tableUri: KijiURI, configuration: Configuration)(fn: KijiTable => T): T = {
+    withKiji(tableUri, configuration) { kiji: Kiji =>
+      withKijiTable(kiji, tableUri.getTable)(fn)
+    }
+  }
+
+  /**
+   * Performs an operation that uses a Kiji table writer.
+   *
+   * @tparam T is the return type of the operation.
+   * @param table the desired Kiji table writer belongs to.
+   * @param fn is the operation to perform.
+   * @return the result of the operation.
+   */
+  def withKijiTableWriter[T](table: KijiTable)(fn: KijiTableWriter => T): T = {
+    doAndClose(table.openTableWriter)(fn)
+  }
+
+  /**
+   * Performs an operation that uses a Kiji table writer.
+   *
+   * @tparam T is the return type of the operation.
+   * @param tableUri addressing the Kiji table to open.
+   * @param configuration identifying the cluster running Kiji.
+   * @param fn is the operation to perform.
+   * @return the result of the operation.
+   */
+  def withKijiTableWriter[T](
+      tableUri: KijiURI,
+      configuration: Configuration)(fn: KijiTableWriter => T): T = {
+    withKijiTable(tableUri, configuration) { table: KijiTable =>
+      withKijiTableWriter(table)(fn)
+    }
+  }
+
+  /**
+   * Performs an operation that uses a Kiji table reader.
+   *
+   * @tparam T is the return type of the operation.
+   * @param table the desired Kiji table reader belongs to.
+   * @param fn is the operation to perform.
+   * @return the result of the operation.
+   */
+  def withKijiTableReader[T](table: KijiTable)(fn: KijiTableReader => T): T = {
+    doAndClose(table.openTableReader)(fn)
+  }
+
+  /**
+   * Performs an operation that uses a Kiji table reader.
+   *
+   * @tparam T is the return type of the operation.
+   * @param tableUri addressing the Kiji table to open.
+   * @param configuration identifying the cluster running Kiji.
+   * @param fn is the operation to perform.
+   * @return the result of the operation.
+   */
+  def withKijiTableReader[T](
+      tableUri: KijiURI,
+      configuration: Configuration)(fn: KijiTableReader => T): T = {
+    withKijiTable(tableUri, configuration) { table: KijiTable =>
+      withKijiTableReader(table)(fn)
+    }
+  }
+
+  /**
+   * Reads a resource from the classpath into a string.
+   *
+   * @param path to the desired resource (of the form: "path/to/your/resource").
+   * @return the contents of the resource as a string.
+   */
+  def resourceAsString(path: String): String = {
+    val inputStream = getClass()
+        .getClassLoader()
+        .getResourceAsStream(path)
+
+    doAndClose(Source.fromInputStream(inputStream)) { source =>
+      source.mkString
+    }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/SpecificCellSpecs.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/SpecificCellSpecs.scala
new file mode 100644
index 0000000000000000000000000000000000000000..843ba73b8647a7f8fe37b9177bd3e3b2a35225f9
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/SpecificCellSpecs.scala
@@ -0,0 +1,228 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import java.io.ByteArrayInputStream
+import java.io.ByteArrayOutputStream
+import java.util.HashMap
+import java.util.{Map => JMap}
+import java.util.Properties
+
+import scala.collection.JavaConverters.asScalaSetConverter
+import scala.collection.JavaConverters.mapAsJavaMapConverter
+
+import org.apache.avro.specific.SpecificRecord
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.express.flow.ColumnInputSpec
+import org.kiji.express.flow.SchemaSpec
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiTable
+import org.kiji.schema.layout.CellSpec
+import org.kiji.schema.layout.KijiTableLayout
+
+import SchemaSpec.Specific
+
+@ApiAudience.Framework
+@ApiStability.Experimental
+object SpecificCellSpecs {
+  val CELLSPEC_OVERRIDE_CONF_KEY: String = "kiji.express.input.cellspec.overrides"
+
+  /**
+   * Convert a map from field names to column input spec into a serialized map from column names to
+   * specific AvroRecord class names to use as overriding reader schemas when reading from those
+   * columns.
+   *
+   * @param columns a mapping from field name to column input spec. The column name and overriding
+   *     AvroRecord class name from each column input spec will be used to populate the output
+   *     serialized map.
+   * @return a serialized form of a map from column name to AvroRecord class name.
+   */
+  def serializeOverrides(
+      columns: Map[String, ColumnInputSpec]
+  ): String = {
+    val serializableOverrides = collectOverrides(columns)
+        .map { entry: (KijiColumnName, Class[_ <: SpecificRecord]) =>
+          val (key, value) = entry
+
+          (key.toString, value.getName)
+        }
+        .toMap
+
+    return serializeMap(serializableOverrides)
+  }
+
+  /**
+   * Deserialize an XML representation of a mapping from column name to AvroRecord class name and
+   * create a mapping from KijiColumnName to CellSpec which can be used by a KijiTableReader
+   * constructor to override reader schemas for the given columns.
+   *
+   * @param table the KijiTable from which to retrieve base CellSpecs.
+   * @param serializedMap the XML representation of the columns for which to override reader schemas
+   *     and the associated AvroRecord classes to use as reader schemas.
+   * @return a map from column name to Cellspec which can be used by a KijiTableReader constructor
+   *     to override reader schemas for the given columns.
+   */
+  def deserializeOverrides(
+      table: KijiTable,
+      serializedMap: String
+  ): Map[KijiColumnName, CellSpec] = {
+    return innerBuildCellSpecs(table.getLayout, deserializeMap(serializedMap))
+  }
+
+  /**
+   * Merge generic and specific CellSpecs favoring specifics.
+   *
+   * @param generics complete mapping from all columns to associated generic CellSpecs.
+   * @param specifics mapping of columns whose reader schemas should be overridden by specific Avro
+   *      classes as specified in the associated CellSpecs.
+   * @return a mapping from column name to CellSpec containing all mappings from specifics and all
+   *      other mappings from generics.
+   */
+  def mergeCellSpecs(
+      generics: JMap[KijiColumnName, CellSpec],
+      specifics: Map[KijiColumnName, CellSpec]
+  ): JMap[KijiColumnName, CellSpec] = {
+    // This JMap is necessary instead of generics.putAll(specifics) because generics is cached in
+    // GenericCellSpec.
+    val merged: JMap[KijiColumnName, CellSpec] = new HashMap[KijiColumnName, CellSpec]
+    merged.putAll(generics)
+    merged.putAll(specifics.asJava)
+    return merged
+  }
+
+  /**
+   * Build overridden CellSpecs for a given set of ColumnInputSpec.
+   *
+   * @param layout is the layout of the KijiTable from which base CellSpecs are drawn.
+   * @param columns are the ColumnInputSpecs from which to build overriding CellSpecs.
+   * @return a mapping from KijiColumnName to overriding CellSpec for that column.
+   */
+  def buildCellSpecs(
+      layout: KijiTableLayout,
+      columns: Map[String, ColumnInputSpec]
+  ): Map[KijiColumnName, CellSpec] = {
+    return innerBuildCellSpecs(layout, collectOverrides(columns))
+  }
+
+  /**
+   * Collects specific AvroRecord classes to use as overriding reader schemas. Input map keys are
+   * ignored.  Output map keys will be column names retrieved from the ColumnInputSpec and output
+   * map values will be AvroRecord classes to use as overriding reader schemas for associated
+   * columns.
+   *
+   * @param columns a mapping from field name to ColumnInputSpec.
+   * @return a mapping from column name to the AvroRecord class to use as the reader schema when
+   *     reading values from that column.
+   */
+  private def collectOverrides(
+      columns: Map[String, ColumnInputSpec]
+  ): Map[KijiColumnName, Class[_ <: SpecificRecord]] = {
+    columns.values
+        // Need only those columns that have specific Avro classes defined
+        .map { col => (col.columnName, col.schemaSpec) }
+        .collect { case (name, Specific(klass)) => (name, klass) }
+        .toMap
+  }
+
+  /**
+   * Serialize a map from column name to AvroRecord class name into an XML string for storage in the
+   * job configuration.
+   *
+   * @param mapToSerialize the map from column name to AvroRecord class name.
+   * @return a serialized version of these reader schema overrides.
+   */
+  private def serializeMap(
+      mapToSerialize: Map[String, String]
+  ): String = {
+    val props: Properties = new Properties()
+    // Add all map entries to the props.
+    mapToSerialize
+        .foreach {
+          case (column: String, avroClass: String) => props.setProperty(column, avroClass)
+        }
+    // Write the properties to an XML string.
+    val outputStream: ByteArrayOutputStream = new ByteArrayOutputStream()
+    props.storeToXML(
+        outputStream,
+        "These properties represent specific AvroRecord reader schema overrides. "
+        + "Keys are columns, values are specific AvroRecord classes.",
+        "UTF-8")
+    return outputStream.toString("UTF-8")
+  }
+
+
+  /**
+   * Deserializes an XML representation of a mapping from columns to AvroRecord classes.
+   *
+   * @param serializedMap the XML representation of the map.
+   * @return a mapping from KijiColumnName to SpecificRecord class.
+   */
+  private def deserializeMap(
+      serializedMap: String
+  ): Map[KijiColumnName, Class[_ <: SpecificRecord]] = {
+    // Load the properties from the serialized xml string.
+    val props: Properties = new Properties()
+    props.loadFromXML(new ByteArrayInputStream(serializedMap.getBytes))
+
+    return props.stringPropertyNames().asScala
+        .map {
+          case (column: String) => {
+            val kcn: KijiColumnName = new KijiColumnName(column)
+            val avroClass: Class[_ <: SpecificRecord] = avroClassForName(props.getProperty(column))
+            (kcn, avroClass)
+          }
+        }
+        .toMap
+  }
+
+  /**
+   * Constructs CellSpecs from a KijiTableLayout and a collection of reader schema overrides.
+   *
+   * @param layout the table layout from which to retrieve base CellSpecs.
+   * @param overrides a mapping from column to overriding reader schema.
+   * @return a mapping from column name to CellSpec which can be used in a KijiTableReader
+   *     constructor to override reader schemas.
+   */
+  private def innerBuildCellSpecs(
+      layout: KijiTableLayout,
+      overrides: Map[KijiColumnName, Class[_ <: SpecificRecord]]
+  ): Map[KijiColumnName, CellSpec] = {
+    return overrides
+        .map { entry: (KijiColumnName, Class[_ <: SpecificRecord]) =>
+          val (column, avroClass) = entry
+
+          (column, layout.getCellSpec(column).setSpecificRecord(avroClass))
+        }
+  }
+
+  /**
+   * Gets the AvroRecord Class for a given classname.
+   *
+   * @param className the name of the Class to retrieve.
+   * @return the AvroRecord Class for the given name.
+   */
+  private def avroClassForName(
+      className: String
+  ): Class[_ <: SpecificRecord] = {
+    return Class.forName(className).asSubclass(classOf[SpecificRecord])
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/Tuples.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/Tuples.scala
new file mode 100644
index 0000000000000000000000000000000000000000..20c70c2c5546fd4edc0fd4dac6a8675eb7067469
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/Tuples.scala
@@ -0,0 +1,189 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import scala.collection.JavaConverters.asScalaIteratorConverter
+
+import cascading.tuple.Fields
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+
+/**
+ * The Tuples object contains various convenience functions for dealing with scala and
+ * Cascading/Scalding tuples.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+object Tuples {
+  /**
+   * Converts a Cascading fields object into a sequence of the fields it contains. This only
+   * supports fields without a special [[cascading.tuple.Fields.Kind]].
+   *
+   * @param fields to convert.
+   * @return a sequence of field names.
+   */
+  def fieldsToSeq(fields: Fields): Seq[String] = {
+    fields
+        .iterator()
+        .asScala
+        .map { field => field.toString }
+        .toSeq
+  }
+
+  /**
+   * Converts a tuple into an appropriate representation for processing by a model phase function.
+   * Handles instances of Tuple1 as special cases and unpacks them to permit functions with only one
+   * parameter to be defined without expecting their argument to be wrapped in a Tuple1 instance.
+   *
+   * @tparam T is the type of the output function argument.
+   * @param tuple to convert.
+   * @return an argument ready to be passed to a model phase function.
+   */
+  def tupleToFnArg[T](tuple: Product): T = {
+    tuple match {
+      case Tuple1(x1) => x1.asInstanceOf[T]
+      case other => other.asInstanceOf[T]
+    }
+  }
+
+  /**
+   * Converts a function return value into a tuple. Handles the case where the provided result is
+   * not a tuple by wrapping it in a Tuple1 instance.
+   *
+   * @param result from a model phase function.
+   * @return a processed tuple.
+   */
+  def fnResultToTuple(result: Any): Product = {
+    result match {
+      case tuple: Tuple1[_] => tuple
+      case tuple: Tuple2[_, _] => tuple
+      case tuple: Tuple3[_, _, _] => tuple
+      case tuple: Tuple4[_, _, _, _] => tuple
+      case tuple: Tuple5[_, _, _, _, _] => tuple
+      case tuple: Tuple6[_, _, _, _, _, _] => tuple
+      case tuple: Tuple7[_, _, _, _, _, _, _] => tuple
+      case tuple: Tuple8[_, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple9[_, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple10[_, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple11[_, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple12[_, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple13[_, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple14[_, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple15[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple16[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple17[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple18[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple19[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple20[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple21[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case tuple: Tuple22[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
+      case other => Tuple1(other)
+    }
+  }
+
+  /**
+   * Converts a sequence to a tuple.
+   *
+   * @tparam T is the type of the output tuple.
+   * @param sequence to convert.
+   * @return a tuple converted from the provided sequence.
+   */
+  def seqToTuple[T <: Product](sequence: Seq[_]): T = {
+    val tuple = sequence match {
+      case Seq(x1) => {
+        Tuple1(x1)
+      }
+      case Seq(x1, x2) => {
+        Tuple2(x1, x2)
+      }
+      case Seq(x1, x2, x3) => {
+        Tuple3(x1, x2, x3)
+      }
+      case Seq(x1, x2, x3, x4) => {
+        Tuple4(x1, x2, x3, x4)
+      }
+      case Seq(x1, x2, x3, x4, x5) => {
+        Tuple5(x1, x2, x3, x4, x5)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6) => {
+        Tuple6(x1, x2, x3, x4, x5, x6)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7) => {
+        Tuple7(x1, x2, x3, x4, x5, x6, x7)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8) => {
+        Tuple8(x1, x2, x3, x4, x5, x6, x7, x8)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9) => {
+        Tuple9(x1, x2, x3, x4, x5, x6, x7, x8, x9)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10) => {
+        Tuple10(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11) => {
+        Tuple11(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12) => {
+        Tuple12(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13) => {
+        Tuple13(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14) => {
+        Tuple14(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15) => {
+        Tuple15(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16) => {
+        Tuple16(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17) => {
+        Tuple17(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18) => {
+        Tuple18(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
+          x19) => {
+        Tuple19(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
+            x19)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
+          x19, x20) => {
+        Tuple20(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
+            x19, x20)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
+          x19, x20, x21) => {
+        Tuple21(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
+            x19, x20, x21)
+      }
+      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
+          x19, x20, x21, x22) => {
+        Tuple22(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
+            x19, x20, x21, x22)
+      }
+    }
+
+    tuple.asInstanceOf[T]
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/flow/util/package.scala b/kiji-express/src/main/scala/org/kiji/express/flow/util/package.scala
new file mode 100644
index 0000000000000000000000000000000000000000..0eb151eb96061dc8b2a2a03ea9216348634b5250
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/flow/util/package.scala
@@ -0,0 +1,25 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+/**
+ * Package containing utility classes for the KijiExpress flow API.
+ */
+package object util
diff --git a/kiji-express/src/main/scala/org/kiji/express/package.scala b/kiji-express/src/main/scala/org/kiji/express/package.scala
new file mode 100644
index 0000000000000000000000000000000000000000..c969e7b4d1926cb5e8c523c70bd1dc783a43f6ea
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/package.scala
@@ -0,0 +1,65 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji
+
+/**
+ * KijiExpress is a domain-specific language for analyzing and modeling data stored in Kiji.
+ *
+ * For more information on Kiji, please visit [[http://www.kiji.org]] and the Kiji Github page at
+ * [[https://github.com/kijiproject]]. KijiExpress is built atop Scalding
+ * ([[https://github.com/twitter/scalding]]), a library for writing MapReduce workflows.
+ *
+ * === Getting started. ===
+ * To use KijiExpress, import the Scalding library, the `express` library, and members of the
+ * flow module [[org.kiji.express.flow]].
+ * {{{
+ *   import com.twitter.scalding._
+ *   import org.kiji.express._
+ *   import org.kiji.express.flow._
+ * }}}
+ * Doing so will import several classes and functions that make it easy to author analysis
+ * pipelines.
+ *
+ * === Working with data from a Kiji table. ===
+ * Scalding represents distributed data sets as a collection of tuples with named fields.
+ * Likewise, KijiExpress represents a row in a Kiji table as a tuple.
+ * [[org.kiji.express.flow]] provides a factory, named `KijiInput`,
+ * that make it easy to specify what columns you want to read from a Kiji table and what names
+ * they should have in row tuples. For example, to read the value of a column named `info:text`
+ * from a table named `postings` into the tuple field named `text`, you could write the following.
+ *
+ * {{{
+ *   KijiInput("kiji://.env/default/postings", "info:text" -> "text")
+ * }}}
+ *
+ * The result of the above expression is a [[org.kiji.express.flow.KijiSource]] (an implementation
+ * of Scalding's `Source`) which represents the rows of the Kiji table as a collection of tuples.
+ * At this point, functional operators can be used to manipulate and analyze the data. For example,
+ * to split the contents of the column `info:text` into individual words,
+ * you could write the following.
+ * {{{
+ *   KijiInput("kiji://.env/default/postings")("info:text" -> 'text)
+ *       .flatMap('text -> 'word) { word: Seq[FlowCell[String]] =>
+ *         word.head.datum.split("""\s+""")
+ *       }
+ * }}}
+ */
+package object express
+
diff --git a/kiji-express/src/main/scala/org/kiji/express/repl/ExpressILoop.scala b/kiji-express/src/main/scala/org/kiji/express/repl/ExpressILoop.scala
new file mode 100644
index 0000000000000000000000000000000000000000..c84628b7f3ae2a4458544fc429ae25498873a74a
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/repl/ExpressILoop.scala
@@ -0,0 +1,107 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.repl
+
+import scala.tools.nsc.interpreter.ILoop
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.schema.shell.ShellMain
+
+/**
+ * A class providing KijiExpress specific commands for inclusion in the KijiExpress REPL.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] class ExpressILoop extends ILoop {
+  /**
+   * Commands specific to the KijiExpress REPL. To define a new command use one of the following
+   * factory methods:
+   * - `LoopCommand.nullary` for commands that take no arguments
+   * - `LoopCommand.cmd` for commands that take one string argument
+   * - `LoopCommand.varargs` for commands that take multiple string arguments
+   */
+  private val expressCommands: List[LoopCommand] = List(
+      LoopCommand.varargs("schema-shell",
+          "",
+          "Runs the KijiSchema shell.",
+          schemaShellCommand)
+  )
+
+  /**
+   * Change the shell prompt to read express&gt;
+   *
+   * @return a prompt string to use for this REPL.
+   */
+  override def prompt: String = "\nexpress> "
+
+  /**
+   * Gets the list of commands that this REPL supports.
+   *
+   * @return a list of the command supported by this REPL.
+   */
+  override def commands: List[LoopCommand] = super.commands ++ expressCommands
+
+  /**
+   * Determines whether the kiji-schema-shell jar is on the classpath.
+   *
+   * @return `true` if kiji-schema-shell is on the classpath, `false` otherwise.
+   */
+  private def isSchemaShellEnabled: Boolean = {
+    try {
+      ShellMain.version()
+      true
+    } catch {
+      case _: NoClassDefFoundError => false
+    }
+  }
+
+  /**
+   * Runs an instance of the KijiSchema Shell within the Scala REPL.
+   *
+   * @param args that should be passed to the instance of KijiSchema Shell to run.
+   * @return the result of running the command, which should always be the default result.
+   */
+  private def schemaShellCommand(args: List[String]): Result = {
+    if (isSchemaShellEnabled) {
+      try {
+        // Create a shell runner and use it to run an instance of the REPL with no arguments.
+        val shellMain = new ShellMain()
+        // Run the shell.
+        val exitCode = shellMain.run()
+        if (exitCode == 0) {
+          Result.resultFromString("KijiSchema Shell exited with success.")
+        } else {
+          Result.resultFromString("KijiSchema Shell exited with code: " + exitCode)
+        }
+      } finally {
+        // Close all connections properly before exiting.
+        ShellMain.shellKijiSystem.shutdown()
+      }
+    } else {
+      Result.resultFromString("The KijiSchema Shell jar is not on the classpath. "
+          + "Set the environment variable SCHEMA_SHELL_HOME to the root of a KijiSchema Shell "
+          + "distribution before running the KijiExpress Shell to enable KijiSchema Shell "
+          + "features.")
+    }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/repl/ExpressShell.scala b/kiji-express/src/main/scala/org/kiji/express/repl/ExpressShell.scala
new file mode 100644
index 0000000000000000000000000000000000000000..645bd3c4bb96b41170b506068d4aef15cbb0d05c
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/repl/ExpressShell.scala
@@ -0,0 +1,147 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.repl
+
+import java.io.File
+import java.io.FileOutputStream
+import java.util.jar.JarEntry
+import java.util.jar.JarOutputStream
+
+import scala.tools.nsc.GenericRunnerCommand
+import scala.tools.nsc.MainGenericRunner
+import scala.tools.nsc.interpreter.ILoop
+import scala.tools.nsc.io.VirtualDirectory
+
+import com.google.common.io.Files
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.annotations.Inheritance
+import org.kiji.express.flow.util.Resources._
+
+/**
+ * A runner for a Scala REPL providing functionality extensions specific to working with
+ * KijiExpress.
+ */
+@ApiAudience.Private
+@ApiStability.Experimental
+@Inheritance.Sealed
+private[express] object ExpressShell extends MainGenericRunner {
+
+  /**
+   * An instance of the Scala REPL the user will interact with.
+   */
+  private[express] var expressREPL: Option[ILoop] = None
+
+  /**
+   * A comma-separated list of paths to jars to add to the distributed cache of run jobs,
+   * provided as a system property through the command line. Framework developers should use this
+   * value when determining jars to ship with jobs.
+   */
+  private[express] val tmpjars: Option[String] = Option(System.getProperty("tmpjars"))
+
+  /**
+   * The main entry point for executing the REPL.
+   *
+   * This method is lifted from [[scala.tools.nsc.MainGenericRunner]] and modified to allow
+   * for custom functionality, including determining at runtime if the REPL is running,
+   * and making custom REPL colon-commands available to the user.
+   *
+   * @param args passed from the command line.
+   * @return `true` if execution was successful, `false` otherwise.
+   */
+  override def process(args: Array[String]): Boolean = {
+    // Process command line arguments into a settings object, and use that to start the REPL.
+    val command = new GenericRunnerCommand(args.toList, (x: String) => errorFn(x))
+    import command.settings
+    expressREPL = Some(new ExpressILoop)
+    expressREPL.get.process(settings)
+  }
+
+  /**
+   * Runs an instance of the shell.
+   *
+   * @param args from the command line.
+   */
+  def main(args: Array[String]) {
+    val retVal = process(args)
+    if (!retVal) {
+      sys.exit(1)
+    }
+  }
+
+  /**
+   * Creates a jar file in a temporary directory containing the code thus far compiled by the REPL.
+   *
+   * @return some file for the jar created, or `None` if the REPL is not running.
+   */
+  private[express] def createReplCodeJar(): Option[File] = {
+    expressREPL.map { repl =>
+      val virtualDirectory = repl.virtualDirectory
+      val tempJar = new File(Files.createTempDir(),
+        "scalding-repl-session-" + System.currentTimeMillis() + ".jar")
+      createJar(virtualDirectory, tempJar)
+    }
+  }
+
+  /**
+   * Creates a jar file from the classes contained in a virtual directory.
+   *
+   * @param virtualDirectory containing classes that should be added to the jar.
+   * @param jarFile that will be written.
+   * @return the jarFile specified and written.
+   */
+  private[express] def createJar(virtualDirectory: VirtualDirectory, jarFile: File): File = {
+    doAndClose(new JarOutputStream(new FileOutputStream(jarFile))) { jarStream =>
+      addVirtualDirectoryToJar(virtualDirectory, "", jarStream)
+    }
+    jarFile
+  }
+
+  /**
+   * Add the contents of the specified virtual directory to a jar. This method will recursively
+   * descend into subdirectories to add their contents.
+   *
+   * @param dir is a virtual directory whose contents should be added.
+   * @param entryPath for classes found in the virtual directory.
+   * @param jarStream for writing the jar file.
+   */
+  private def addVirtualDirectoryToJar(
+      dir: VirtualDirectory,
+      entryPath: String,
+      jarStream: JarOutputStream) {
+    dir.foreach { file =>
+      if (file.isDirectory) {
+        // Recursively descend into subdirectories, adjusting the package name as we do.
+        val dirPath = entryPath + file.name + "/"
+        val entry: JarEntry = new JarEntry(dirPath)
+        jarStream.putNextEntry(entry)
+        jarStream.closeEntry()
+        addVirtualDirectoryToJar(file.asInstanceOf[VirtualDirectory], dirPath, jarStream)
+      } else if (file.hasExtension("class")) {
+        // Add class files as an entry in the jar file and write the class to the jar.
+        val entry: JarEntry = new JarEntry(entryPath + file.name)
+        jarStream.putNextEntry(entry)
+        jarStream.write(file.toByteArray)
+        jarStream.closeEntry()
+      }
+    }
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/repl/Implicits.scala b/kiji-express/src/main/scala/org/kiji/express/repl/Implicits.scala
new file mode 100644
index 0000000000000000000000000000000000000000..d7a352df5a05b69639b3315f056ed27b6f40b202
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/repl/Implicits.scala
@@ -0,0 +1,149 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.repl
+
+import cascading.flow.FlowDef
+import cascading.pipe.Pipe
+import com.twitter.scalding.FieldConversions
+import com.twitter.scalding.IterableSource
+import com.twitter.scalding.RichPipe
+import com.twitter.scalding.Source
+import com.twitter.scalding.TupleConversions
+import com.twitter.scalding.TupleConverter
+import com.twitter.scalding.TupleSetter
+
+import org.kiji.annotations.ApiAudience
+import org.kiji.annotations.ApiStability
+import org.kiji.express.flow.util.AvroTupleConversions
+import org.kiji.express.flow.util.PipeConversions
+
+/**
+ * Object containing various implicit conversions required to create Scalding flows in the REPL.
+ * Most of these conversions come from Scalding's Job class.
+ */
+@ApiAudience.Framework
+@ApiStability.Experimental
+object Implicits
+    extends PipeConversions
+    with TupleConversions
+    with FieldConversions
+    with AvroTupleConversions {
+  /** Implicit flowDef for this KijiExpress shell session. */
+  implicit var flowDef: FlowDef = getEmptyFlowDef
+
+  /**
+   * Sets the flow definition in implicit scope to an empty flow definition.
+   */
+  def resetFlowDef() {
+    flowDef = getEmptyFlowDef
+  }
+
+  /**
+   * Gets a new, empty, flow definition.
+   *
+   * @return a new, empty flow definition.
+   */
+  private[express] def getEmptyFlowDef: FlowDef = {
+    val fd = new FlowDef
+    fd.setName("ExpressShell")
+    fd
+  }
+
+  /**
+   * Converts a Cascading Pipe to a Scalding RichPipe. This method permits implicit conversions from
+   * Pipe to RichPipe.
+   *
+   * @param pipe to convert to a RichPipe.
+   * @return a RichPipe wrapping the specified Pipe.
+   */
+  implicit def pipeToRichPipe(pipe: Pipe): RichPipe = new RichPipe(pipe)
+
+  /**
+   * Converts a Scalding RichPipe to a Cascading Pipe. This method permits implicit conversions from
+   * RichPipe to Pipe.
+   *
+   * @param richPipe to convert to a Pipe.
+   * @return the Pipe wrapped by the specified RichPipe.
+   */
+  implicit def richPipeToPipe(richPipe: RichPipe): Pipe = richPipe.pipe
+
+  /**
+   * Converts a Source to a RichPipe. This method permits implicit conversions from Source to
+   * RichPipe.
+   *
+   * @param source to convert to a RichPipe.
+   * @return a RichPipe wrapping the result of reading the specified Source.
+   */
+  implicit def sourceToRichPipe(source: Source): RichPipe = RichPipe(source.read)
+
+  /**
+   * Converts a Source to a Pipe. This method permits implicit conversions from Source to Pipe.
+   *
+   * @param source to convert to a Pipe.
+   * @return a Pipe that is the result of reading the specified Source.
+   */
+  implicit def sourceToPipe(source: Source): Pipe = source.read
+
+  /**
+   * Converts an iterable into a Source with index (int-based) fields.
+   *
+   * @param iterable to convert into a Source.
+   * @param setter implicitly retrieved and used to convert the specified iterable into a Source.
+   * @param converter implicitly retrieved and used to convert the specified iterable into a Source.
+   * @return a Source backed by the specified iterable.
+   */
+  implicit def iterableToSource[T](
+      iterable: Iterable[T])
+      (implicit setter: TupleSetter[T],
+          converter: TupleConverter[T]): Source = {
+    IterableSource[T](iterable)(setter, converter)
+  }
+
+  /**
+   * Converts an iterable into a Pipe with index (int-based) fields.
+   *
+   * @param iterable to convert into a Pipe.
+   * @param setter implicitly retrieved and used to convert the specified iterable into a Pipe.
+   * @param converter implicitly retrieved and used to convert the specified iterable into a Pipe.
+   * @return a Pipe backed by the specified iterable.
+   */
+  implicit def iterableToPipe[T](
+      iterable: Iterable[T])
+      (implicit setter: TupleSetter[T],
+          converter: TupleConverter[T]): Pipe = {
+    iterableToSource(iterable)(setter, converter).read
+  }
+
+  /**
+   * Converts an iterable into a RichPipe with index (int-based) fields.
+   *
+   * @param iterable to convert into a RichPipe.
+   * @param setter implicitly retrieved and used to convert the specified iterable into a RichPipe.
+   * @param converter implicitly retrieved and used to convert the specified iterable into a
+   *     RichPipe.
+   * @return a RichPipe backed by the specified iterable.
+   */
+  implicit def iterableToRichPipe[T](
+      iterable: Iterable[T])
+      (implicit setter: TupleSetter[T],
+          converter: TupleConverter[T]): RichPipe = {
+    RichPipe(iterableToPipe(iterable)(setter, converter))
+  }
+}
diff --git a/kiji-express/src/main/scala/org/kiji/express/repl/package.scala b/kiji-express/src/main/scala/org/kiji/express/repl/package.scala
new file mode 100644
index 0000000000000000000000000000000000000000..b184a200a37951b7c8dbc61cccc716851ac170ec
--- /dev/null
+++ b/kiji-express/src/main/scala/org/kiji/express/repl/package.scala
@@ -0,0 +1,31 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express
+
+/**
+ * Package containing classes/objects supporting the KijiExpress shell. Includes classes required
+ * for integrating with the scala REPL. To launch the KijiExpress shell, run the following express
+ * command:
+ *
+ * {{{
+ *   express shell
+ * }}}
+ */
+package object repl
diff --git a/kiji-express/src/main/scalastyle/scalastyle_config.xml b/kiji-express/src/main/scalastyle/scalastyle_config.xml
new file mode 100644
index 0000000000000000000000000000000000000000..3079d60919f85a0f24b44fc6db302a8ae16c00a3
--- /dev/null
+++ b/kiji-express/src/main/scalastyle/scalastyle_config.xml
@@ -0,0 +1,73 @@
+<!--
+    (c) Copyright 2013 WibiData, Inc.
+
+    See the NOTICE file distributed with this work for additional
+    information regarding copyright ownership.
+
+    Licensed under the Apache License, Version 2.0 (the "License");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+        http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+-->
+<scalastyle>
+  <name>Kiji scalastyle configuration</name>
+  <check enabled="true" class="org.scalastyle.file.FileLineLengthChecker" level="error">
+    <parameters>
+      <parameter name="maxLineLength"><![CDATA[100]]></parameter>
+      <parameter name="tabSize"><![CDATA[2]]></parameter>
+    </parameters>
+  </check>
+  <check enabled="true" class="org.scalastyle.file.FileTabChecker" level="error"></check>
+  <check enabled="true" class="org.scalastyle.file.HeaderMatchesChecker" level="error">
+    <parameters>
+      <parameter name="header"><![CDATA[/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */]]>
+      </parameter>
+    </parameters>
+  </check>
+  <check enabled="true" class="org.scalastyle.file.NewLineAtEofChecker" level="error"></check>
+  <check enabled="true" class="org.scalastyle.file.WhitespaceEndOfLineChecker" level="error"></check>
+  <check enabled="true" class="org.scalastyle.scalariform.ClassNamesChecker" level="error">
+    <parameters>
+      <parameter name="regex"><![CDATA[^[A-Z][A-Za-z0-9]*$]]></parameter>
+    </parameters>
+  </check>
+  <check enabled="true" class="org.scalastyle.scalariform.ObjectNamesChecker" level="error">
+    <parameters>
+      <parameter name="regex"><![CDATA[^[A-Z][A-Za-z0-9]*$]]></parameter>
+    </parameters>
+  </check>
+  <check enabled="true" class="org.scalastyle.scalariform.PackageObjectNamesChecker" level="error">
+    <parameters>
+      <parameter name="regex"><![CDATA[^[a-z][A-Za-z0-9]*$]]></parameter>
+    </parameters>
+  </check>
+  <check enabled="true" class="org.scalastyle.scalariform.IfBraceChecker" level="error"></check>
+  <check enabled="true" class="org.scalastyle.scalariform.NoFinalizeChecker" level="error"></check>
+  <check enabled="true" class="org.scalastyle.scalariform.NoWhitespaceAfterLeftBracketChecker" level="error"></check>
+  <check enabled="true" class="org.scalastyle.scalariform.NoWhitespaceBeforeLeftBracketChecker" level="error"></check>
+  <check enabled="true" class="org.scalastyle.scalariform.PublicMethodsHaveTypeChecker" level="error"></check>
+</scalastyle>
diff --git a/kiji-express/src/main/scripts/express b/kiji-express/src/main/scripts/express
new file mode 100755
index 0000000000000000000000000000000000000000..3c27bbd7885d1b1efadd71066acbe7efde001acc
--- /dev/null
+++ b/kiji-express/src/main/scripts/express
@@ -0,0 +1,581 @@
+#!/usr/bin/env bash
+#
+#   (c) Copyright 2013 WibiData, Inc.
+#
+#   See the NOTICE file distributed with this work for additional
+#   information regarding copyright ownership.
+#
+#   Licensed under the Apache License, Version 2.0 (the "License");
+#   you may not use this file except in compliance with the License.
+#   You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#   Unless required by applicable law or agreed to in writing, software
+#   distributed under the License is distributed on an "AS IS" BASIS,
+#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#   See the License for the specific language governing permissions and
+#   limitations under the License.
+#
+#
+#   The express script provides tools for running KijiExpress scripts and interacting with the
+#   KijiExpress system.
+#   Tools are run as:
+#
+#   bash> $EXPRESS_HOME/bin/express <tool-name> [options]
+#
+#   For full usage information, use:
+#
+#   bash> $EXPRESS_HOME/bin/express help
+#
+
+# Resolve a symlink to its absolute target, like how 'readlink -f' works on Linux.
+function resolve_symlink() {
+  TARGET_FILE=${1}
+
+  if [ -z "$TARGET_FILE" ]; then
+    echo ""
+    return 0
+  fi
+
+  cd $(dirname "$TARGET_FILE")
+  TARGET_FILE=$(basename "$TARGET_FILE")
+
+  # Iterate down a (possible) chain of symlinks
+  count=0
+  while [ -L "$TARGET_FILE" ]; do
+    if [ "$count" -gt 1000 ]; then
+      # Just stop here, we've hit 1,000 recursive symlinks. (cycle?)
+      break
+    fi
+
+    TARGET_FILE=$(readlink "$TARGET_FILE")
+    cd $(dirname "$TARGET_FILE")
+    TARGET_FILE=$(basename "$TARGET_FILE")
+    count=$(( $count + 1 ))
+  done
+
+  # Compute the canonicalized name by finding the physical path
+  # for the directory we're in and appending the target file.
+  PHYS_DIR=$(pwd -P)
+  RESULT="$PHYS_DIR/$TARGET_FILE"
+  echo "$RESULT"
+}
+
+prgm="$0"
+prgm=`resolve_symlink "$prgm"`
+bin=`dirname "$prgm"`
+bin=`cd "${bin}" && pwd`
+
+EXPRESS_HOME="${EXPRESS_HOME:-${bin}/../}"
+
+# Any arguments you want to pass to KijiExpress's jvm may be done via this env var.
+EXPRESS_JAVA_OPTS=${EXPRESS_JAVA_OPTS:-""}
+
+# This is a workaround for OS X Lion, where a bug in JRE 1.6
+# creates a lot of 'SCDynamicStore' errors.
+if [ "$(uname)" == "Darwin" ]; then
+  EXPRESS_JAVA_OPTS="$EXPRESS_JAVA_OPTS -Djava.security.krb5.realm= -Djava.security.krb5.kdc="
+fi
+
+# An existing set of directories to use for the java.library.path property should
+# be set with JAVA_LIBRARY_PATH.
+JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH:-""}
+
+# Try CDH defaults.
+HBASE_HOME="${HBASE_HOME:-/usr/lib/hbase}"
+HADOOP_HOME="${HADOOP_HOME:-/usr/lib/hadoop}"
+
+# First make sure we have everything we need in the environment.
+if [ -z "${EXPRESS_HOME}" -o ! -d "${EXPRESS_HOME}" ]; then
+  echo "Please set your EXPRESS_HOME environment variable."
+  exit 1
+fi
+if [ -z "${HBASE_HOME}" -o ! -d "${HBASE_HOME}" ]; then
+  echo "Please set your HBASE_HOME environment variable."
+  exit 1
+fi
+if [ -z "${HADOOP_HOME}" -o ! -d "${HADOOP_HOME}" ]; then
+  echo "Please set your HADOOP_HOME environment variable."
+  exit 1
+fi
+
+if [ -z "${1}" ]; then
+  echo "express: Tool launcher for KijiExpress."
+  echo "Run 'express help' to see a list of available tools."
+  exit 1
+fi
+
+# Removes classpath entries that match the given regexp (partial match, not full
+# match).
+function remove_classpath_entries() {
+  local cp=${1}
+  local regex=${2}
+
+  echo $cp | sed "s/[^:]*$regex[^:]*/::/g" | sed 's/::*/:/g'
+  return 0
+}
+
+# Helper to build classpaths correctly
+function append_path() {
+  if [ -z "${1}" ]; then
+    echo ${2}
+  else
+    echo ${1}:${2}
+  fi
+}
+
+# Scrubs classpaths of a given jar. Mutate will dig into *s, only mutating them
+# if it finds the given jar.
+# mutate_classpath scrubme.jar "$(hadoop classpath)"
+function mutate_classpath () {
+  local mutated_classpath
+  local jar_to_scrub=${1}
+  shift
+
+  # Stop expanding globs
+  set -f
+  IFS=: read -r -a classpath <<< ${@}
+
+  for path in $classpath; do
+    # If it ends with a glob we'll need to dig deeper for jars
+    if [ "${path: -1:1}" = "*" ]; then
+      set +f
+      local expanded_classpath=$(JARS=(${path}.jar); IFS=:; echo "${JARS[*]}")
+      set -f
+
+      # If the expanded classpath contains the jar in question, we'll
+      # scrub it later.
+      if [[ $expanded_classpath =~ .*$jar_to_scrub.* ]]; then
+        mutated_classpath=$(append_path $mutated_classpath $expanded_classpath)
+
+      # If the expanded classpath doesn't contain the jar in question, use
+      # the glob version to reduce clutter.
+      else
+        mutated_classpath=$(append_path $mutated_classpath $path)
+      fi
+    # No glob just use the path
+    else
+      mutated_classpath=$(append_path $mutated_classpath $path)
+    fi
+  done
+
+  # Scrub all instances of the jar
+  mutated_classpath=$(remove_classpath_entries "$mutated_classpath" "$jar_to_scrub")
+  echo $mutated_classpath
+
+  set +f
+}
+
+# Detect and extract the current Hadoop version number. e.g. "Hadoop 2.x-..." -> "2" You can
+# override this with $KIJI_HADOOP_DISTRO_VER (e.g. "hadoop1" or "hadoop2").
+function extract_hadoop_major_version() {
+  hadoop_major_version=$(${HADOOP_HOME}/bin/hadoop version | head -1 | cut -c 8)
+  if [ -z "${hadoop_major_version}" -a -z "${KIJI_HADOOP_DISTRO_VER}" ]; then
+    echo "Warning: Unknown Hadoop version. May not be able to load all Kiji jars."
+    echo "Set KIJI_HADOOP_DISTRO_VER to 'hadoop1' or 'hadoop2' to load these."
+  else
+    KIJI_HADOOP_DISTRO_VER=${KIJI_HADOOP_DISTRO_VER:-"hadoop${hadoop_major_version}"}
+  fi
+}
+
+# Extracts the --libjars value from the command line and appends/prepends this value to any
+# classpath variables.To ensure that --libjars doesn't get passed into the scalding_tool, this
+# function effectively rebuilds the remainder of the command line arguments by removing the
+# --libjars argument and value. The result is stored in a new global variable called ${COMMAND_ARGS}
+# which is used by other parts of the script in lieu of ${@}.
+function extract_classpath() {
+  return_args=""
+  while (( "${#}" )); do
+    if [ "${1}" == "--libjars" ]; then
+      libjars_cp=${2}
+      shift
+    else
+      return_args="${return_args} ${1}"
+    fi
+    shift
+  done
+  COMMAND_ARGS=${return_args}
+
+  # Gather the express dependency jars.
+  if [ -z "${KIJI_HOME}" -o ! -d "${KIJI_HOME}" ]; then
+   echo "Please set your KIJI_HOME environment variable."
+   exit 1
+  fi
+  if [ -z "${KIJI_MR_HOME}" -o ! -d "${KIJI_MR_HOME}" ]; then
+   echo "Please set your KIJI_MR_HOME environment variable."
+   exit 1
+  fi
+
+  # Add KijiExpress specific jars.
+  express_libjars="${EXPRESS_HOME}/lib/*"
+
+  # If SCHEMA_SHELL_HOME is set, add kiji-schema-shell jars to the classpath to enable features
+  # that require it.
+  schema_shell_libjars=""
+  if [[ -n "${SCHEMA_SHELL_HOME}" ]]; then
+    schema_shell_libjars="${SCHEMA_SHELL_HOME}/lib/*"
+  fi
+  # We may have Hadoop distribution-specific jars to load in
+  # $KIJI_HOME/lib/distribution/hadoopN, where N is the major digit of the Hadoop
+  # version. Only load at most one such set of jars.
+  extract_hadoop_major_version
+
+  # If MODELING_HOME is set, add kiji-modeling jars to the classpath to enable using the express
+  # script with kiji-modeling.  THIS IS A HACK!
+  # TODO(EXP-265): Remove this when kiji-modeling has its own tool launcher.
+  modeling_libjars=""
+  if [[ -n "${MODELING_HOME}" ]]; then
+    modeling_libjars="${MODELING_HOME}/lib/*"
+  fi
+
+  # Add KijiMR distribution specific jars.
+  if [[ -n "${KIJI_MR_HOME}" && "${KIJI_HOME}" != "${KIJI_MR_HOME}" ]]; then
+    mr_libjars="${KIJI_MR_HOME}/lib/*"
+    mr_distrodirs="${KIJI_MR_HOME}/lib/distribution/${KIJI_HADOOP_DISTRO_VER}"
+    if [ -d "${mr_distrodirs}" ]; then
+      mr_distrojars="${mr_distrodirs}/*"
+    fi
+  fi
+
+  # Add KijiSchema distribution specific jars.
+  schema_libjars="${libjars}:${KIJI_HOME}/lib/*"
+  schema_distrodir="$KIJI_HOME/lib/distribution/$KIJI_HADOOP_DISTRO_VER"
+  if [ -d "${schema_distrodir}" ]; then
+    schema_distrojars="${schema_distrodir}/*"
+  fi
+
+  # Compose everything together into a classpath.
+  libjars="${express_libjars}:${mr_distrojars}:${mr_libjars}:${schema_distrojars}:${schema_libjars}:${schema_shell_libjars}:${modeling_libjars}"
+
+  # Gather the HBase classpath.
+  hbase_cp=$(${HBASE_HOME}/bin/hbase classpath)
+  hbase_cp=$(mutate_classpath 'slf4j-log4j12' "${hbase_cp}")
+
+  # Hadoop classpath
+  hadoop_cp=$(${HADOOP_HOME}/bin/hadoop classpath)
+  hadoop_cp=$(mutate_classpath 'slf4j-log4j12' "${hadoop_cp}")
+
+  # Note that we put the libjars before the hbase jars, in case there are conflicts.
+  express_conf=${EXPRESS_HOME}/conf
+  # We put $libjars_cp at the beginning classpath to allow users to win when there are
+  # conflicts.
+  express_cp="${libjars_cp}:${express_conf}:${libjars}:${hadoop_cp}:${hbase_cp}"
+
+  # Use parts of the classpath to determine jars to send with jobs through the distributed cache.
+  tmpjars_cp="${libjars_cp}:${libjars}"
+  tmpjars=$(java -cp ${express_cp} org.kiji.express.flow.tool.TmpJarsTool ${tmpjars_cp})
+
+  # Determine location of Hadoop native libraries and set java.library.path.
+  if [ -d "${HADOOP_HOME}/lib/native" ]; then
+    JAVA_PLATFORM=`java -cp ${hadoop_cp} -Xmx32m org.apache.hadoop.util.PlatformName | sed -e "s/ /_/g"`
+    if [ -d "${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}" ]; then
+      # if $HADOOP_HOME/lib/native/$JAVA_PLATFORM exists, use native libs from there.
+      if [ ! -z "${JAVA_LIBRARY_PATH}" ]; then
+        JAVA_LIBRARY_PATH="${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}"
+      else
+        JAVA_LIBRARY_PATH="${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}"
+      fi
+    elif [ -d "${HADOOP_HOME}/lib/native" ]; then
+      # If not, check for a global $HADOOP_HOME/lib/native/ and just use that dir.
+      if [ ! -z "${JAVA_LIBRARY_PATH}" ]; then
+        JAVA_LIBRARY_PATH="${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native/"
+      else
+        JAVA_LIBRARY_PATH="${HADOOP_HOME}/lib/native/"
+      fi
+    fi
+  fi
+}
+
+function jar_usage() {
+  echo "Usage: express jar <jarFile> <mainClass> [args...]"
+  echo "       express job <jarFile> <jobClass> [args...]"
+}
+
+function jar_command() {
+  if [[ ${#} > 0 && ${1} == "--help" ]]; then
+    jar_usage
+    echo
+    exit 0
+  fi
+  user_target=${1}
+  class=${2}
+  shift 2
+  COMMAND_ARGS=${@}
+  if [ -z "${user_target}" ]; then
+    echo "Error: no jar file specified."
+    jar_usage
+    exit 1
+  fi
+  if [ ! -f "${user_target}" ]; then
+    echo "Error: cannot find jar file ${user_target}"
+    jar_usage
+    exit 1
+  fi
+  if [ -z "${class}" ]; then
+    echo "Error: no main class specified."
+    jar_usage
+    exit 1
+  fi
+  express_cp="${user_target}:${express_cp}"
+}
+
+function print_tool_usage() {
+  echo 'The express script can run programs written using KijiExpress.'
+  echo
+  echo 'USAGE'
+  echo
+  echo '  express <command> [--libjars <list of dependency jars separated by colon> <args>'
+  echo
+  echo 'COMMANDS'
+  echo
+  echo '  help          - Displays this help message. Use --verbose for more information.'
+  echo '  shell         - Starts an interactive shell for running KijiExpress code.'
+  echo '  schema-shell  - Starts KijiSchema Shell loaded with KijiExpress extensions.'
+    'removed in a future release.'
+  echo '  job           - Runs a compiled KijiExpress job.'
+  echo '  jar           - Runs an arbitrary Scala or Java program.'
+  echo '  classpath     - Prints the classpath used to run KijiExpress.'
+  echo
+}
+
+function print_env_usage() {
+  echo
+  echo "ENVIRONMENT VARIABLES"
+  echo
+  echo "  Users can set several environment variables to change the behavior of the express"
+  echo "  script."
+  echo "  These include:"
+  echo
+  echo "  EXPRESS_JAVA_OPTS   Should contain extra arguments to pass to the JVM used to run"
+  echo "                      KijiExpress. By default, EXPRESS_JAVA_OPTS is empty."
+  echo
+  echo "  JAVA_LIBRARY_PATH   Should contain a colon-separated list of paths to additional native"
+  echo "                      libraries to pass to the JVM (through the java.library.path"
+  echo "                      property). Note the express script will always pass the native"
+  echo "                      libraries included with your Hadoop distribution to the JVM. By"
+  echo "                      default JAVA_LIBRARY_PATH is empty."
+}
+
+command=${1}
+
+case ${command} in
+  help)
+    shift
+    print_tool_usage
+    if [[ ${1} == "--verbose" ]]; then
+      print_env_usage
+    fi
+    exit 0
+    ;;
+
+  classpath)
+    shift
+    extract_classpath "${@}"
+    echo "${express_cp}"
+    exit 0
+    ;;
+
+  job)
+    shift  # pop off the command
+    extract_classpath "${@}"
+    jar_command ${COMMAND_ARGS}
+    if [[ "${COMMAND_ARGS}" != *--hdfs* ]] && [[ "${COMMAND_ARGS}" != *--local* ]]; then
+      # Default run mode is local.
+      run_mode_flag="--local"
+    fi
+    scalding_tool="com.twitter.scalding.Tool"
+    ;;
+
+  jar)
+    shift  # pop off the command
+    extract_classpath "${@}"
+    jar_command ${COMMAND_ARGS}
+    ;;
+
+  schema-shell)
+    shift # pop off command
+    # Check if SCHEMA_SHELL_HOME is set. If not we cannot run the shell.
+    if [ -z "${SCHEMA_SHELL_HOME}" ]; then
+      echo "The environment variable SCHEMA_SHELL_HOME is undefined, and so KijiSchema Shell"
+      echo "cannot be run. Please set SCHEMA_SHELL_HOME to the path to a KijiSchema Shell"
+      echo "distribution and try again."
+      exit 1
+    fi
+    extract_classpath "${@}"
+    schema_shell_script="${SCHEMA_SHELL_HOME}/bin/kiji-schema-shell"
+    # We'll add express dependencies to KIJI_CLASSPATH so that they are picked up by
+    # kiji-schema-shell.
+    export KIJI_CLASSPATH="${KIJI_CLASSPATH}:${express_cp}"
+    # Pass tmpjars to kiji-schema-shell using a JVM property, which express's kiji-schema-shell
+    # module knows to read and use to populate tmpjars for launched jobs.
+    JAVA_OPTS="${JAVA_OPTS} -Dexpress.tmpjars=${tmpjars}"
+    # Also specify that schema validation should be disabled.
+    JAVA_OPTS="${JAVA_OPTS} -Dorg.kiji.schema.impl.AvroCellEncoder.SCHEMA_VALIDATION=DISABLED"
+    export JAVA_OPTS
+    # Invoke the kiji-schema-shell command with the express modules preloaded.
+    ${schema_shell_script} --modules=modeling ${COMMAND_ARGS}
+    exit $?
+    ;;
+
+  shell)
+    shift # pop off the command
+
+    # Pass a classpath to run the shell with and jars to ship with the distributed cache to
+    # the shell runner.
+    extract_classpath "${@}"
+    export EXPRESS_CP="${express_cp}"
+    export TMPJARS="${tmpjars}"
+
+    # Pass a run mode to the shell runner.
+    local_mode_script="${bin}/local-mode.scala"
+    hdfs_mode_script="${bin}/hdfs-mode.scala"
+    if [[ "${COMMAND_ARGS}" == *--hdfs* ]]; then
+      export EXPRESS_MODE="${hdfs_mode_script}"
+      COMMAND_ARGS=$(echo "${COMMAND_ARGS}" | sed s/--hdfs//)
+    elif [[ "${COMMAND_ARGS}" == *--local* ]]; then
+      export EXPRESS_MODE="${local_mode_script}"
+      COMMAND_ARGS=$(echo "${COMMAND_ARGS}" | sed s/--local//)
+    else
+      export EXPRESS_MODE="${local_mode_script}"
+    fi
+
+    # Run the shell
+    "${bin}/express-shell" ${COMMAND_ARGS}
+    exit $?
+    ;;
+  *)
+    echo "Unknown command: ${command}"
+    echo "Try:"
+    echo "  express help"
+    exit 1
+    ;;
+esac
+
+export EXPRESS_JAVA_OPTS
+
+java_opts=
+if [ ! -z "${JAVA_LIBRARY_PATH}" ]; then
+  java_opts="${java_opts} -Djava.library.path=${JAVA_LIBRARY_PATH}"
+fi
+
+# Run it!
+if [ -z "${scalding_tool}" ]; then
+  # In this case the user is running an arbitrary jar with express code on the classpath.
+  # TODO EXP-243: Until we add a keyword in the express/modeling projects to run a model
+  # lifecycle tool, we use `express jar` to run it. You may need to add the following
+  # flags for this to work: "-Dtmpjars=${tmpjars}" and
+  # "-Dmapreduce.task.classpath.user.precedence=true"
+  exec java \
+    -cp "${express_cp}" ${java_opts} ${EXPRESS_JAVA_OPTS} \
+    "${class}" ${COMMAND_ARGS}
+else
+  canonical_user_target=$(resolve_symlink ${user_target})
+  if [ -z "${canonical_user_target}" ]; then
+    echo "File does not exist: ${user_target}"
+    exit 1
+  fi
+
+  # If the user has put any -Dhadoop.arg=value elements in their arguments ($COMMAND_ARGS),
+  # then we need to extract these left-justified arguments from COMMAND_ARGS and add them
+  # to HADOOP_ARGS to pass as argments to scalding_tool to be parsed by Hadoop's
+  # GenericOptionsParser. The remaining arguments must be delivered as the final arguments
+  # to scalding_tool after its other arguments.
+
+  # We define and then execute two methods on $COMMAND_ARGS to do this separation.
+
+  function get_hadoop_argv() {
+    out=""
+    while [ ! -z "${1}" ]; do
+      if [ "${1}" == "-D" ]; then
+        out="${out} -D ${2}"
+        shift # Consume -D
+        shift # Consume prop=val
+      elif [[ "${1}" == -D* ]]; then
+        # Argument matches -Dprop=val.
+        # Note [[ eval ]] and lack of "quotes" around -D*.
+        out="${out} ${1}"
+        shift
+      elif [ "${1}" == "-conf" ]; then
+        out="${out} -conf ${2}"
+        shift # Consume -conf
+        shift # Consume <configuration file>
+      elif [ "${1}" == "-fs" ]; then
+        out="${out} -fs ${2}"
+        shift # Consume -fs
+        shift # Consume <local|namenode:port>
+      elif [ "${1}" == "-jt" ]; then
+        out="${out} -jt ${2}"
+        shift # Consume -jt
+        shift # Consume <local|jobtracker:port>
+      elif [ "${1}" == "-archives" ]; then
+        out="${out} -archives ${2}"
+        shift # Consume -archives
+        shift # Consume <comma separated list of jars>
+      else
+        break # Matched a non -D argument; stop parsing.
+      fi
+    done
+
+    # Echo all the -Dargs.
+    echo "$out"
+  }
+
+  # Return the part of COMMAND_ARGS that does not contain leading -D prop=val or -Dprop-val
+  function get_user_argv() {
+    while [ ! -z "${1}" ]; do
+      if [ "${1}" == "-D" ]; then
+        shift
+        shift # Consume this and the following prop=val
+      elif [[ "${1}" == -D* ]]; then
+        shift # Consume -Dprop=val
+      elif [ "${1}" == "-conf" ]; then
+        shift # Consume -conf
+        shift # Consume <configuration file>
+      elif [ "${1}" == "-fs" ]; then
+        shift # Consume -fs
+        shift # Consume <local|namenode:port>
+      elif [ "${1}" == "-jt" ]; then
+        shift # Consume -jt
+        shift # Consume <local|jobtracker:port>
+      elif [ "${1}" == "-archives" ]; then
+        shift # Consume -archives
+        shift # Consume <comma separated list of jars>
+      else
+        break
+      fi
+    done
+
+    # Echo the remaining args
+    echo $*
+  }
+
+  hadoop_argv=$(get_hadoop_argv $COMMAND_ARGS)
+  user_argv=$(get_user_argv $COMMAND_ARGS)
+
+  if [ -z "${class}" ]; then
+    # In this case the user is running an uncompiled script.
+    exec java \
+      -cp "${express_cp}" ${java_opts} ${EXPRESS_JAVA_OPTS} \
+      -Dorg.kiji.schema.impl.AvroCellEncoder.SCHEMA_VALIDATION=DISABLED \
+      ${scalding_tool} \
+      "-Dtmpjars=${tmpjars}" \
+      "-Dmapreduce.task.classpath.user.precedence=true" \
+      ${hadoop_argv} \
+      "${canonical_user_target}" \
+      "${run_mode_flag}" \
+      ${user_argv}
+  else
+    # In this case the user is running a compiled Scalding Job in a jar.
+    exec java \
+      -cp "${express_cp}" ${java_opts} ${EXPRESS_JAVA_OPTS} \
+      -Dorg.kiji.schema.impl.AvroCellEncoder.SCHEMA_VALIDATION=DISABLED \
+      ${scalding_tool} \
+      "-Dtmpjars=file://${canonical_user_target},${tmpjars}" \
+      "-Dmapreduce.task.classpath.user.precedence=true" \
+      ${hadoop_argv} \
+      "${class}" \
+      "${run_mode_flag}" \
+      ${user_argv}
+  fi
+fi
diff --git a/kiji-express/src/main/scripts/express-shell b/kiji-express/src/main/scripts/express-shell
new file mode 100755
index 0000000000000000000000000000000000000000..3ec658dcb37932d6257c5ada44f506296e7e23bb
--- /dev/null
+++ b/kiji-express/src/main/scripts/express-shell
@@ -0,0 +1,172 @@
+#!/bin/bash --posix
+#
+#   (c) Copyright 2013 WibiData, Inc.
+#
+#   See the NOTICE file distributed with this work for additional
+#   information regarding copyright ownership.
+#
+#   Licensed under the Apache License, Version 2.0 (the "License");
+#   you may not use this file except in compliance with the License.
+#   You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#   Unless required by applicable law or agreed to in writing, software
+#   distributed under the License is distributed on an "AS IS" BASIS,
+#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#   See the License for the specific language governing permissions and
+#   limitations under the License.
+#
+# Provides an entry point for running a KijiExpress-specific Scala shell.
+
+# Identify the bin dir in the distribution from which this script is running.
+bin=$(dirname $0)
+bin=$(cd ${bin} && pwd)
+
+# Set the main class used to run the KijiExpress Scala REPL.
+main_class="org.kiji.express.repl.ExpressShell"
+
+# This script should be run from the express script, which should export an environment variable
+# named EXPRESS_CP
+if [[ -z "${EXPRESS_CP}" ]]; then
+  echo "The environment variable EXPRESS_CP is undefined."
+  echo "If you are trying to run the KijiExpress shell, use the express command to do so."
+fi
+
+# It should also export TMPJARS, which should contain a comma-separated list of paths to jars
+# to add to the distributed cache of run jobs.
+if [[ -z "${TMPJARS}" ]]; then
+  echo "The environment variable TMPJARS is undefined."
+  echo "If you are trying to run the KijiExpress shell, use the express command to do so."
+fi
+
+# It should also export EXPRESS_MODE, which should be the path to a script of Scala code the
+# REPL should preload so that Hadoop jobs execute in the Cascading local runner or a Hadoop
+# cluster.
+if [[ -z "${EXPRESS_MODE}" ]]; then
+  echo "The environment variable EXPRESS_MODE is undefined."
+  echo "If you are trying to run the KijiExpress shell, use the express command to do so."
+fi
+
+# There should be a file containing import statements the REPL should execute on startup in the
+# bin directory.
+imports_script="${bin}/imports.scala"
+
+# Not sure what the right default is here: trying nonzero.
+scala_exit_status=1
+saved_stty=""
+
+# restore stty settings (echo in particular)
+function restoreSttySettings() {
+  if [[ -n ${SCALA_RUNNER_DEBUG} ]]; then
+    echo "restoring stty: ${saved_stty}"
+  fi
+
+  stty ${saved_stty}
+  saved_stty=""
+}
+
+function onExit() {
+  if [[ "${saved_stty}" != "" ]]; then
+    restoreSttySettings
+    exit ${scala_exit_status}
+  fi
+}
+
+# to reenable echo if we are interrupted before completing.
+trap onExit INT
+
+# save terminal settings
+saved_stty=$(stty -g 2>/dev/null)
+# clear on error so we don't later try to restore them
+if [[ ! $? ]]; then
+  saved_stty=""
+fi
+if [[ -n ${SCALA_RUNNER_DEBUG} ]]; then
+  echo "saved stty: ${saved_stty}"
+fi
+
+cygwin=false;
+case "$(uname)" in
+    CYGWIN*) cygwin=true ;;
+esac
+
+CYGWIN_JLINE_TERMINAL=
+if ${cygwin}; then
+    if [ "${OS}" = "Windows_NT" ] && cygpath -m .>/dev/null 2>/dev/null ; then
+        format=mixed
+    else
+        format=windows
+    fi
+    WIBI_SHELL_CLASSPATH=`cygpath --path --$format "$WIBI_SHELL_CLASSPATH"`
+    case "${TERM}" in
+        rxvt* | xterm*)
+            stty -icanon min 1 -echo
+            CYGWIN_JLINE_TERMINAL="-Djline.terminal=scala.tools.jline.UnixTerminal"
+        ;;
+    esac
+fi
+
+[ -n "${JAVA_OPTS}" ] || JAVA_OPTS="-Xmx256M -Xms32M"
+
+# break out -D and -J options and add them to JAVA_OPTS as well
+# so they reach the underlying JVM in time to do some good.  The
+# -D options will be available as system properties.
+declare -a java_args
+declare -a scala_args
+
+# Don't use the bootstrap classloader.
+CPSELECT="-classpath "
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    -D*)
+      # pass to scala as well: otherwise we lose it sometimes when we
+      # need it, e.g. communicating with a server compiler.
+      java_args=("${java_args[@]}" "$1")
+      scala_args=("${scala_args[@]}" "$1")
+      shift
+      ;;
+    -J*)
+      # as with -D, pass to scala even though it will almost
+      # never be used.
+      java_args=("${java_args[@]}" "${1:2}")
+      scala_args=("${scala_args[@]}" "$1")
+      shift
+      ;;
+    -toolcp)
+      TOOL_CLASSPATH="$TOOL_CLASSPATH:$2"
+      shift 2
+      ;;
+    *)
+      scala_args=("${scala_args[@]}" "$1")
+      shift
+      ;;
+  esac
+done
+
+# reset "$@" to the remaining args
+set -- "${scala_args[@]}"
+
+if [ -z "${JAVACMD}" -a -n "${JAVA_HOME}" -a -x "${JAVA_HOME}/bin/java" ]; then
+  JAVACMD="${JAVA_HOME}/bin/java"
+fi
+
+"${JAVACMD:=java}" \
+  ${JAVA_OPTS} \
+  "${java_args[@]}" \
+  ${CPSELECT}"${TOOL_CLASSPATH}:${EXPRESS_CP}" \
+  -Dscala.usejavacp=true \
+  -Denv.emacs="${EMACS}" \
+  -Dtmpjars="${TMPJARS}" \
+  ${CYGWIN_JLINE_TERMINAL} \
+  ${main_class} ${@} \
+  -i "${imports_script}" \
+  -i "${EXPRESS_MODE}" \
+  -Yrepl-sync
+# The -Yrepl-sync option is a fix for the 2.9.1 REPL. This should probably not be necessary in the future.
+
+# record the exit status lest it be overwritten:
+# then reenable echo and propagate the code.
+scala_exit_status=$?
+onExit
diff --git a/kiji-express/src/main/scripts/hdfs-mode.scala b/kiji-express/src/main/scripts/hdfs-mode.scala
new file mode 100644
index 0000000000000000000000000000000000000000..4b4d8715be488db506372cf9e8b75486123aa2bf
--- /dev/null
+++ b/kiji-express/src/main/scripts/hdfs-mode.scala
@@ -0,0 +1,21 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+com.twitter.scalding.Mode.mode =
+    com.twitter.scalding.Hdfs(false, org.apache.hadoop.hbase.HBaseConfiguration.create())
+
diff --git a/kiji-express/src/main/scripts/imports.scala b/kiji-express/src/main/scripts/imports.scala
new file mode 100644
index 0000000000000000000000000000000000000000..35a032dd5be9dbd49a17e05cbbc0c21c9e3f464e
--- /dev/null
+++ b/kiji-express/src/main/scripts/imports.scala
@@ -0,0 +1,23 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import com.twitter.scalding._
+import org.kiji.express._
+import org.kiji.express.flow._
+import org.kiji.express.repl.Implicits._
+
diff --git a/kiji-express/src/main/scripts/local-mode.scala b/kiji-express/src/main/scripts/local-mode.scala
new file mode 100644
index 0000000000000000000000000000000000000000..342c4e4f1f678147574f6c206d4f59be14ff28a6
--- /dev/null
+++ b/kiji-express/src/main/scripts/local-mode.scala
@@ -0,0 +1,20 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+com.twitter.scalding.Mode.mode = com.twitter.scalding.Local(false)
+
diff --git a/kiji-express/src/test/avro/TestRecords.avdl b/kiji-express/src/test/avro/TestRecords.avdl
new file mode 100644
index 0000000000000000000000000000000000000000..e66516a47764ad3cf1c5274819e7febf950ad818
--- /dev/null
+++ b/kiji-express/src/test/avro/TestRecords.avdl
@@ -0,0 +1,41 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** This protocol defines avro objects used for testing. */
+@namespace("org.kiji.express.avro")
+protocol Testing {
+  /** Used to test overriding a HashSpec reader schema with a specific record. */
+  record SpecificRecordTest {
+    /** hash_size field exists in HasSpec, so we are able to read it with this specific record. */
+    int hash_size;
+  }
+
+  record SimpleRecord {
+    long l;
+    string s;
+    string o = "default-value";
+  }
+
+  record NameFormats {
+    int snake_case_ugh;
+    int CamelCaseEvenWorse;
+    int camelPupCase;
+    int SCREAMING_SNAKE_CASE_YAH;
+  }
+}
diff --git a/kiji-express/src/test/resources/data/input_lines.txt b/kiji-express/src/test/resources/data/input_lines.txt
new file mode 100644
index 0000000000000000000000000000000000000000..719695c6a62aa7b846b0664ba549e137469892fe
--- /dev/null
+++ b/kiji-express/src/test/resources/data/input_lines.txt
@@ -0,0 +1,5 @@
+key1 10
+key1 20
+key1 30
+key2 10
+key2 20
\ No newline at end of file
diff --git a/kiji-express/src/test/resources/layout/avro-types-1.3.json b/kiji-express/src/test/resources/layout/avro-types-1.3.json
new file mode 100644
index 0000000000000000000000000000000000000000..70f12cf3cbd89691d98360f36c7dfe631a02369c
--- /dev/null
+++ b/kiji-express/src/test/resources/layout/avro-types-1.3.json
@@ -0,0 +1,108 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+{
+  name : "table",
+  keys_format : {
+    encoding : "FORMATTED",
+    salt : {
+      hash_size : 2
+    },
+    components : [ {
+      name : "dummy",
+      type : "STRING"
+    }]
+  },
+  locality_groups : [ {
+    name : "default",
+    in_memory : false,
+    max_versions : 2147483647,
+    ttl_seconds : 2147483647,
+    compression_type : "GZ",
+    families : [ {
+      name : "family",
+      columns : [ {
+        name : "column1",
+        column_schema : {
+          type : "INLINE",
+          value : '{"type": "fixed", "size": 16, "name": "md5"}'
+        }
+      }, {
+        name : "column2",
+        column_schema : {
+          type : "INLINE",
+          value : '"bytes"'
+        }
+      }, {
+        name : "column3",
+        column_schema : {
+          type : "AVRO",
+          default_reader: {
+            json: "{\"type\":\"record\",\"name\":\"HashSpec\",\"namespace\":\"org.kiji.schema.avro\",\"fields\":[{\"name\":\"hash_type\",\"type\":{\"type\":\"enum\",\"name\":\"HashType\",\"doc\":\"MD5 hashing (16 bytes).\",\"symbols\":[\"MD5\"]},\"default\":\"MD5\"},{\"name\":\"hash_size\",\"type\":\"int\",\"default\":16},{\"name\":\"suppress_key_materialization\",\"type\":\"boolean\",\"default\":false}]}"
+          }
+        }
+      }, {
+        name : "column_validated",
+        column_schema : {
+          type : "AVRO",
+          avro_validation_policy : "DEVELOPER",
+          default_reader : {
+            json: '{"type": "record", "name": "stringcontainer", "doc": "A simple inline record that contains a string", "fields": [{"name": "contained_string", "type": "string", "id": "int"}]}'
+          }
+        }
+      }, {
+        name : "double_column",
+        column_schema : {
+          type : "INLINE",
+          value : '"double"'
+        }
+      }, {
+        name : "column_compatibility",
+        column_schema : {
+          avro_validation_policy : "SCHEMA_1_0",
+          type : "INLINE",
+          value : '{"type": "record", "name": "stringcontainer", "doc": "A simple inline record that contains a string", "fields": [{"name": "contained_string", "type": "string", "id": "int"}]}'
+        }
+      }]
+    }, {
+      name : "searches",
+      description : "A map-type column family",
+      map_schema : {
+        type: "INLINE",
+        value: '"int"'
+      }
+    }, {
+      name : "searches_dev",
+      description : "A map-type column family in dev mode",
+      map_schema : {
+        avro_validation_policy : "DEVELOPER",
+        type: "AVRO",
+        value: '"int"',
+        writers: [{json: '"int"'}]
+      }
+    }, {
+      name : "animals",
+      description : "A map-type column family for testing numeric-initial strings",
+      map_schema : {
+        type: "INLINE",
+        value: '"string"'
+      }
+    }]
+  } ],
+  version : "layout-1.3.0"
+}
diff --git a/kiji-express/src/test/resources/layout/avro-types-complete.json b/kiji-express/src/test/resources/layout/avro-types-complete.json
new file mode 100644
index 0000000000000000000000000000000000000000..2fe9d47ba8089431811f6f5a8ba0e6881df57db0
--- /dev/null
+++ b/kiji-express/src/test/resources/layout/avro-types-complete.json
@@ -0,0 +1,191 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+{
+  name: "avro_types_complete",
+  keys_format: {
+    encoding: "FORMATTED",
+    salt: {
+      hash_size: 2
+    },
+    components: [ {
+      name: "dummy",
+      type: "STRING"
+    } ]
+  },
+  locality_groups:  [ {
+    name: "default",
+    in_memory: false,
+    max_versions: 2147483647,
+    ttl_seconds: 2147483647,
+    bloom_type: "ROW",
+    compression_type: "GZ",
+    families: [ {
+      name: "strict",
+      columns: [ {
+        name: "counter",
+        column_schema: {
+          storage: "FINAL",
+          type: "COUNTER"
+        }
+      }, {
+        name: "raw",
+        column_schema: {
+          type: "RAW_BYTES"
+        }
+      },{
+        name: "null",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '"null"'},
+          readers: [{json: '"null"'}],
+          writers: [{json: '"null"'}]
+        }
+      },{
+        name: "boolean",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '"boolean"'},
+          readers: [{json: '"boolean"'}],
+          writers: [{json: '"boolean"'}]
+        }
+      }, {
+        name: "int",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '"int"'},
+          readers: [{json: '["int", "long"]'}],
+          writers: [{json: '"int"'}]
+        }
+      }, {
+        name: "long",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '"long"'},
+          readers: [{json: '"long"'}],
+          writers: [{json: '["int", "long"]'}]
+        }
+      }, {
+        name: "float",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '"float"'},
+          readers: [{json: '["float", "double"]'}],
+          writers: [{json: '"float"'}]
+        }
+      }, {
+        name: "double",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '"double"'},
+          readers: [{json: '"double"'}],
+          writers: [{json: '["float", "double"]'}]
+        }
+        }, {
+        name: "bytes",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '"bytes"'},
+          readers: [{json: '"bytes"'}],
+          writers: [{json: '"bytes"'}]
+        }
+      }, {
+        name: "string",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '{ "type": "string", "avro.java.string": "String" }'},
+          readers: [{json: '{ "type": "string", "avro.java.string": "String" }'}],
+          writers: [{json: '{ "type": "string" }'}]
+        }
+      }, {
+        name: "specific",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          specific_reader_schema_class: "org.kiji.express.avro.SimpleRecord",
+          readers: [{json: '{"type":"record","name":"SimpleRecord","namespace":"org.kiji.express.avro","fields":[{"name":"l","type":"long"},{"name":"s","type":{"type":"string","avro.java.string":"String"}},{"name":"o","type":{"type":"string","avro.java.string":"String"},"default":"default-value"}]}'}],
+          writers: [{json: '{"type":"record","name":"SimpleRecord","namespace":"org.kiji.express.avro","fields":[{"name":"l","type":"long"},{"name":"s","type":{"type":"string","avro.java.string":"String"}},{"name":"o","type":{"type":"string","avro.java.string":"String"},"default":"default-value"}]}'}]
+        }
+      }, {
+        name: "generic",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '{"type": "record", "name": "Vector", "fields": [{"name": "length", "type": "int"}, {"name": "angle", "type": "float"}]}'},
+          readers: [{json: '{"type": "record", "name": "Vector", "fields": [{"name": "length", "type": "int"}, {"name": "angle", "type": "float"}]}'}],
+          writers: [{json: '{"type": "record", "name": "Vector", "fields": [{"name": "length", "type": "int"}, {"name": "angle", "type": "float"}]}'}]
+        }
+      }, {
+        name: "enum",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '{"type": "enum", "name": "Direction", "symbols": ["NORTH", "EAST", "SOUTH", "WEST"]}'},
+          readers: [{json: '{"type": "enum", "name": "Direction", "symbols": ["NORTH", "EAST", "SOUTH", "WEST"]}'}],
+          writers: [{json: '{"type": "enum", "name": "Direction", "symbols": ["NORTH", "EAST", "SOUTH", "WEST"]}'}]
+        }
+      }, {
+        name: "array",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '{"type": "array", "items": { "type": "string", "avro.java.string": "String" }}'},
+          readers: [{json: '{"type": "array", "items": { "type": "string", "avro.java.string": "String" }}'}],
+          writers: [{json: '{"type": "array", "items": { "type": "string", "avro.java.string": "String" }}'}]
+        }
+      }, {
+        name: "map",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '{"type": "map", "values": "int"}'},
+          readers: [{json: '{"type": "map", "values": "int"}'}],
+          writers: [{json: '{"type": "map", "values": "int"}'}]
+        }
+      }, {
+        name: "union",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '[{ "type": "string", "avro.java.string": "String" }, "int"]'},
+          readers: [{json: '[{ "type": "string", "avro.java.string": "String" }, "int"]'}],
+          writers: [{json: '["string", "int"]'}]
+        }
+      }, {
+        name: "fixed",
+        column_schema: {
+          type: "AVRO",
+          avro_validation_policy: "STRICT",
+          default_reader: {json: '{"type": "fixed", "size": 10, "name": "hash"}'},
+          readers: [{json: '{"type": "fixed", "size": 10, "name": "hash"}'}],
+          writers: [{json: '{"type": "fixed", "size": 10, "name": "hash"}'}]
+        }
+      } ]
+    } ]
+  } ],
+  version: "layout-1.3.0"
+}
diff --git a/kiji-express/src/test/resources/layout/avro-types.json b/kiji-express/src/test/resources/layout/avro-types.json
new file mode 100644
index 0000000000000000000000000000000000000000..e05befd9bedf25cab6479d064c95759afebe4a09
--- /dev/null
+++ b/kiji-express/src/test/resources/layout/avro-types.json
@@ -0,0 +1,101 @@
+/**
+ * (c) Copyright 2012 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+{
+  name : "table",
+  keys_format : {
+    encoding : "FORMATTED",
+    salt : {
+      hash_size : 2
+    },
+    components : [ {
+      name : "dummy",
+      type : "STRING"
+    }]
+  },
+  locality_groups : [ {
+    name : "default",
+    in_memory : false,
+    max_versions : 2147483647,
+    ttl_seconds : 2147483647,
+    compression_type : "GZ",
+    families : [ {
+      name : "family",
+      columns : [ {
+        name : "column1",
+        column_schema : {
+          type : "INLINE",
+          value : '{"type": "fixed", "size": 16, "name": "md5"}'
+        }
+      }, {
+        name : "column2",
+        column_schema : {
+          type : "INLINE",
+          value : '"bytes"'
+        }
+      }, {
+        name : "double_column",
+        column_schema : {
+          type : "INLINE",
+          value : '"double"'
+        }
+      }, {
+        name : "column3",
+        column_schema : {
+          type : "CLASS",
+          value : "org.kiji.schema.avro.HashSpec"
+        }
+      }, {
+        name : "column4",
+        column_schema : {
+          type : "INLINE",
+          value : '{"type": "record", "name": "stringcontainer", "doc": "A simple inline record that contains a string", "fields": [{"name": "contained_string", "type": "string", "id": "int"}]}'
+        }
+      }, {
+        name : "column5",
+        description: "A class that does not exist on the classpath, for testing purposes.",
+        column_schema : {
+          type : "CLASS",
+          value : "org.kiji.nonexistent.class"
+        }
+      }, {
+        name : "simple",
+        column_schema : {
+          avro_validation_policy : "SCHEMA_1_0",
+          type : "INLINE",
+          value : "{\"type\":\"record\",\"name\":\"SimpleRecord\",\"namespace\":\"org.kiji.express.avro\",\"fields\":[{\"name\":\"l\",\"type\":\"long\"},{\"name\":\"s\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"o\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"},\"default\":\"default-value\"}]}"
+        }
+      }]
+    }, {
+      name : "searches",
+      description : "A map-type column family",
+      map_schema : {
+        type: "INLINE",
+        value: '"int"'
+      }
+    }, {
+      name : "animals",
+      description : "A map-type column family for testing numeric-initial strings",
+      map_schema : {
+        type: "INLINE",
+        value: '"string"'
+      }
+    }]
+  } ],
+  version : "layout-1.1.0"
+}
diff --git a/kiji-express/src/test/resources/log4j.properties b/kiji-express/src/test/resources/log4j.properties
new file mode 100644
index 0000000000000000000000000000000000000000..c5518b6664509ead8d0c9aad1f14544bbbfd6f53
--- /dev/null
+++ b/kiji-express/src/test/resources/log4j.properties
@@ -0,0 +1,69 @@
+#
+#   (c) Copyright 2013 WibiData, Inc.
+#
+#   See the NOTICE file distributed with this work for additional
+#   information regarding copyright ownership.
+#
+#   Licensed under the Apache License, Version 2.0 (the "License");
+#   you may not use this file except in compliance with the License.
+#   You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#   Unless required by applicable law or agreed to in writing, software
+#   distributed under the License is distributed on an "AS IS" BASIS,
+#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#   See the License for the specific language governing permissions and
+#   limitations under the License.
+log4j.rootLogger=${kiji.logger}
+log4j.logger.cleanup=${kiji.cleanupLogger}
+
+# By default, log INFO to the console.
+kiji.logger=INFO,console
+kiji.cleanupLogger=DEBUG,cleanup
+
+# Uncomment the following to log to the console:
+#kiji.logger=DEBUG,console
+
+# Define a console appender.
+log4j.appender.console=org.apache.log4j.ConsoleAppender
+log4j.appender.console.target=System.err
+log4j.appender.console.layout=org.apache.log4j.PatternLayout
+log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c: %m%n
+
+# Define the cleanup appender.
+log4j.appender.cleanup=org.apache.log4j.RollingFileAppender
+log4j.appender.cleanup.Append=false
+log4j.appender.cleanup.File=target/cleanup.log
+log4j.appender.cleanup.layout=org.apache.log4j.PatternLayout
+log4j.appender.cleanup.layout.ConversionPattern=CLEANUP: %d{yy/MM/dd HH:mm:ss} %p %c: %m%n
+
+# Quiet down zookeeper; it's too noisy.
+log4j.logger.org.apache.zookeeper=WARN
+log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=WARN
+log4j.logger.org.apache.hadoop.hbase.zookeeper=WARN
+log4j.logger.org.apache.hadoop.hbase.client.HBaseAdmin=WARN
+
+# Hadoop 1.x / HBase 0.92 emits many SASL exceptions to stdout; silence these.
+log4j.logger.org.apache.zookeeper.client.ZooKeeperSaslClient=ERROR
+
+# Suppress Kiji installer messages for tests
+log4j.logger.org.kiji.schema.KijiInstaller=WARN
+
+# We do want kiji debug logging for all classes that have explicit finalizers.
+# These classes have special-purpose loggers just for their leak cleanup traces.
+# They are enabled below.
+log4j.logger.org.kiji.schema.impl.HBaseKiji.Cleanup=DEBUG
+log4j.logger.org.kiji.schema.impl.HBaseKijiTable.Cleanup=DEBUG
+log4j.logger.org.kiji.schema.impl.HBaseKijiRowScanner.Cleanup=DEBUG
+log4j.logger.org.kiji.schema.impl.HBaseSchemaTable.Cleanup=DEBUG
+log4j.logger.org.kiji.schema.impl.HBaseSystemTable.Cleanup=DEBUG
+log4j.logger.org.kiji.schema.impl.HBaseKijiTableReader.Cleanup=DEBUG
+log4j.logger.org.kiji.schema.impl.HBaseKijiTableWriter.Cleanup=DEBUG
+log4j.logger.org.kiji.schema.KijiMetaTable.Cleanup=DEBUG
+log4j.logger.org.kiji.schema.KijiTablePool.Cleanup=DEBUG
+
+# Loggers in the Kiji framework:
+log4j.logger.org.kiji.schema=WARN
+log4j.logger.org.kiji.mapreduce=WARN
+log4j.logger.org.kiji.express=DEBUG
diff --git a/kiji-express/src/test/resources/two-double-columns.json b/kiji-express/src/test/resources/two-double-columns.json
new file mode 100644
index 0000000000000000000000000000000000000000..349e95fc65d9d401e84ce0b696345108648ec22c
--- /dev/null
+++ b/kiji-express/src/test/resources/two-double-columns.json
@@ -0,0 +1,50 @@
+/**
+ * (c) Copyright 2012 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+{
+  name : "table",
+  keys_format : {
+    encoding : "HASH",
+    hash_type : "MD5",
+    hash_size : 16
+  },
+  locality_groups : [ {
+    name : "default",
+    in_memory : false,
+    max_versions : 2147483647,
+    ttl_seconds : 2147483647,
+    compression_type : "GZ",
+    families : [ {
+      name : "family",
+      columns : [ {
+        name : "column1",
+        column_schema : {
+          type : "INLINE",
+          value : '"double"'
+        }
+      }, {
+        name : "column2",
+        column_schema : {
+          type : "INLINE",
+          value : '"double"'
+        }
+      } ]
+    } ]
+  } ],
+  version : "layout-1.0"
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/KijiSuite.scala b/kiji-express/src/test/scala/org/kiji/express/KijiSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..c2a25c1539b7ba8a1682491923d548a3806d375e
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/KijiSuite.scala
@@ -0,0 +1,178 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express
+
+import java.io.InputStream
+import java.util.concurrent.atomic.AtomicInteger
+
+import com.twitter.scalding.TupleConversions
+import org.scalatest.FunSuite
+
+import org.kiji.express.flow.FlowCell
+import org.kiji.express.flow.util.Resources._
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiTable
+import org.kiji.schema.layout.KijiTableLayout
+import org.kiji.schema.layout.KijiTableLayouts
+import org.kiji.schema.shell.api.Client
+import org.kiji.schema.util.InstanceBuilder
+
+/** Contains convenience methods for writing tests that use Kiji. */
+trait KijiSuite
+    extends FunSuite
+    with TupleConversions {
+  // Counter for incrementing instance names by.
+  val counter: AtomicInteger = new AtomicInteger(0)
+
+  /**
+   * Builds a slice containing no values.  This can be used to test for behavior of missing
+   * values.
+   *
+   * @tparam T type of the values in the returned slice.
+   * @return an empty slice.
+   */
+  def missing[T](): Seq[FlowCell[T]] = Seq()
+
+  /**
+   * Builds a slice from a group type column name and list of version, value pairs.
+   *
+   * @tparam T type of the values contained within desired slice.
+   * @param columnName for a group type family, of the form "family:qualifier"
+   * @param values pairs of (version, value) to build the slice with.
+   * @return a slice containing the specified cells.
+   */
+  def slice[T](columnName: String, values: (Long, T)*): Seq[FlowCell[T]] = {
+    val parsedName = new KijiColumnName(columnName)
+    require(
+        parsedName.isFullyQualified,
+        "Fully qualified column names must be of the form \"family:qualifier\"."
+    )
+
+    values
+        .map { entry: (Long, T) =>
+          val (version, value) = entry
+          FlowCell(parsedName.getFamily, parsedName.getQualifier, version, value)
+        }
+  }
+
+  /**
+   * Builds a slice from a map type column name and a list of qualifier, version, value triples.
+   *
+   * @tparam T type of the values contained within desired slice.
+   * @param columnName for a map type family, of the form "family"
+   * @param values are triples of (qualifier, version, value) to build the slice with.
+   * @return a slice containing the specified cells.
+   */
+  def mapSlice[T](columnName: String, values: (String, Long, T)*): Seq[FlowCell[T]] = {
+    val parsedName = new KijiColumnName(columnName)
+    require(
+        !parsedName.isFullyQualified,
+        "Column family names must not contain any ':' characters."
+    )
+
+    values
+        .map { entry: (String, Long, T) =>
+          val (qualifier, version, value) = entry
+          FlowCell(parsedName.getFamily, qualifier, version, value)
+        }
+  }
+
+  /**
+   * Constructs and starts a test Kiji instance that uses fake-hbase.
+   *
+   * @param instanceName Name of the test Kiji instance.
+   * @return A handle to the Kiji instance that just got constructed. Note: This object must be
+   *     {{{release()}}}'d once it is no longer needed.
+   */
+  def makeTestKiji(instanceName: String = "default"): Kiji = {
+    new InstanceBuilder(instanceName).build()
+  }
+
+  /**
+   * Constructs and starts a test Kiji instance and creates a Kiji table.
+   *
+   * @param layout Layout of the test table.
+   * @param instanceName Name of the Kiji instance to create.
+   * @return A handle to the Kiji table that just got constructed. Note: This object must be
+   *     {{{release()}}}'d once it is no longer needed.
+   */
+  def makeTestKijiTable(
+      layout: KijiTableLayout,
+      instanceName: String = "default_%s".format(counter.incrementAndGet())
+  ): KijiTable = {
+    val tableName = layout.getName
+    val kiji: Kiji = new InstanceBuilder(instanceName)
+        .withTable(tableName, layout)
+        .build()
+
+    val table: KijiTable = kiji.openTable(tableName)
+    kiji.release()
+    return table
+  }
+
+  /**
+   * Loads a [[org.kiji.schema.layout.KijiTableLayout]] from the classpath. See
+   * [[org.kiji.schema.layout.KijiTableLayouts]] for some layouts that get put on the classpath
+   * by KijiSchema.
+   *
+   * @param resourcePath Path to the layout definition file.
+   * @return The layout contained within the provided resource.
+   */
+  def layout(resourcePath: String): KijiTableLayout = {
+    val tableLayoutDef = KijiTableLayouts.getLayout(resourcePath)
+    KijiTableLayout.newLayout(tableLayoutDef)
+  }
+
+  /**
+   * Executes a series of KijiSchema Shell DDL commands, separated by `;`.
+   *
+   * @param kiji to execute the commands against.
+   * @param commands to execute against the Kiji instance.
+   */
+  def executeDDLString(kiji: Kiji, commands: String) {
+    doAndClose(Client.newInstance(kiji.getURI)) { ddlClient =>
+      ddlClient.executeUpdate(commands)
+    }
+  }
+
+  /**
+   * Executes a series of KijiSchema Shell DDL commands, separated by `;`.
+   *
+   * @param kiji to execute the commands against.
+   * @param stream to read a series of commands to execute against the Kiji instance.
+   */
+  def executeDDLStream(kiji: Kiji, stream: InputStream) {
+    doAndClose(Client.newInstance(kiji.getURI)) { ddlClient =>
+      ddlClient.executeStream(stream)
+    }
+  }
+
+  /**
+   * Executes a series of KijiSchema Shell DDL commands, separated by `;`.
+   *
+   * @param kiji to execute the commands against.
+   * @param resourcePath to the classpath resource that a series of commands to execute
+   *     against the Kiji instance will be read from.
+   */
+  def executeDDLResource(kiji: Kiji, resourcePath: String) {
+    executeDDLStream(kiji, getClass.getClassLoader.getResourceAsStream(resourcePath))
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/ColumnSpecSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/ColumnSpecSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..18b81add03b87d392dbdbfb7ca2c9818afd86c9f
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/ColumnSpecSuite.scala
@@ -0,0 +1,141 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.apache.avro.Schema
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+
+@RunWith(classOf[JUnitRunner])
+class ColumnSpecSuite extends FunSuite {
+  val filter = RegexQualifierFilterSpec(".*")
+  val colFamily = "myfamily"
+  val colQualifier = "myqualifier"
+  val qualifierSelector = 'qualifierSym
+  val schema = Some(Schema.create(Schema.Type.LONG))
+
+  // TODO(CHOP-37): Test with different filters once the new method of specifying filters
+  // correctly implements the .equals() and hashCode() methods.
+  // Should be able to change the following line to:
+  // def filter = new RegexQualifierColumnFilter(".*")
+  val maxVersions = 2
+
+  val colWithOptions: QualifiedColumnInputSpec = QualifiedColumnInputSpec(
+      family = colFamily,
+      qualifier = colQualifier,
+      maxVersions = maxVersions,
+      filter = Some(filter)
+  )
+
+  test("Fields of a ColumnFamilyInputSpec are the same as those it is constructed with.") {
+    val col: ColumnFamilyInputSpec = new ColumnFamilyInputSpec(family = colFamily)
+    assert(colFamily === col.family)
+  }
+
+  test("ColumnInputSpec factory method creates ColumnFamilyInputSpec.") {
+    val col = ColumnInputSpec(colFamily)
+    assert(col.isInstanceOf[ColumnFamilyInputSpec])
+    assert(colFamily === col.asInstanceOf[ColumnFamilyInputSpec].family)
+  }
+
+  test("Fields of a ColumnFamilyOutputSpec are the same as those it is constructed with.") {
+    val col: ColumnFamilyOutputSpec = ColumnFamilyOutputSpec(colFamily, qualifierSelector)
+    assert(colFamily === col.family)
+    assert(qualifierSelector === col.qualifierSelector)
+    assert(None === col.schemaSpec.schema)
+  }
+
+  test("ColumnFamilyOutputSpec factory method creates ColumnFamilyOutputSpec.") {
+    val col = ColumnFamilyOutputSpec(colFamily, qualifierSelector, schema.get)
+
+    assert(colFamily === col.family)
+    assert(qualifierSelector === qualifierSelector)
+    assert(schema === col.schemaSpec.schema)
+  }
+
+  test("Fields of a QualifiedColumnInputSpec are the same as those it is constructed with.") {
+    val col: QualifiedColumnInputSpec = QualifiedColumnInputSpec(colFamily, colQualifier)
+    assert(colFamily === col.family)
+    assert(colQualifier === col.qualifier)
+  }
+
+  test("ColumnInputSpec factory method creates QualifiedColumnInputSpec.") {
+    val col = QualifiedColumnInputSpec(colFamily, colQualifier)
+    assert(col.isInstanceOf[QualifiedColumnInputSpec])
+    assert(colFamily === col.asInstanceOf[QualifiedColumnInputSpec].family)
+    assert(colQualifier === col.asInstanceOf[QualifiedColumnInputSpec].qualifier)
+  }
+
+  test("Fields of a QualifiedColumnOutputSpec are the same as those it is constructed with.") {
+    val col: QualifiedColumnOutputSpec =
+        QualifiedColumnOutputSpec(colFamily, colQualifier)
+
+    assert(colFamily === col.family)
+    assert(colQualifier === col.qualifier)
+    assert(None === col.schemaSpec.schema)
+  }
+
+  test("QualifiedColumnOutputSpec factory method creates QualifiedColumnOutputSpec.") {
+    val col = QualifiedColumnOutputSpec(colFamily, colQualifier, schema.get)
+    assert(colQualifier === col.qualifier)
+    assert(colFamily === col.family)
+    assert(schema === col.schemaSpec.schema)
+  }
+
+  test("Two ColumnFamilys with the same parameters are equal and hash to the same value.") {
+    val col1 = new ColumnFamilyInputSpec(colFamily)
+    val col2 = new ColumnFamilyInputSpec(colFamily)
+
+    assert(col1 === col2)
+    assert(col1.hashCode() === col2.hashCode())
+  }
+
+  test("Two qualified columns with the same parameters are equal and hash to the same value.") {
+    val col1 = new QualifiedColumnInputSpec(colFamily, colQualifier)
+    val col2 = new QualifiedColumnInputSpec(colFamily, colQualifier)
+
+    assert(col1 === col2)
+    assert(col1.hashCode() === col2.hashCode())
+  }
+
+  test("maxVersions is the same as constructed with.") {
+    assert(maxVersions == colWithOptions.maxVersions)
+  }
+
+  test("Default maxVersions is 1.") {
+    assert(1 == QualifiedColumnInputSpec(colFamily, colQualifier).maxVersions)
+  }
+
+  test("Filter is the same as constructed with.") {
+    assert(Some(filter) == colWithOptions.filter)
+  }
+
+  test("ColumnInputSpec with the same maxVersions & filter are equal and hash to the same value.") {
+
+    val col2: QualifiedColumnInputSpec = new QualifiedColumnInputSpec(
+        colFamily, colQualifier,
+        filter = Some(filter),
+        maxVersions = maxVersions
+    )
+    assert(col2 === colWithOptions)
+    assert(col2.hashCode() === colWithOptions.hashCode())
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/EntityIdSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/EntityIdSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..d1574a67619517df9576bff5a15f2376a61ac413
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/EntityIdSuite.scala
@@ -0,0 +1,451 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import scala.collection.JavaConverters.seqAsJavaListConverter
+import scala.collection.mutable.Buffer
+
+import com.twitter.scalding.Args
+import com.twitter.scalding.JobTest
+import com.twitter.scalding.TextLine
+import com.twitter.scalding.Tsv
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.hbase.HBaseConfiguration
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.flow.EntityId.HashedEntityId
+import org.kiji.express.flow.util.Resources.doAndRelease
+import org.kiji.schema.EntityIdFactory
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiURI
+import org.kiji.schema.layout.KijiTableLayout
+import org.kiji.schema.layout.KijiTableLayouts
+
+/**
+ * Unit tests for [[org.kiji.express.flow.EntityId]].
+ */
+@RunWith(classOf[JUnitRunner])
+class EntityIdSuite extends KijiSuite {
+
+  import org.kiji.express.flow.EntityIdSuite._
+
+  /** Table layout with formatted entity IDs to use for tests. */
+  val formattedEntityIdLayout: KijiTableLayout = layout(KijiTableLayouts.FORMATTED_RKF)
+  // Create a table to use for testing
+  val formattedTableUri: KijiURI =
+      doAndRelease(makeTestKijiTable(formattedEntityIdLayout)) { table: KijiTable => table.getURI }
+
+  /** Table layout with hashed entity IDs to use for tests. */
+  val hashedEntityIdLayout: KijiTableLayout = layout(KijiTableLayouts.HASHED_FORMATTED_RKF)
+  // Create a table to use for testing
+  val hashedTableUri: KijiURI =
+      doAndRelease(makeTestKijiTable(hashedEntityIdLayout)) { table: KijiTable => table.getURI }
+
+  val configuration: Configuration = HBaseConfiguration.create()
+
+  val formattedEidFactory = EntityIdFactory.getFactory(formattedEntityIdLayout)
+  val hashedEidFactory = EntityIdFactory.getFactory(hashedEntityIdLayout)
+
+  // ------- "Unit tests" for comparisons and creation. -------
+  test("Create an Express EntityId from a Kiji EntityId and vice versa in a formatted table.") {
+    val expressEid = EntityId("test", "1", "2", 1, 7L)
+    val kijiEid = expressEid.toJavaEntityId(formattedEidFactory)
+    val expected: java.util.List[AnyRef] =
+        Seq[AnyRef]("test", "1", "2", 1: java.lang.Integer, 7L: java.lang.Long).asJava
+
+    assert(expected === kijiEid.getComponents)
+
+    val recreate = EntityId.fromJavaEntityId(kijiEid)
+
+    assert(expressEid === recreate)
+    assert(recreate(0) === "test")
+  }
+
+  test("Create an Express EntityId from a Kiji EntityId and vice versa in a hashed table.") {
+    val origKijiEid = hashedEidFactory.getEntityId("test")
+
+    val expressEid = HashedEntityId(origKijiEid.getHBaseRowKey())
+    val expressToKijiEid = expressEid.toJavaEntityId(hashedEidFactory)
+
+    val recreate = EntityId.fromJavaEntityId(expressToKijiEid)
+    assert(recreate.components.equals(List(origKijiEid.getHBaseRowKey())))
+  }
+
+  test("Creating an EntityId from a Hashed table fails if there is more than one component.") {
+    val eid: EntityId = EntityId("one", 2)
+    val exception = intercept[org.kiji.schema.EntityIdException] {
+      eid.toJavaEntityId(hashedEidFactory)
+    }
+    assert(exception.getMessage.contains("Too many components"))
+  }
+
+  test("Test equality between two EntityIds.") {
+    val eidComponents1: EntityId = EntityId("test", 1)
+    val eidComponents2: EntityId = EntityId("test", 1)
+
+    assert(eidComponents1 === eidComponents2)
+    assert(eidComponents2 === eidComponents1)
+  }
+
+  test("Test comparison between two EntityIds.") {
+    val eidComponents1: EntityId = EntityId("test", 2)
+    val eidComponents2: EntityId = EntityId("test", 3)
+
+    assert(eidComponents2 > eidComponents1)
+    assert(eidComponents1 < eidComponents2)
+  }
+
+  test("Test comparison between two EntityIds with different lengths.") {
+    val eidComponents1: EntityId = EntityId("test", 2)
+    val eidComponents2: EntityId = EntityId("test", 2, 1)
+
+    assert(eidComponents2 > eidComponents1)
+    assert(eidComponents1 < eidComponents2)
+  }
+
+  test("Test comparison between two EntityIds with different formats fails.") {
+    val eidComponents1: EntityId = EntityId("test", 2)
+    val eidComponents2: EntityId = EntityId("test", 2L)
+
+    val exception = intercept[EntityIdFormatMismatchException] {
+      eidComponents1 < eidComponents2
+    }
+
+    // Exception message should be something like:
+    // Mismatched Formats: Components: [java.lang.String,java.lang.Integer] and  Components:
+    // [java.lang.String,java.lang.Long] do not match.
+
+    assert(exception.getMessage.contains("String"))
+    assert(exception.getMessage.contains("Integer"))
+    assert(exception.getMessage.contains("Long"))
+  }
+
+  // ------- "integration tests" for joins. -------
+  /** Simple table layout to use for tests. The row keys are hashed. */
+  val simpleLayout: KijiTableLayout = layout(KijiTableLayouts.SIMPLE_TWO_COLUMNS)
+
+  /** Table layout using Avro schemas to use for tests. The row keys are formatted. */
+  val avroLayout: KijiTableLayout = layout("layout/avro-types.json")
+
+  test("Runs a job that joins two pipes, on user-created EntityIds.") {
+    // Create main input.
+    val mainInput: List[(String, String)] = List(
+      ("0", "0row"),
+      ("1", "1row"),
+      ("2", "2row"))
+
+    // Create input from side data.
+    val sideInput: List[(String, String)] = List(("0", "0row"), ("1", "2row"))
+
+    // Validate output.
+    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
+      assert(outputBuffer.size === 2)
+    }
+
+    // Create the JobTest for this test.
+    val jobTest = JobTest(new JoinUserEntityIdsJob(_))
+      .arg("input", "mainInputFile")
+      .arg("side-input", "sideInputFile")
+      .arg("output", "outputFile")
+      .source(TextLine("mainInputFile"), mainInput)
+      .source(TextLine("sideInputFile"), sideInput)
+      .sink(Tsv("outputFile"))(validateTest)
+
+    // Run the test in local mode.
+    jobTest.run.finish
+
+    // Run the test in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+
+  test("Runs a job that joins two pipes, on user-created and from a table (formatted) EntityIds.") {
+    // URI of the Kiji table to use.
+    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Create input from Kiji table.
+    val joinKijiInput: List[(EntityId, Seq[FlowCell[String]])] = List(
+      (EntityId("0row"), mapSlice("animals", ("0column", 0L, "0 dogs"))),
+      (EntityId("1row"), mapSlice("animals", ("0column", 0L, "1 cat"))),
+      (EntityId("2row"), mapSlice("animals", ("0column", 0L, "2 fish"))))
+
+    // Create input from side data.
+    val sideInput: List[(String, String)] = List(("0", "0row"), ("1", "2row"))
+
+    // Validate output.
+    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
+      assert(outputBuffer.size === 2)
+    }
+
+    // Create the JobTest for this test.
+    val jobTest = JobTest(new JoinUserAndFormattedFromTableJob(_))
+      .arg("input", uri)
+      .arg("side-input", "sideInputFile")
+      .arg("output", "outputFile")
+      .source(KijiInput(uri, ("animals" -> 'animals)), joinKijiInput)
+      .source(TextLine("sideInputFile"), sideInput)
+      .sink(Tsv("outputFile"))(validateTest)
+
+    // Run the test in local mode.
+    jobTest.run.finish
+
+
+    // Run the test in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+
+  test("Runs a job that joins two pipes, on EntityIds from a table (hashed), in local mode.") {
+    // URI of the hashed Kiji table to use.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Create input from hashed Kiji table.
+    val joinInput1: List[(EntityId, Seq[FlowCell[String]])] = List(
+      (EntityId("0row"), slice("family:column1", (0L, "0 dogs"))),
+      (EntityId("1row"), slice("family:column1", (0L, "1 cat"))),
+      (EntityId("2row"), slice("family:column1", (0L, "2 fish"))))
+
+
+    // Create input from hashed Kiji table.
+    val joinInput2: List[(EntityId, Seq[FlowCell[String]])] = List(
+      (EntityId("0row"), slice("family:column2", (0L, "0 boop"))),
+      (EntityId("2row"), slice("family:column2", (1L, "1 cat")))
+      )
+
+    // Validate output.
+    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
+      assert(outputBuffer.size === 2)
+    }
+
+    // Create the JobTest for this test.
+    val jobTest = JobTest(new JoinHashedEntityIdsJob(_))
+      .arg("input1", uri)
+      .arg("input2", uri)
+      .arg("output", "outputFile")
+      .source(KijiInput(uri, ("family:column1" -> 'animals)), joinInput1)
+      .source(KijiInput(uri, ("family:column2" -> 'slice)), joinInput2)
+      .sink(Tsv("outputFile"))(validateTest)
+
+    // Run the test in local mode.
+    jobTest.run.finish
+  }
+
+  test("Runs a job that joins two pipes, on EntityIds from a table (hashed), in hadoop mode.") {
+    // URI of the hashed Kiji table to use.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Create input from hashed Kiji table.
+    val joinInput1: List[(EntityId, Seq[FlowCell[String]])] = List(
+      (EntityId("0row"), slice("family:column1", (0L, "0 dogs"))),
+      (EntityId("1row"), slice("family:column1", (0L, "1 cat"))),
+      (EntityId("2row"), slice("family:column1", (0L, "2 fish"))))
+
+    // Create input from hashed Kiji table.
+    val joinInput2: List[(EntityId, Seq[FlowCell[String]])] = List(
+      (EntityId("0row"), slice("family:column2", (0L, "0 boop"))),
+      (EntityId("2row"), slice("family:column2", (0L, "2 beep"))))
+
+    // Validate output.
+    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
+      assert(outputBuffer.size === 2)
+    }
+
+    // Create the JobTest for this test.
+    val jobTest = JobTest(new JoinHashedEntityIdsJob(_))
+      .arg("input1", uri)
+      .arg("input2", uri)
+      .arg("output", "outputFile")
+      .source(KijiInput(uri, ("family:column1" -> 'animals)), joinInput1)
+      .source(KijiInput(uri, ("family:column2" -> 'slice)), joinInput2)
+      .sink(Tsv("outputFile"))(validateTest)
+
+    // Run the test in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+
+  test("A job that joins two pipes, on EntityIds from a table (formatted) in local mode.") {
+    // URI of a formatted Kiji table to use.
+    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Create input from formatted Kiji table.
+    val joinInput1: List[(EntityId, Seq[FlowCell[Int]])] = List(
+      (EntityId("0row"), mapSlice("searches", ("0column", 0L, 0))),
+      (EntityId("2row"), mapSlice("searches", ("0column", 0L, 2))))
+
+    // Create input from formatted Kiji table.
+    val joinInput2: List[(EntityId, Seq[FlowCell[String]])] = List(
+      (EntityId("0row"), mapSlice("animals", ("0column", 0L, "0 dogs"))),
+      (EntityId("1row"), mapSlice("animals", ("0column", 0L, "1 cat"))),
+      (EntityId("2row"), mapSlice("animals", ("0column", 0L, "2 fish"))))
+
+    // Validate output.
+    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
+      assert(outputBuffer.size === 2)
+    }
+
+    // Create the JobTest for this test.
+    val jobTest = JobTest(new JoinFormattedEntityIdsJob(_))
+      .arg("input1", uri)
+      .arg("input2", uri)
+      .arg("output", "outputFile")
+      .source(KijiInput(uri, ("searches" -> 'searches)), joinInput1)
+      .source(KijiInput(uri, ("animals" -> 'animals)), joinInput2)
+      .sink(Tsv("outputFile"))(validateTest)
+
+    // Run the test in local mode.
+    jobTest.run.finish
+  }
+
+  test("A job that joins two pipes, on EntityIds from a table (formatted) in hadoop mode.") {
+    // URI of a formatted Kiji table to use.
+    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Create input from formatted Kiji table.
+    val joinInput1: List[(EntityId, Seq[FlowCell[Int]])] = List(
+      (EntityId("0row"), mapSlice("searches", ("0column", 0L, 0))),
+      (EntityId("2row"), mapSlice("searches", ("0column", 0L, 2))))
+
+    // Create input from formatted Kiji table.
+    val joinInput2: List[(EntityId, Seq[FlowCell[String]])] = List(
+      (EntityId("0row"), mapSlice("animals", ("0column", 0L, "0 dogs"))),
+      (EntityId("1row"), mapSlice("animals", ("0column", 0L, "1 cat"))),
+      (EntityId("2row"), mapSlice("animals", ("0column", 0L, "2 fish"))))
+
+    // Validate output.
+    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
+      assert(outputBuffer.size === 2)
+    }
+
+    // Create the JobTest for this test.
+    val jobTest = JobTest(new JoinFormattedEntityIdsJob(_))
+      .arg("input1", uri)
+      .arg("input2", uri)
+      .arg("output", "outputFile")
+      .source(KijiInput(uri, ("searches" -> 'searches)), joinInput1)
+      .source(KijiInput(uri, ("animals" -> 'animals)), joinInput2)
+      .sink(Tsv("outputFile"))(validateTest)
+
+    // Run the test in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+}
+
+/** Companion object for EntityIdSuite. Contains test jobs. */
+object EntityIdSuite {
+  /**
+   * A job that tests joining two pipes, on user-constructed EntityIds.
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
+   *     Kiji table, and "output", which specifies the path to a text file.
+   */
+  class JoinUserEntityIdsJob(args: Args) extends KijiJob(args) {
+    val sidePipe = TextLine(args("side-input"))
+      .read
+      .map('line -> 'entityId) { line: String => EntityId(line) }
+      .project('entityId)
+
+    TextLine(args("input"))
+      .map('line -> 'entityId) { line: String => EntityId(line) }
+      .joinWithSmaller('entityId -> 'entityId, sidePipe)
+      .write(Tsv(args("output")))
+  }
+
+  /**
+   * A job that tests joining two pipes, one with a user-constructed EntityId and one with
+   * a formatted EntityId from a Kiji table.
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
+   *     Kiji table, and "output", which specifies the path to a text file.
+   */
+  class JoinUserAndFormattedFromTableJob(args: Args) extends KijiJob(args) {
+    val sidePipe = TextLine(args("side-input"))
+      .read
+      .map('line -> 'entityId) { line: String => EntityId(line) }
+
+    KijiInput(args("input"), ("animals" -> 'animals))
+
+      .map('animals -> 'terms) { animals: Seq[FlowCell[CharSequence]] => animals.toString }
+      .joinWithSmaller('entityId -> 'entityId, sidePipe)
+      .write(Tsv(args("output")))
+  }
+
+  /**
+   * A job that tests joining two pipes, one with a user-constructed EntityId and one with
+   * a hashed EntityId from a Kiji table.
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
+   *     Kiji table, and "output", which specifies the path to a text file.
+   */
+  class JoinUserAndHashedFromTableJob(args: Args) extends KijiJob(args) {
+    val sidePipe = TextLine(args("side-input"))
+      .read
+      .map('line -> 'entityId) { line: String => EntityId(line) }
+      .project('entityId)
+
+    KijiInput(args("input"), ("family:column1" -> 'slice))
+      .map('slice -> 'terms) { slice: Seq[FlowCell[CharSequence]] => slice.head.datum.toString }
+      .joinWithSmaller('entityId -> 'entityId, sidePipe)
+      .write(Tsv(args("output")))
+  }
+
+  /**
+   * A job that tests joining two pipes, on EntityIds from a table with row key format HASHED.
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
+   *     Kiji table, and "output", which specifies the path to a text file.
+   */
+  class JoinHashedEntityIdsJob(args: Args) extends KijiJob(args) {
+    val pipe1 = KijiInput(args("input1"), ("family:column1" -> 'animals))
+
+    KijiInput(args("input2"), ("family:column2" -> 'slice))
+      .map('animals -> 'animal) { slice: Seq[FlowCell[CharSequence]] => slice.head.datum.toString }
+
+    KijiInput(args("input2"), ("family:column2" -> 'slice))
+      .map('slice -> 'terms) { slice:Seq[FlowCell[CharSequence]] => slice.head.datum.toString }
+      .joinWithSmaller('entityId -> 'entityId, pipe1)
+      .write(Tsv(args("output")))
+  }
+
+  /**
+   * A job that tests joining two pipes, on EntityIds from a table with row key format formatted.
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
+   *     Kiji table, and "output", which specifies the path to a text file.
+   */
+  class JoinFormattedEntityIdsJob(args: Args) extends KijiJob(args) {
+    val pipe1 = KijiInput(args("input1"), ("searches" -> 'searches))
+      .map('searches -> 'term) { slice:Seq[FlowCell[Int]] => slice.head.datum }
+
+    KijiInput(args("input2"), ("animals" -> 'animals))
+      .map('animals -> 'animal) { slice: Seq[FlowCell[CharSequence]] => slice.head.datum.toString }
+      .joinWithSmaller('entityId -> 'entityId, pipe1)
+      .write(Tsv(args("output")))
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/FlowModuleSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/FlowModuleSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..9dd9dcd70a28a7c59d9b5dfb80610ddba08253fc
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/FlowModuleSuite.scala
@@ -0,0 +1,179 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.flow.framework.KijiScheme
+import org.kiji.schema.KijiInvalidNameException
+
+@RunWith(classOf[JUnitRunner])
+class FlowModuleSuite extends FunSuite {
+  val tableURI = "kiji://.env/default/table"
+
+  test("Flow module forbids creating an input map-type column with a qualifier in the column "
+      + "name.") {
+    intercept[KijiInvalidNameException] {
+      new ColumnFamilyInputSpec("info:word")
+    }
+  }
+
+  test("Flow module forbids creating an output map-type column with a qualifier in the column "
+      + "name.") {
+    intercept[KijiInvalidNameException] {
+      new ColumnFamilyOutputSpec("info:word", 'foo)
+    }
+  }
+
+  test("Flow module permits creating an output map-type column specifying the qualifier field") {
+    new ColumnFamilyOutputSpec("searches", 'terms)
+  }
+
+  test("Flow module permits specifying a qualifier regex on ColumnFamilyInputSpec.") {
+    val colReq = new ColumnFamilyInputSpec(
+        "search",
+        filter = Some(RegexQualifierFilterSpec(""".*\.com"""))
+    )
+
+    // TODO: Test it filters keyvalues correctly.
+    assert(colReq.filter.get.isInstanceOf[RegexQualifierFilterSpec])
+  }
+
+  test("Flow module permits specifying a qualifier regex (with filter) on ColumnFamilyInputSpec.") {
+    val colReq = new ColumnFamilyInputSpec("search",
+      filter=Some(RegexQualifierFilterSpec(""".*\.com""")))
+
+    // TODO: Test it filters keyvalues correctly.
+    assert(colReq.filter.get.isInstanceOf[RegexQualifierFilterSpec])
+  }
+
+  test("Flow module permits specifying versions on map-type columns without qualifier regex.") {
+    val colReq = ColumnFamilyInputSpec("search", maxVersions=2)
+    assert(2 === colReq.maxVersions)
+  }
+
+  test("Flow module permits specifying versions on a group-type column.") {
+    val colReq = QualifiedColumnInputSpec("info", "word", maxVersions=2)
+    assert(2 === colReq.maxVersions)
+  }
+
+  test("Flow module uses default versions of 1 for all ColumnInputSpecs.") {
+    val groupReq = QualifiedColumnInputSpec("info", "word")
+    val mapReq = ColumnFamilyInputSpec("searches")
+
+    assert(1 === groupReq.maxVersions)
+    assert(1 === mapReq.maxVersions)
+  }
+
+  test("Flow module permits creating inputs and outputs with no mappings.") {
+    val input: KijiSource = KijiInput(tableURI, columns = Map[ColumnInputSpec, Symbol]())
+    val output: KijiSource = KijiOutput(tableURI, columns = Map[Symbol, ColumnOutputSpec]())
+
+    assert(input.inputColumns.isEmpty)
+    assert(input.outputColumns.isEmpty)
+    assert(output.inputColumns.isEmpty)
+    assert(output.outputColumns.isEmpty)
+  }
+
+  test("Flow module permits creating KijiSources as inputs with default options.") {
+    val input: KijiSource = KijiInput(tableURI, "info:word" -> 'word)
+    val expectedScheme = new KijiScheme(
+        timeRange = All,
+        timestampField = None,
+        inputColumns = Map("word" -> QualifiedColumnInputSpec("info", "word")))
+
+    assert(expectedScheme === input.hdfsScheme)
+  }
+
+  test("Flow module permits specifying timerange for KijiInput.") {
+    val input = KijiInput(tableURI, timeRange=Between(0L,40L), columns="info:word" -> 'word)
+    val expectedScheme = new KijiScheme(
+        Between(0L, 40L),
+        None,
+        Map("word" -> QualifiedColumnInputSpec("info", "word")))
+
+    assert(expectedScheme === input.hdfsScheme)
+  }
+
+  test("Flow module permits creating KijiSources with multiple columns.") {
+    val input: KijiSource = KijiInput(tableURI, "info:word" -> 'word, "info:title" -> 'title)
+    val expectedScheme: KijiScheme = {
+      new KijiScheme(
+          All,
+          None,
+          Map(
+              "word" -> QualifiedColumnInputSpec("info", "word"),
+              "title" -> QualifiedColumnInputSpec("info", "title")))
+    }
+
+    assert(expectedScheme === input.hdfsScheme)
+  }
+
+  test("Flow module permits specifying options for a column.") {
+    KijiInput(
+        tableURI,
+        Map(QualifiedColumnInputSpec("info", "word") -> 'word)
+    )
+
+    KijiInput(
+        tableURI,
+        Map(QualifiedColumnInputSpec("info", "word", maxVersions = 1) -> 'word)
+    )
+
+    KijiInput(
+        tableURI,
+        Map(
+            ColumnFamilyInputSpec(
+                "searches",
+                maxVersions = 1,
+                filter = Some(new RegexQualifierFilterSpec(".*"))
+            ) -> 'word
+        )
+    )
+  }
+
+  test("Flow module permits specifying different options for different columns.") {
+    KijiInput(
+        tableURI,
+        Map(
+            QualifiedColumnInputSpec("info", "word", maxVersions = 1) -> 'word,
+            QualifiedColumnInputSpec("info", "title", maxVersions = 2) -> 'title))
+  }
+
+  test("Flow module permits creating KijiSource with the default timestamp field") {
+    val output: KijiSource = KijiOutput(tableURI, 'words -> "info:words")
+    val expectedScheme: KijiScheme = new KijiScheme(
+        timeRange = All,
+        timestampField = None,
+        outputColumns = Map("words" -> QualifiedColumnOutputSpec("info", "words")))
+    assert(expectedScheme === output.hdfsScheme)
+  }
+
+  test("Flow module permits creating KijiSource with a timestamp field") {
+    val output: KijiSource = KijiOutput(tableURI, 'time, 'words -> "info:words")
+    val expectedScheme: KijiScheme = new KijiScheme(
+        timeRange = All,
+        timestampField = Some('time),
+        outputColumns = Map("words" -> QualifiedColumnOutputSpec("info", "words")))
+    assert(expectedScheme === output.hdfsScheme)
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/KijiJobSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/KijiJobSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..5286694c226197299634f8c60aca14cc66aa9e8d
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/KijiJobSuite.scala
@@ -0,0 +1,274 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import scala.collection.mutable
+
+import cascading.tuple.Fields
+import com.twitter.scalding.Args
+import com.twitter.scalding.JobTest
+import com.twitter.scalding.TextLine
+import com.twitter.scalding.Tsv
+import org.apache.avro.generic.GenericRecord
+import org.apache.avro.generic.GenericRecordBuilder
+import org.apache.avro.specific.SpecificRecord
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.avro.SimpleRecord
+import org.kiji.express.flow.util.Resources.doAndRelease
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiURI
+import org.kiji.schema.layout.KijiTableLayout
+
+@RunWith(classOf[JUnitRunner])
+class KijiJobSuite extends KijiSuite {
+  val avroLayout: KijiTableLayout = layout("layout/avro-types.json")
+  val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
+    table.getURI().toString()
+  }
+
+  val rawInputs: List[(Long, String)] = List(
+    (1, "input 1"),
+    (2, "input 2"),
+    (3, "input 3"),
+    (4, "input 4"),
+    (5, "input 5"))
+
+  val eids: List[EntityId] = List("row-1", "row-2", "row-3", "row-4", "row-5").map(EntityId(_))
+
+  val genericInputs: List[GenericRecord] = {
+    val builder = new GenericRecordBuilder(SimpleRecord.getClassSchema)
+    rawInputs.map { case (l: Long, s: String) => builder.set("l", l).set("s", s).build }
+  }
+
+  val specificInputs: List[SimpleRecord] = {
+    val builder = SimpleRecord.newBuilder()
+    rawInputs.map { case (l: Long, s: String) => builder.setL(l).setS(s).build }
+  }
+
+  def validateUnpacking(output: mutable.Buffer[(Long, String, String)]): Unit = {
+    val inputMap = rawInputs.toMap
+    output.foreach { case (l: Long, s: String, o: String) =>
+      assert(inputMap(l) === s)
+      assert("default-value" === o)
+    }
+  }
+
+  test("A KijiJob can pack a generic Avro record.") {
+    def validatePacking(outputs: mutable.Buffer[(EntityId, Seq[FlowCell[GenericRecord]])]) {
+      val inputMap = rawInputs.toMap
+      outputs.foreach { case (_: EntityId, slice: Seq[FlowCell[GenericRecord]]) =>
+        val record = slice.head.datum
+        assert(inputMap(record.get("l").asInstanceOf[Long]) === record.get("s"))
+        assert("default-value" === record.get("o"))
+      }
+    }
+
+    val jobTest = JobTest(new PackGenericRecordJob(_))
+        .arg("input", "inputFile")
+        .arg("uri", uri)
+        .source(Tsv("inputFile", fields = new Fields("l", "s")), rawInputs)
+        .sink(KijiOutput(uri, 'record -> "family:simple"))(validatePacking)
+
+    // Run in local mode.
+    jobTest.run.finish
+    // Run in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+
+  test("A KijiJob can pack a specific Avro record.") {
+    def validatePacking(outputs: mutable.Buffer[(EntityId, Seq[FlowCell[SimpleRecord]])]) {
+      val inputMap = rawInputs.toMap
+      outputs.foreach { case (_: EntityId, slice: Seq[FlowCell[SimpleRecord]]) =>
+        val record = slice.head.datum
+        assert(inputMap(record.getL) === record.getS)
+        assert("default-value" === record.getO)
+      }
+    }
+
+    val jobTest = JobTest(new PackSpecificRecordJob(_))
+        .arg("input", "inputFile")
+        .arg("uri", uri)
+        .source(Tsv("inputFile", fields = new Fields("l", "s")), rawInputs)
+        .sink(KijiOutput(uri,
+            Map('record ->
+                QualifiedColumnOutputSpec("family", "simple", classOf[SimpleRecord]))))(
+                    validatePacking)
+
+    // Run in local mode.
+    jobTest.run.finish
+    // Run in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+
+  test("A KijiJob can unpack a generic record.") {
+    val slices: List[Seq[FlowCell[GenericRecord]]] = genericInputs.map { record: GenericRecord =>
+      List(FlowCell("family", "simple", datum = record))
+    }
+    val input: List[(EntityId, Seq[FlowCell[GenericRecord]])] = eids.zip(slices)
+
+    val jobTest = JobTest(new UnpackGenericRecordJob(_))
+        .arg("input", uri)
+        .arg("output", "outputFile")
+        .source(KijiInput(uri,
+            Map(QualifiedColumnInputSpec("family", "simple", SimpleRecord.getClassSchema)
+                -> 'slice)), input)
+        .sink(Tsv("outputFile"))(validateUnpacking)
+
+    // Run in local mode.
+    jobTest.run.finish
+    // Run in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+
+  test("A KijiJob can unpack a specific record.") {
+    val slices: List[Seq[FlowCell[SpecificRecord]]] = specificInputs
+        .map { record: SpecificRecord =>
+          List(FlowCell("family", "simple", datum = record))
+        }
+    val input: List[(EntityId, Seq[FlowCell[SpecificRecord]])] = eids.zip(slices)
+
+    val jobTest = JobTest(new UnpackSpecificRecordJob(_))
+        .arg("input", uri)
+        .arg("output", "outputFile")
+        .source(KijiInput(uri,
+      Map(QualifiedColumnInputSpec("family", "simple", classOf[SimpleRecord])
+          -> 'slice)), input)
+        .sink(Tsv("outputFile"))(validateUnpacking)
+
+    // Run in local mode.
+    jobTest.run.finish
+    // Run in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+
+  test("A KijiJob is not run if the Kiji instance in the output doesn't exist.") {
+    class BasicJob(args: Args) extends KijiJob(args) {
+      TextLine(args("input"))
+        .map ('line -> 'entityId) { line: String => EntityId(line) }
+        .write(KijiOutput(args("output"), 'line -> "family:column1"))
+    }
+
+    val nonexistentInstanceURI: String = KijiURI.newBuilder(uri)
+        .withInstanceName("nonexistent_instance")
+        .build()
+        .toString
+
+    val basicInput: List[(String, String)] = List[(String, String)]()
+
+    def validateBasicJob(outputBuffer: mutable.Buffer[String]) { /** Nothing to validate. */ }
+
+    val jobTest = JobTest(new BasicJob(_))
+        .arg("input", "inputFile")
+        .arg("output", nonexistentInstanceURI)
+        .source(TextLine("inputFile"), basicInput)
+        .sink(KijiOutput(nonexistentInstanceURI, 'line -> "family:column1"))(validateBasicJob)
+
+    val hadoopException = intercept[InvalidKijiTapException] { jobTest.runHadoop.finish }
+    val localException = intercept[InvalidKijiTapException] { jobTest.run.finish }
+
+    assert(localException.getMessage === hadoopException.getMessage)
+    assert(localException.getMessage.contains("nonexistent_instance"))
+  }
+
+  test("A KijiJob is not run if the Kiji table in the output doesn't exist.") {
+    class BasicJob(args: Args) extends KijiJob(args) {
+      TextLine(args("input"))
+        .write(KijiOutput(args("output"), 'line -> "family:column1"))
+    }
+
+    val nonexistentTableURI: String = KijiURI.newBuilder(uri)
+        .withTableName("nonexistent_table")
+        .build()
+        .toString
+
+    val basicInput: List[(String, String)] = List[(String, String)]()
+
+    def validateBasicJob(outputBuffer: mutable.Buffer[String]) { /** Nothing to validate. */ }
+
+    val jobTest = JobTest(new BasicJob(_))
+        .arg("input", "inputFile")
+        .arg("output", nonexistentTableURI)
+        .source(TextLine("inputFile"), basicInput)
+        .sink(KijiOutput(nonexistentTableURI, 'line -> "family:column1"))(validateBasicJob)
+
+    val localException = intercept[InvalidKijiTapException] { jobTest.run.finish }
+    val hadoopException = intercept[InvalidKijiTapException] { jobTest.runHadoop.finish }
+
+    assert(localException.getMessage === hadoopException.getMessage)
+    assert(localException.getMessage.contains("nonexistent_table"))
+  }
+
+  test("A KijiJob is not run if any of the columns don't exist.") {
+    class BasicJob(args: Args) extends KijiJob(args) {
+      TextLine(args("input"))
+        .write(KijiOutput(args("output"), 'line -> "family:nonexistent_column"))
+    }
+
+    val basicInput: List[(String, String)] = List[(String, String)]()
+
+    def validateBasicJob(outputBuffer: mutable.Buffer[String]) { /** Nothing to validate. */ }
+
+    val jobTest = JobTest(new BasicJob(_))
+        .arg("input", "inputFile")
+        .arg("output", uri)
+        .source(TextLine("inputFile"), basicInput)
+        .sink(KijiOutput(uri, 'line -> "family:nonexistent_column"))(validateBasicJob)
+
+    val localException = intercept[InvalidKijiTapException] { jobTest.run.finish }
+    val hadoopException = intercept[InvalidKijiTapException] { jobTest.runHadoop.finish }
+
+    assert(localException.getMessage === hadoopException.getMessage)
+    assert(localException.getMessage.contains("nonexistent_column"))
+  }
+}
+
+class PackGenericRecordJob(args: Args) extends KijiJob(args) {
+  Tsv(args("input"), fields = ('l, 's)).read
+      .packGenericRecordTo(('l, 's) -> 'record)(SimpleRecord.getClassSchema)
+      .insert('entityId, EntityId("foo"))
+      .write(KijiOutput(args("uri"), 'record -> "family:simple"))
+}
+
+class PackSpecificRecordJob(args: Args) extends KijiJob(args) {
+  Tsv(args("input"), fields = ('l, 's)).read
+      .packTo[SimpleRecord](('l, 's) -> 'record)
+      .insert('entityId, EntityId("foo"))
+      .write(KijiOutput(args("uri"),
+          Map('record -> QualifiedColumnOutputSpec("family", "simple", classOf[SimpleRecord]))))
+}
+
+class UnpackGenericRecordJob(args: Args) extends KijiJob(args) {
+  KijiInput(args("input"),
+      Map(QualifiedColumnInputSpec("family", "simple", SimpleRecord.getClassSchema) -> 'slice))
+      .mapTo('slice -> 'record) { slice: Seq[FlowCell[GenericRecord]] => slice.head.datum }
+      .unpackTo[GenericRecord]('record -> ('l, 's, 'o))
+      .write(Tsv(args("output")))
+}
+
+class UnpackSpecificRecordJob(args: Args) extends KijiJob(args) {
+  KijiInput(args("input"),
+      Map(QualifiedColumnInputSpec("family", "simple", classOf[SimpleRecord]) -> 'slice))
+      .map('slice -> 'record) { slice: Seq[FlowCell[SimpleRecord]] => slice.head.datum }
+      .unpackTo[SimpleRecord]('record -> ('l, 's, 'o))
+      .write(Tsv(args("output")))
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/KijiPipeSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/KijiPipeSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..0bbd53730b92a88049db9bc69b41b14f3c206412
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/KijiPipeSuite.scala
@@ -0,0 +1,103 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import scala.collection.mutable.Buffer
+
+import com.twitter.scalding.Args
+import com.twitter.scalding.Job
+import com.twitter.scalding.JobTest
+import com.twitter.scalding.Tsv
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.flow.util.Resources._
+import org.kiji.express.repl.Implicits._
+import org.kiji.schema.KijiTable
+import org.kiji.schema.layout.KijiTableLayout
+import org.kiji.schema.layout.KijiTableLayouts
+
+@RunWith(classOf[JUnitRunner])
+class KijiPipeSuite extends KijiSuite {
+  /** Table layout to use for tests. */
+  val layout: KijiTableLayout = layout(KijiTableLayouts.SIMPLE_TWO_COLUMNS)
+
+  /** Input tuples to use for word count tests. */
+  def wordCountInput(uri: String): List[(EntityId, Seq[FlowCell[String]])] = {
+    List(
+      ( EntityId("row01"), slice("family:column1", (1L, "hello")) ),
+      ( EntityId("row02"), slice("family:column1", (2L, "hello")) ),
+      ( EntityId("row03"), slice("family:column1", (1L, "world")) ),
+      ( EntityId("row04"), slice("family:column1", (3L, "hello")) ))
+  }
+
+  /**
+   * Validates output from the word count tests.
+
+   * @param outputBuffer containing data that output buffer has in it after the job has been run.
+   */
+  def validateWordCount(outputBuffer: Buffer[(String, Int)]) {
+    val outMap = outputBuffer.toMap
+
+    // Validate that the output is as expected.
+    assert(3 === outMap("hello"))
+    assert(1 === outMap("world"))
+  }
+
+  // Create test Kiji table.
+  val uri: String = doAndRelease(makeTestKijiTable(layout)) { table: KijiTable =>
+    table.getURI().toString()
+  }
+
+  // A name for a dummy file for test job output.
+  val outputFile: String = "outputFile"
+
+  // A job obtained by converting a Cascading Pipe to a KijiPipe, which is then used to obtain
+  // a Scalding Job from the pipe.
+  def jobToRun(args: Args): Job = {
+    // Setup input to bind values from the "family:column1" column to the symbol 'word.
+    KijiInput(uri, "family:column1" -> 'word)
+    // Sanitize the word.
+    .map('word -> 'cleanword) { words: Seq[FlowCell[CharSequence]] =>
+      words.head.datum
+          .toString()
+          .toLowerCase()
+    }
+    // Count the occurrences of each word.
+    .groupBy('cleanword) { occurences => occurences.size }
+    // Write the result to a file.
+    .write(Tsv("outputFile"))
+    .getJob(args)
+  }
+
+  /** The job tester we'll use to run the test job in either local or hadoop mode. */
+  val jobTest = JobTest(jobToRun(_))
+      .source(KijiInput(uri, "family:column1" -> 'word), wordCountInput(uri))
+      .sink(Tsv("outputFile"))(validateWordCount)
+
+  test("A KijiPipe can be used to obtain a Scalding job that is run in local mode.") {
+    jobTest.run.finish
+  }
+
+  test("A KijiPipe can be used to obtain a Scalding job that is run with Hadoop.") {
+    jobTest.runHadoop.finish
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/KijiSourceSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/KijiSourceSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..be7802f6dc8d9fa468ee6c3c4d0686082fa20ca0
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/KijiSourceSuite.scala
@@ -0,0 +1,995 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import java.util.UUID
+
+import scala.collection.mutable.Buffer
+
+import cascading.tuple.Fields
+import com.twitter.scalding.Args
+import com.twitter.scalding.JobTest
+import com.twitter.scalding.TextLine
+import com.twitter.scalding.Tsv
+import org.apache.avro.generic.GenericRecord
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.avro.SimpleRecord
+import org.kiji.express.flow.SchemaSpec.Specific
+import org.kiji.express.flow.util.Resources.doAndRelease
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiTable
+import org.kiji.schema.avro.AvroValidationPolicy
+import org.kiji.schema.avro.HashSpec
+import org.kiji.schema.avro.HashType
+import org.kiji.schema.avro.TableLayoutDesc
+import org.kiji.schema.avro.TestRecord
+import org.kiji.schema.layout.KijiTableLayout
+import org.kiji.schema.layout.KijiTableLayouts
+import org.kiji.schema.layout.TableLayoutBuilder
+
+@RunWith(classOf[JUnitRunner])
+class KijiSourceSuite
+    extends KijiSuite {
+  import KijiSourceSuite._
+
+  /** Simple table layout to use for tests. The row keys are hashed. */
+  val simpleLayout: KijiTableLayout = layout(KijiTableLayouts.SIMPLE_TWO_COLUMNS)
+
+  /** Table layout using Avro schemas to use for tests. The row keys are formatted. */
+  val avroLayout: KijiTableLayout = layout("layout/avro-types.json")
+
+  /** Input tuples to use for word count tests. */
+  def wordCountInput(uri: String): List[(EntityId, Seq[FlowCell[String]])] = {
+    List(
+        ( EntityId("row01"), slice("family:column1", (1L, "hello")) ),
+        ( EntityId("row02"), slice("family:column1", (2L, "hello")) ),
+        ( EntityId("row03"), slice("family:column1", (1L, "world")) ),
+        ( EntityId("row04"), slice("family:column1", (3L, "hello")) ))
+  }
+
+  /**
+   * Validates output from [[com.twitter.scalding.examples.WordCountJob]].
+   *
+   * @param outputBuffer containing data that output buffer has in it after the job has been run.
+   */
+  def validateWordCount(outputBuffer: Buffer[(String, Int)]) {
+    val outMap = outputBuffer.toMap
+
+    // Validate that the output is as expected.
+    assert(3 === outMap("hello"))
+    assert(1 === outMap("world"))
+  }
+
+  test("a word-count job that reads from a Kiji table is run using Scalding's local mode") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Build test job.
+    JobTest(new WordCountJob(_))
+        .arg("input", uri)
+        .arg("output", "outputFile")
+        .source(KijiInput(uri, "family:column1" -> 'word), wordCountInput(uri))
+        .sink(Tsv("outputFile"))(validateWordCount)
+        // Run the test job.
+        .run
+        .finish
+  }
+
+  test("a word-count job that reads from a Kiji table is run using Hadoop") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Build test job.
+    JobTest(new WordCountJob(_))
+        .arg("input", uri)
+        .arg("output", "outputFile")
+        .source(KijiInput(uri, "family:column1" -> 'word), wordCountInput(uri))
+        .sink(Tsv("outputFile"))(validateWordCount)
+        // Run the test job.
+        .runHadoop
+        .finish
+  }
+
+  /** Input tuples to use for import tests. */
+  val importMultipleTimestamps: List[(String, String)] = List(
+      ( "0", "1 eid1 word1" ),
+      ( "1", "3 eid1 word2" ),
+      ( "2", "5 eid2 word3" ),
+      ( "3", "7 eid2 word4" ))
+
+  /**
+   * Validates output from [[org.kiji.express.flow.KijiSourceSuite.ImportJob]].
+   *
+   * @param outputBuffer containing data that the Kiji table has in it after the job has been run.
+   */
+
+  def validateMultipleTimestamps(outputBuffer: Buffer[(EntityId, Seq[FlowCell[CharSequence]])]) {
+    assert(outputBuffer.size === 2)
+
+    // There should be two Cells written to each column.
+    assert(outputBuffer(0)._2.size === 2)
+    assert(outputBuffer(1)._2.size === 2)
+
+    assert(outputBuffer(0)._2.head.datum.toString === "word2")
+    assert(outputBuffer(0)._2.last.datum.toString === "word1")
+    assert(outputBuffer(1)._2.head.datum.toString === "word4")
+    assert(outputBuffer(1)._2.last.datum.toString === "word3")
+  }
+
+  test("An import job with multiple timestamps imports all timestamps in local mode.") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Build test job.
+    JobTest(new MultipleTimestampsImportJob(_))
+        .arg("input", "inputFile")
+        .arg("output", uri)
+        .source(TextLine("inputFile"), importMultipleTimestamps)
+        .sink(KijiOutput(uri, 'timestamp, 'word -> "family:column1"))(validateMultipleTimestamps)
+        // Run the test job.
+        .run
+        .finish
+  }
+
+  test("An import with multiple timestamps imports all timestamps using Hadoop.") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Build test job.
+    JobTest(new MultipleTimestampsImportJob(_))
+        .arg("input", "inputFile")
+        .arg("output", uri)
+        .source(TextLine("inputFile"), importMultipleTimestamps)
+        .sink(KijiOutput(uri, 'timestamp, 'word -> "family:column1"))(validateMultipleTimestamps)
+        // Run the test job.
+        .run
+        .finish
+  }
+
+  /** Input tuples to use for import tests. */
+  val importInput: List[(String, String)] = List(
+      ( "0", "hello hello hello world world hello" ),
+      ( "1", "world hello   world      hello" ))
+
+  /**
+   * Validates output from [[org.kiji.express.flow.KijiSourceSuite.ImportJob]].
+   *
+   * @param outputBuffer containing data that the Kiji table has in it after the job has been run.
+   */
+
+  def validateImport(outputBuffer: Buffer[(EntityId, Seq[FlowCell[CharSequence]])]) {
+    assert(10 === outputBuffer.size)
+
+    // Perform a non-distributed word count.
+    val wordCounts: (Int, Int) = outputBuffer
+        // Extract words from each row.
+        .flatMap { row =>
+          val (_, slice) = row
+          slice
+              .map { cell: FlowCell[CharSequence] =>
+              cell.datum
+              }
+        }
+        // Count the words.
+        .foldLeft((0, 0)) { (counts, word) =>
+          // Unpack the counters.
+          val (helloCount, worldCount) = counts
+
+          // Increment the appropriate counter and return both.
+          word.toString() match {
+            case "hello" => (helloCount + 1, worldCount)
+            case "world" => (helloCount, worldCount + 1)
+          }
+        }
+
+    // Make sure that the counts are as expected.
+    assert((6, 4) === wordCounts)
+  }
+
+  test("an import job that writes to a Kiji table is run using Scalding's local mode") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Build test job.
+    JobTest(new ImportJob(_))
+        .arg("input", "inputFile")
+        .arg("output", uri)
+        .source(TextLine("inputFile"), importInput)
+        .sink(KijiOutput(uri, 'word -> "family:column1"))(validateImport)
+        // Run the test job.
+        .run
+        .finish
+  }
+
+  test("an import job that writes to a Kiji table is run using Hadoop") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Build test job.
+    JobTest(new ImportJob(_))
+        .arg("input", "inputFile")
+        .arg("output", uri)
+        .source(TextLine("inputFile"), importInput)
+        .sink(KijiOutput(uri, 'word -> "family:column1"))(validateImport)
+        // Run the test job.
+        .runHadoop
+        .finish
+  }
+
+  val importWithTimeInput = (0L, "Line-0") :: (1L, "Line-1") :: (2L, "Line-2") :: Nil
+
+  /**
+   * Validates output from [[org.kiji.express.flow.KijiSourceSuite.ImportJobWithTime]].
+   *
+   * @param outputBuffer containing data that the Kiji table has in it after the job has been run.
+   */
+  def validateImportWithTime(outputBuffer: Buffer[(EntityId, Seq[FlowCell[CharSequence]])]) {
+    // There should be one cell per row in the output.
+    val cellsPerRow = outputBuffer.unzip._2.map { m => (m.head.version, m.head.datum) }
+    // Sort by timestamp.
+    val cellsSortedByTime = cellsPerRow.sortBy { case(ts, line) => ts }
+    // Verify contents.
+    (0 until 2).foreach { index =>
+      val cell = cellsSortedByTime(index)
+      assert(index.toLong === cell._1)
+      assert("Line-" + index === cell._2.toString)
+    }
+  }
+
+  test("an import job that writes to a Kiji table with timestamps is run using local mode") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Build test job.
+    JobTest(new ImportJobWithTime(_))
+    .arg("input", "inputFile")
+    .arg("output", uri)
+    .source(TextLine("inputFile"), importWithTimeInput)
+    .sink(KijiOutput(uri, 'offset, 'line -> "family:column1"))(validateImportWithTime)
+    // Run the test job.
+    .run
+    .finish
+  }
+
+  test("an import job that writes to a Kiji table with timestamps is run using Hadoop") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Build test job.
+    JobTest(new ImportJobWithTime(_))
+    .arg("input", "inputFile")
+    .arg("output", uri)
+    .source(TextLine("inputFile"), importWithTimeInput)
+    .sink(KijiOutput(uri, 'offset, 'line -> "family:column1"))(validateImportWithTime)
+    // Run the test job.
+    .runHadoop
+    .finish
+  }
+
+  // Input tuples to use for version count tests.
+  def versionCountInput(uri: String): List[(EntityId, Seq[FlowCell[String]])] = {
+    List(
+        ( EntityId("row01"), slice("family:column1", (10L, "two"), (20L, "two")) ),
+        ( EntityId("row02"), slice("family:column1",
+            (10L, "three"),
+            (20L, "three"),
+            (30L, "three") ) ),
+        ( EntityId("row03"), slice("family:column1", (10L, "hello")) ))
+  }
+
+  test("a job that requests maxVersions gets them") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    def validateVersionCount(outputBuffer: Buffer[(Int, Int)]) {
+      val outMap = outputBuffer.toMap
+      // There should be two rows with 2 returned versions
+      // since one input row has 3 versions but we only requested two.
+      assert(1 == outMap(1))
+      assert(2 == outMap(2))
+    }
+
+    // Build test job.
+    val source =
+        KijiInput(uri, Map((ColumnInputSpec("family:column1", maxVersions=2) -> 'words)))
+    JobTest(new VersionsJob(source)(_))
+        .arg("output", "outputFile")
+        .source(source, versionCountInput(uri))
+        .sink(Tsv("outputFile"))(validateVersionCount)
+        // Run the test job.
+        .run
+        .finish
+  }
+
+  test("a job that requests a time range gets them") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+
+    def validateVersionCount(outputBuffer: Buffer[(Int, Int)]) {
+      val outMap = outputBuffer.toMap
+      // There should be two rows with 2 returned versions
+      // since one input row has 3 versions but we only requested two.
+      assert(1 === outMap.size)
+      assert(2 === outMap(1))
+    }
+
+    // Build test job.
+    val source = KijiInput(uri, Between(15L, 25L), "family:column1" -> 'words)
+    JobTest(new VersionsJob(source)(_))
+        .arg("output", "outputFile")
+        .source(source, versionCountInput(uri))
+        .sink(Tsv("outputFile"))(validateVersionCount)
+        // Run the test job.
+        .run
+        .finish
+  }
+
+  test("Specific records can be returned to JobTest's validate method") {
+    val uri: String = doAndRelease(makeTestKiji()) { kiji: Kiji =>
+      val baseDesc: TableLayoutDesc = KijiTableLayouts.getLayout(KijiTableLayouts.SCHEMA_REG_TEST)
+      baseDesc.setVersion("layout-1.3.0")
+
+      val desc = new TableLayoutBuilder(baseDesc, kiji)
+          .withAvroValidationPolicy(new KijiColumnName("info:fullname"), AvroValidationPolicy.NONE)
+          .build()
+
+      // Create test Kiji table.
+      doAndRelease {
+        kiji.createTable(desc)
+        kiji.openTable(desc.getName)
+      } { table: KijiTable =>
+        table.getURI.toString
+      }
+    }
+
+    // Build test job.
+    class TestSpecificRecordWriteJob(args: Args) extends KijiJob(args) {
+      Tsv(args("input"), ('entityId, 'fullname))
+          .write(
+              KijiOutput(
+                  tableUri = args("output"),
+                  columns = Map(
+                      'fullname -> QualifiedColumnOutputSpec(
+                          family = "info",
+                          qualifier = "fullname",
+                          schemaSpec = SchemaSpec.Specific(classOf[TestRecord])
+                      )
+                  )
+              )
+          )
+    }
+
+    val inputRecord1 = TestRecord
+        .newBuilder()
+        .setA("foo")
+        .setB(2)
+        .setC(4)
+        .build()
+    val inputRecord2 = TestRecord
+        .newBuilder()
+        .setA("bar")
+        .setB(1)
+        .setC(3)
+        .build()
+    val inputRecord3 = TestRecord
+        .newBuilder()
+        .setA("baz")
+        .setB(9)
+        .setC(8)
+        .build()
+    val inputRecords = Seq(
+        (EntityId("row01"), inputRecord1),
+        (EntityId("row02"), inputRecord2),
+        (EntityId("row03"), inputRecord3)
+    )
+
+    def validateSpecificWrite(outputBuffer: Buffer[(EntityId, Seq[FlowCell[TestRecord]])]) {
+      val outputMap = outputBuffer.toMap
+      assert(outputMap(EntityId("row01")).head.datum.getClass === classOf[TestRecord])
+      assert(outputMap(EntityId("row02")).head.datum.getClass === classOf[TestRecord])
+      assert(outputMap(EntityId("row03")).head.datum.getClass === classOf[TestRecord])
+      assert(outputMap(EntityId("row01")).head.datum === inputRecord1)
+      assert(outputMap(EntityId("row02")).head.datum === inputRecord2)
+      assert(outputMap(EntityId("row03")).head.datum === inputRecord3)
+    }
+
+    JobTest(new TestSpecificRecordWriteJob(_))
+        .arg("input", "inputFile")
+        .arg("output", uri)
+        .source(Tsv("inputFile", new Fields("entityId", "fullname")), inputRecords)
+        .sink(
+            KijiOutput(
+                uri,
+                Map(
+                    'fullname -> QualifiedColumnOutputSpec(
+                        family = "info",
+                        qualifier = "fullname",
+                        schemaSpec = SchemaSpec.Specific(classOf[TestRecord])
+                    )
+                )
+            )
+        ) (validateSpecificWrite)
+        .run
+        .runHadoop
+        .finish
+  }
+
+  // TODO(EXP-7): Write this test.
+  test("a word-count job that uses the type-safe api is run") {
+    pending
+  }
+
+  // TODO(EXP-6): Write this test.
+  test("a job that uses the matrix api is run") {
+    pending
+  }
+
+  test("test conversion of column value of type string between java and scala in Hadoop mode") {
+    def validateSimpleAvroChecker(outputBuffer: Buffer[(String, Int)]) {
+      val outMap = outputBuffer.toMap
+      // Validate that the output is as expected.
+      intercept[java.util.NoSuchElementException]{outMap("false")}
+      assert(6 === outMap("true"))
+    }
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+    // Input tuples to use for avro/scala conversion tests.
+    val avroCheckerInput: List[(EntityId, Seq[FlowCell[String]])] = List(
+        ( EntityId("row01"), slice("family:column1", (10L,"two"), (20L, "two")) ),
+        ( EntityId("row02"), slice("family:column1",
+            (10L, "three"),
+            (20L, "three"),
+            (30L, "three") ) ),
+        ( EntityId("row03"), slice("family:column1", (10L, "hello")) ))
+    // Build test job.
+    val testSource = KijiInput(
+        uri,
+        Map(ColumnInputSpec("family:column1", maxVersions=all) -> 'word))
+    JobTest(new AvroToScalaChecker(testSource)(_))
+      .arg("input", uri)
+      .arg("output", "outputFile")
+      .source(testSource, avroCheckerInput)
+      .sink(Tsv("outputFile"))(validateSimpleAvroChecker)
+    // Run the test job.
+      .runHadoop
+      .finish
+  }
+
+  test("A job that reads using the generic API is run.") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    val specificRecord = new HashSpec()
+    specificRecord.setHashType(HashType.MD5)
+    specificRecord.setHashSize(13)
+    specificRecord.setSuppressKeyMaterialization(true)
+    def genericReadInput(uri: String): List[(EntityId, Seq[FlowCell[HashSpec]])] = {
+      List((EntityId("row01"), slice("family:column3", (10L, specificRecord))))
+    }
+
+    def validateGenericRead(outputBuffer: Buffer[(Int, Int)]): Unit = {
+      assert (1 === outputBuffer.size)
+      // There exactly 1 record with hash_size of 13.
+      assert ((13, 1) === outputBuffer(0))
+    }
+
+    val jobTest = JobTest(new GenericAvroReadJob(_))
+        .arg("input", uri)
+        .arg("output", "outputFile")
+        .source(KijiInput(uri, Map (ColumnInputSpec("family:column3") -> 'records)),
+            genericReadInput(uri))
+        .sink(Tsv("outputFile"))(validateGenericRead)
+
+    // Run in local mode
+    jobTest.run.finish
+
+    // Run in hadoop mode
+    jobTest.runHadoop.finish
+  }
+
+  test("A job that reads using the specific API is run.") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    val specificRecord = new HashSpec()
+    specificRecord.setHashType(HashType.MD5)
+    specificRecord.setHashSize(13)
+    specificRecord.setSuppressKeyMaterialization(true)
+    def genericReadInput(uri: String): List[(EntityId, Seq[FlowCell[HashSpec]])] = {
+      List((EntityId("row01"), slice("family:column3", (10L, specificRecord))))
+    }
+
+    def validateSpecificRead(outputBuffer: Buffer[(Int, Int)]): Unit = {
+      assert (1 === outputBuffer.size)
+      // There exactly 1 record with hash_size of 13.
+      assert ((13, 1) === outputBuffer(0))
+    }
+
+    // Defining the KijiSource directly like this is unfortunate, but necessary to make sure that
+    // the KijiSource referenced here and the one used within SpecificAvroReadJob are identical (the
+    // KijiSources are used as keys for a map of buffers for test code).
+    val ksource = new KijiSource(
+        tableAddress = uri,
+        timeRange = All,
+        timestampField = None,
+        inputColumns = Map('records -> ColumnInputSpec(
+          "family:column3", schemaSpec = Specific(classOf[HashSpec]))),
+        outputColumns = Map('records -> QualifiedColumnOutputSpec("family:column3"))
+    )
+
+    val jobTest = JobTest(new SpecificAvroReadJob(_))
+        .arg("input", uri)
+        .arg("output", "outputFile")
+        .source(ksource, genericReadInput(uri))
+        .sink(Tsv("outputFile"))(validateSpecificRead)
+
+    // Run in local mode
+    jobTest.run.finish
+
+    // Run in hadoop mode
+    jobTest.runHadoop.finish
+  }
+
+  test("A job that writes using the generic API is run.") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Input to use with Text source.
+    val genericWriteInput: List[(String, String)] = List(
+        ( "0", "zero" ),
+        ( "1", "one" ))
+
+    // Validates the output buffer contains the same as the input buffer.
+    def validateGenericWrite(outputBuffer: Buffer[(EntityId, Seq[FlowCell[GenericRecord]])]) {
+      val inputMap: Map[Long, String] = genericWriteInput.map { t => t._1.toLong -> t._2 }.toMap
+      outputBuffer.foreach { t: (EntityId, Seq[FlowCell[GenericRecord]]) =>
+        val entityId = t._1
+        val record = t._2.head.datum
+
+        val s = record.get("s").asInstanceOf[String]
+        val l = record.get("l").asInstanceOf[Long]
+
+        assert(entityId(0) === s)
+        assert(inputMap(l) === s)
+      }
+    }
+
+    val jobTest = JobTest(new GenericAvroWriteJob(_))
+      .arg("input", "inputFile")
+      .arg("output", uri)
+      .source(TextLine("inputFile"), genericWriteInput)
+      .sink(KijiOutput(uri, 'record -> "family:column4"))(validateGenericWrite)
+
+    // Run in local mode.
+    jobTest.run.finish
+
+    // Run in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+
+  test ("A job that writes to map-type column families is run.") {
+    // URI of the Kiji table to use.
+    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Input text.
+    val mapTypeInput: List[(String, String)] = List(
+        ("0", "dogs 4"),
+        ("1", "cats 5"),
+        ("2", "fish 3"))
+
+    // Validate output.
+    def validateMapWrite(
+        outputBuffer: Buffer[(EntityId,Seq[FlowCell[GenericRecord]])]
+    ): Unit = {
+      assert (1 === outputBuffer.size)
+      val outputSlice = outputBuffer(0)._2
+      val outputSliceMap = outputSlice.groupBy(_.qualifier)
+      assert (4 === outputSliceMap("dogs").head.datum)
+      assert (5 === outputSliceMap("cats").head.datum)
+      assert (3 === outputSliceMap("fish").head.datum)
+    }
+
+    // Create the JobTest for this test.
+    val jobTest = JobTest(new MapWriteJob(_))
+        .arg("input", "inputFile")
+        .arg("table", uri)
+        .source(TextLine("inputFile"), mapTypeInput)
+        .sink(KijiOutput(uri, Map('resultCount ->
+            new ColumnFamilyOutputSpec("searches", 'terms))))(validateMapWrite)
+
+    // Run the test.
+    jobTest.run.finish
+    // Run the test in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+
+  test ("A job that writes to map-type column families with numeric column qualifiers is run.") {
+    // URI of the Kiji table to use.
+    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Create input using mapSlice.
+    val mapTypeInput: List[(EntityId, Seq[FlowCell[String]])] = List(
+        ( EntityId("0row"), mapSlice("animals", ("0column", 0L, "0 dogs")) ),
+        ( EntityId("1row"), mapSlice("animals", ("0column", 0L, "1 cat")) ),
+        ( EntityId("2row"), mapSlice("animals", ("0column", 0L, "2 fish")) ))
+
+    // Validate output.
+    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
+      assert(outputBuffer.size === 3)
+      val outputSet = outputBuffer.map { value: Tuple1[String] =>
+        value._1
+      }.toSet
+      assert (outputSet.contains("0 dogs"), "Failed on \"0 dogs\" test")
+      assert (outputSet.contains("1 cat"), "Failed on \"1 cat\" test")
+      assert (outputSet.contains("2 fish"), "Failed on \"2 fish\" test")
+    }
+
+    // Create the JobTest for this test.
+    val jobTest = JobTest(new MapSliceJob(_))
+        .arg("input", uri)
+        .arg("output", "outputFile")
+        .source(KijiInput(uri, "animals" -> 'terms), mapTypeInput)
+        .sink(Tsv("outputFile"))(validateTest)
+
+    // Run the test.
+    jobTest.run.finish
+    // Run the test in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+
+  test("A job that joins two pipes, on string keys, is run in both local and hadoop mode.") {
+    // URI of the Kiji table to use.
+    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    // Create input from Kiji table.
+    val joinKijiInput: List[(EntityId, Seq[FlowCell[String]])] = List(
+        ( EntityId("0row"), mapSlice("animals", ("0column", 0L, "0 dogs")) ),
+        ( EntityId("1row"), mapSlice("animals", ("0column", 0L, "1 cat")) ),
+        ( EntityId("2row"), mapSlice("animals", ("0column", 0L, "2 fish")) ))
+
+    // Create input from side data.
+    val sideInput: List[(String, String)] = List( ("0", "0row"), ("1", "2row") )
+
+    // Validate output.
+    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
+      assert(outputBuffer.size === 2)
+    }
+
+    // Create the JobTest for this test.
+    val jobTest = JobTest(new JoinOnStringsJob(_))
+        .arg("input", uri)
+        .arg("side-input", "sideInputFile")
+        .arg("output", "outputFile")
+        .source(KijiInput(uri, "animals" -> 'animals), joinKijiInput)
+        .source(TextLine("sideInputFile"), sideInput)
+        .sink(Tsv("outputFile"))(validateTest)
+
+    // Run the test in local mode.
+    jobTest.run.finish
+
+    // Run the test in hadoop mode.
+    jobTest.runHadoop.finish
+  }
+}
+
+/** Companion object for KijiSourceSuite. Contains test jobs. */
+object KijiSourceSuite {
+  /**
+   * A job that extracts the most recent string value from the column "family:column1" for all rows
+   * in a Kiji table, and then counts the number of occurrences of those strings across rows.
+   *
+   * @param args to the job. Two arguments are expected: "input", which should specify the URI
+   *     to the Kiji table the job should be run on, and "output", which specifies the output
+   *     Tsv file.
+   */
+  class WordCountJob(args: Args) extends KijiJob(args) {
+    // Setup input to bind values from the "family:column1" column to the symbol 'word.
+    KijiInput(args("input"), "family:column1" -> 'word)
+        // Sanitize the word.
+        .map('word -> 'cleanword) { words:Seq[FlowCell[CharSequence]] =>
+          words.head.datum
+              .toString()
+              .toLowerCase()
+        }
+        // Count the occurrences of each word.
+        .groupBy('cleanword) { occurences => occurences.size }
+        // Write the result to a file.
+        .write(Tsv(args("output")))
+  }
+
+  /**
+   * A job that takes the most recent string value from the column "family:column1" and adds
+   * the letter 's' to the end of it. It passes through the column "family:column2" without
+   * any changes.
+   *
+   * @param args to the job. Two arguments are expected: "input", which should specify the URI
+   *     to the Kiji table the job should be run on, and "output", which specifies the output
+   *     Tsv file.
+   */
+  class TwoColumnJob(args: Args) extends KijiJob(args) {
+    // Setup input to bind values from the "family:column1" column to the symbol 'word.
+    KijiInput(args("input"), "family:column1" -> 'word1, "family:column2" -> 'word2)
+        .map('word1 -> 'pluralword) { words: Seq[FlowCell[CharSequence]] =>
+          words.head.datum.toString() + "s"
+        }
+        .write(Tsv(args("output")))
+  }
+
+  /**
+   * A job that requests specific number of versions and buckets the results by the number of
+   * versions.  The result is pairs of number of versions and the number of rows with that number
+   * of versions.
+   *
+   * @param source that the job will use.
+   * @param args to the job. Two arguments are expected: "input", which should specify the URI
+   *     to the Kiji table the job should be run on, and "output", which specifies the output
+   *     Tsv file.
+   */
+  class VersionsJob(source: KijiSource)(args: Args) extends KijiJob(args) {
+    source
+        // Count the size of words (number of versions).
+        .map('words -> 'versioncount) { words: Seq[FlowCell[String]]=>
+          words.size
+        }
+        .groupBy('versioncount) (_.size)
+        .write(Tsv(args("output")))
+  }
+
+  class MultipleTimestampsImportJob(args: Args) extends KijiJob(args) {
+    // Setup input.
+    TextLine(args("input"))
+        .read
+        // new  the words in each line.
+        .map('line -> ('timestamp, 'entityId, 'word)) { line: String =>
+          val Array(timestamp, eid, token) = line.split("\\s+")
+          (timestamp.toLong, EntityId(eid), token)
+        }
+        // Write the results to the "family:column1" column of a Kiji table.
+        .write(KijiOutput(args("output"), 'timestamp, 'word -> "family:column1"))
+  }
+
+  /**
+   * A job that, for each line in a text file, splits the line into words,
+   * and for each word occurrence writes a copy of the word to the column "family:column1" in a
+   * row for that word in a Kiji table.
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the path to a
+   *     text file, and "output", which specifies the URI to a Kiji table.
+   */
+  class ImportJob(args: Args) extends KijiJob(args) {
+    // Setup input.
+    TextLine(args("input"))
+        .read
+        // new  the words in each line.
+        .flatMap('line -> 'word) { line : String => line.split("\\s+") }
+        // Generate an entityId for each word.
+        .map('word -> 'entityId) { _: String =>
+            EntityId(UUID.randomUUID().toString()) }
+        // Write the results to the "family:column1" column of a Kiji table.
+        .write(KijiOutput(args("output"), 'word -> "family:column1"))
+  }
+
+  /**
+   * A job that, given lines of text, writes each line to row for the line in a Kiji table,
+   * at the column "family:column1", with the offset provided for each line being used as the
+   * timestamp.
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the path to a
+   *     text file, and "output", which specifies the URI to a Kiji table.
+   */
+  class ImportJobWithTime(args: Args) extends KijiJob(args) {
+    // Setup input.
+    TextLine(args("input"))
+        .read
+        // Generate an entityId for each line.
+        .map('line -> 'entityId) { EntityId(_: String) }
+        // Write the results to the "family:column1" column of a Kiji table.
+        .write(KijiOutput(args("output"), 'offset, 'line -> "family:column1"))
+  }
+
+  /**
+   * A job that given input from a Kiji table, ensures the type is accurate.
+   *
+   * @param source that the job will use.
+   * @param args to the job. The input URI for the table and the output file.
+   */
+  class AvroToScalaChecker(source: KijiSource)(args: Args) extends KijiJob(args) {
+    source
+
+        .flatMap('word -> 'matches) { word: Seq[FlowCell[CharSequence]] =>
+          word.map { cell: FlowCell[CharSequence] =>
+            val value = cell.datum
+            if (value.isInstanceOf[CharSequence]) {
+              "true"
+            } else {
+              "false"
+            }
+          }
+        }
+        .groupBy('matches) (_.size)
+        .write(Tsv(args("output")))
+  }
+
+  /**
+   * A job that uses the generic API, getting the "hash_size" field from a generic record, and
+   * writes the number of records that have a certain hash_size.
+   *
+   * @param args to the job. Two arguments are expected: "input", which should specify the URI
+   *     to the Kiji table the job should be run on, and "output", which specifies the output
+   *     Tsv file.
+   */
+  class GenericAvroReadJob(args: Args) extends KijiJob(args) {
+    KijiInput(args("input"), "family:column3" -> 'records)
+        .map('records -> 'hashSizeField) { slice: Seq[FlowCell[GenericRecord]] =>
+          slice.head match {
+            case FlowCell(_, _, _, record: GenericRecord) => {
+              record
+                  .get("hash_size")
+                  .asInstanceOf[Int]
+            }
+          }
+        }
+        .groupBy('hashSizeField)(_.size)
+        .write(Tsv(args("output")))
+  }
+
+  /**
+   * A job that uses the generic API, getting the "hash_size" field from a generic record, and
+   * writes the number of records that have a certain hash_size.
+   *
+   * @param args to the job. Two arguments are expected: "input", which should specify the URI
+   *     to the Kiji table the job should be run on, and "output", which specifies the output
+   *     Tsv file.
+   */
+  class SpecificAvroReadJob(args: Args) extends KijiJob(args) {
+    // Want to read some data out to 'records and then write it back to a Tsv
+    val ksource = new KijiSource(
+        tableAddress = args("input"),
+        timeRange = All,
+        timestampField = None,
+        inputColumns = Map('records -> ColumnInputSpec(
+            "family:column3", schemaSpec = Specific(classOf[HashSpec]))),
+        outputColumns = Map('records -> QualifiedColumnOutputSpec("family:column3")))
+    ksource
+        .map('records -> 'hashSizeField) { slice: Seq[FlowCell[HashSpec]] =>
+          val FlowCell(_, _, _, record) = slice.head
+          record.getHashSize
+        }
+        .groupBy('hashSizeField)(_.size)
+        .write(Tsv(args("output")))
+  }
+
+  /**
+   * A job that uses the generic API, creating a record containing the text from the input,
+   * and writing it to a Kiji table.
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the path to a
+   *     text file, and "output", which specifies the URI to a Kiji table.
+   */
+  class GenericAvroWriteJob(args: Args) extends KijiJob(args) {
+    val tableUri: String = args("output")
+    TextLine(args("input"))
+        .read
+        .map('offset -> 'timestamp) { offset: String => offset.toLong }
+        .map('offset -> 'l) { offset: String => offset.toLong }
+        // Generate an entityId for each line.
+        .map('line -> 'entityId) { EntityId(_: String) }
+        .rename('line -> 's)
+        .packGenericRecord(('l, 's) -> 'record)(SimpleRecord.getClassSchema)
+        // Write the results to the "family:column4" column of a Kiji table.
+        .project('entityId, 'record)
+        .write(KijiOutput(args("output"), 'record -> "family:column4"))
+  }
+
+  /**
+   * A job that writes to a map-type column family.  It takes text from the input and uses it as
+   * search terms and the number of results returned for that term.  All of them belong to the same
+   * entity, "my_eid".
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the path to a
+   *     text file, and "output", which specifies the URI to a Kiji table.
+   */
+  class MapWriteJob(args: Args) extends KijiJob(args) {
+    TextLine(args("input"))
+        .read
+        // Create an entity ID for each line (always the same one, here)
+        .map('line -> 'entityId) { line: String => EntityId("my_eid") }
+        // new  the number of result for each search term
+        .map('line -> ('terms, 'resultCount)) { line: String =>
+          (line.split(" ")(0), line.split(" ")(1).toInt)
+        }
+        // Write the results to the "family:column1" column of a Kiji table.
+        .write(KijiOutput(args("table"), Map('resultCount ->
+          new ColumnFamilyOutputSpec("searches", 'terms))))
+  }
+
+  /**
+   * A job that tests map-type column families using sequences of cells and outputs the results to
+   * a TSV.
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
+   *     Kiji table, and "output", which specifies the path to a text file.
+   */
+  class MapSliceJob(args: Args) extends KijiJob(args) {
+    KijiInput(args("input"), "animals" -> 'terms)
+        .map('terms -> 'values) { terms: Seq[FlowCell[CharSequence]] => terms.head.datum }
+        .project('values)
+        .write(Tsv(args("output")))
+  }
+
+  /**
+   * A job that tests joining two pipes, on String keys.
+   *
+   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
+   *     Kiji table, and "output", which specifies the path to a text file.
+   */
+  class JoinOnStringsJob(args: Args) extends KijiJob(args) {
+    val sidePipe = TextLine(args("side-input"))
+        .read
+        .map('line -> 'entityId) { line: String => EntityId(line) }
+
+    KijiInput(args("input"), "animals" -> 'animals)
+        .map('animals -> 'terms) { animals: Seq[FlowCell[CharSequence]] =>
+          animals.head.datum.toString.split(" ")(0) + "row" }
+        .discard('entityId)
+        .joinWithSmaller('terms -> 'line, sidePipe)
+        .write(Tsv(args("output")))
+  }
+}
+
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/PackUnpackRecordSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/PackUnpackRecordSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..e4622a94dcbdf28fc26ba5e5fa502cc877b4c2fb
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/PackUnpackRecordSuite.scala
@@ -0,0 +1,145 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import cascading.pipe.Pipe
+import cascading.tuple.Fields
+import com.twitter.scalding.Args
+import com.twitter.scalding.TuplePacker
+import com.twitter.scalding.TupleUnpacker
+import org.apache.avro.generic.GenericRecord
+import org.apache.avro.generic.GenericRecordBuilder
+import org.apache.avro.specific.SpecificRecord
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.avro.SimpleRecord
+import org.kiji.express.flow.util.PipeRunner._
+
+@RunWith(classOf[JUnitRunner])
+class PackUnpackRecordSuite extends KijiSuite {
+
+  test("Avro tuple converters should be in implicit scope of KijiJobs.") {
+    class ImplicitJob(args: Args) extends KijiJob(args) {
+      assert(implicitly[TupleUnpacker[GenericRecord]].getClass ===
+          classOf[AvroGenericTupleUnpacker])
+      assert(implicitly[TuplePacker[SpecificRecord]].getClass ===
+          classOf[AvroSpecificTuplePacker[SpecificRecord]])
+      assert(implicitly[TuplePacker[SimpleRecord]].getClass ===
+          classOf[AvroSpecificTuplePacker[SimpleRecord]])
+    }
+  }
+
+  // Import implicits to simulate REPL environment
+  import org.kiji.express.repl.Implicits._
+
+  test("Avro tuple converters should be in implicit scope of express REPL.") {
+    assert(implicitly[TupleUnpacker[GenericRecord]].getClass ===
+        classOf[AvroGenericTupleUnpacker])
+    assert(implicitly[TuplePacker[SpecificRecord]].getClass ===
+        classOf[AvroSpecificTuplePacker[SpecificRecord]])
+    assert(implicitly[TuplePacker[SimpleRecord]].getClass ===
+        classOf[AvroSpecificTuplePacker[SimpleRecord]])
+  }
+
+  /** Schema of SimpleRecord Avro record. */
+  val schema = SimpleRecord.getClassSchema
+
+  /** Sample inputs for testing Avro packing / unpacking in tuple form. */
+  val input: List[(Long, String)] = List(
+    (1, "foobar"),
+    (2, "shoe"),
+    (3, "random"),
+    (99, "baloons"),
+    (356, "sumerians"))
+
+  /** Fields contained in the input. */
+  val inputFields: Fields = ('ls, 's)
+
+  /** Sample inputs for testing Avro packing / unpacking in Specific Record form. */
+  val specificRecords: List[SimpleRecord] = input.map { t =>
+    SimpleRecord.newBuilder.setL(t._1).setS(t._2).build()
+  }
+
+  /** Sample inputs for testing Avro packing / unpacking in Generic Record form. */
+  val genericRecords: List[GenericRecord] = input.map { t =>
+    new GenericRecordBuilder(schema).set("l", t._1).set("s", t._2).build()
+  }
+
+  /** Fields contained in record input. */
+  val recordFields: Fields = 'r
+
+  def validateSpecificSimpleRecord(outputs: Iterable[(Long, String, String, SimpleRecord)]) {
+    outputs.foreach { t =>
+      val (l, s, o, r) = t
+      assert(specificRecords.contains(r))
+      assert(r.getL === l)
+      assert(r.getS === s)
+      assert(r.getO === o)
+    }
+  }
+
+  def validateGenericSimpleRecord(outputs: Iterable[(Long, String, String, GenericRecord)]) {
+    outputs.foreach { t =>
+      val (l, s, o, r) = t
+      assert(genericRecords.contains(r))
+      assert(r.get("l") === l)
+      assert(r.get("s") === s)
+      assert(r.get("o") === o)
+    }
+  }
+
+  test("A KijiPipe can pack fields into a specific Avro record.") {
+    val pipe = new Pipe("pack specific record")
+        .map('ls -> 'l) { ls: String => ls.toLong }
+        .pack[SimpleRecord](('l, 's) -> 'r)
+        .map('r -> 'o) { r: SimpleRecord => r.getO }
+        .project('l, 's, 'o, 'r)
+
+    validateSpecificSimpleRecord(runPipe(pipe, inputFields, input))
+  }
+
+  test("A KijiPipe can pack fields into a generic Avro record.") {
+    val pipe = new Pipe("pack generic record")
+        .map('ls -> 'l) { ls: String => ls.toLong }
+        .packGenericRecord(('l, 's) -> 'r)(schema)
+        .map('r -> 'o) { r: GenericRecord => r.get("o") }
+        .project('l, 's, 'o, 'r)
+
+    validateGenericSimpleRecord(runPipe(pipe, inputFields, input))
+  }
+
+  test("A KijiPipe can unpack a specific Avro record into fields.") {
+    val pipe = new Pipe("unpack generic record")
+        .unpack[SimpleRecord]('r -> ('l, 's, 'o))
+        .project('l, 's, 'o, 'r)
+
+    validateSpecificSimpleRecord(runPipe(pipe, recordFields, specificRecords))
+  }
+
+  test("A KijiPipe can unpack a generic Avro record into fields.") {
+    val pipe = new Pipe("unpack generic record")
+        .unpack[GenericRecord]('r -> ('l, 's, 'o))
+        .project('l, 's, 'o, 'r)
+
+    validateGenericSimpleRecord(runPipe(pipe, recordFields, genericRecords))
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/PagedCellsSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/PagedCellsSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..8851143cd3a4c79b76096ae1de068ce4044a9b3a
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/PagedCellsSuite.scala
@@ -0,0 +1,175 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import scala.collection.mutable.Buffer
+
+import com.twitter.scalding.Args
+import com.twitter.scalding.JobTest
+import com.twitter.scalding.Tsv
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.flow.util.Resources.doAndRelease
+import org.kiji.schema.KijiTable
+import org.kiji.schema.layout.KijiTableLayout
+import org.kiji.schema.layout.KijiTableLayouts
+
+/**
+ * A job that extracts the most recent string value from the column "family:column1" for all rows
+ * in a Kiji table, and then concatenates those strings into a single word.
+ *
+ * @param args to the job. Two arguments are expected: "input", which should specify the URI
+ *     to the Kiji table the job should be run on, and "output", which specifies the output
+ *     Tsv file.
+ */
+class WordConcatJob(args: Args) extends KijiJob(args) {
+  // Setup input to bind values from the "family:column1" column to the symbol 'word.
+  KijiInput(
+      args("input"),
+      Map(ColumnInputSpec("family:column1", all, paging = PagingSpec.Cells(3)) -> 'word))
+    // Sanitize the word.
+    .map('word -> 'cleanword) { words: Seq[FlowCell[CharSequence]] =>
+      words.foldLeft("")((a: String, b: FlowCell[CharSequence]) => a + b.datum.toString)
+    }
+    // Count the occurrences of each word.
+    .groupBy('cleanword) { occurences => occurences.size }
+    // Write the result to a file.
+    .write(Tsv(args("output")))
+}
+
+/**
+ * A job that extracts the most recent string value from the column "family:column1" for all rows
+ * in a Kiji table, and then concatenates those strings into a single word.
+ *
+ * @param args to the job. Two arguments are expected: "input", which should specify the URI
+ *     to the Kiji table the job should be run on, and "output", which specifies the output
+ *     Tsv file.
+ */
+class WordCountFlatMapJob(args: Args) extends KijiJob(args) {
+  // Setup input to bind values from the "family:column1" column to the symbol 'word.
+  KijiInput(
+      args("input"),
+      Map(ColumnInputSpec("family:column1", all, paging = PagingSpec.Cells(3)) -> 'word))
+
+      // Sanitize the word.
+      .flatMap('word -> 'word) { words: Seq[FlowCell[CharSequence]] =>
+          words
+      }.map('word -> 'cleanword) {word: FlowCell[CharSequence] => word.datum.toString}
+      // Count the occurrences of each word.
+      .groupBy('cleanword) { occurences => occurences.size }
+      // Write the result to a file.
+      .write(Tsv(args("output")))
+}
+
+@RunWith(classOf[JUnitRunner])
+class PagedCellsSuite extends KijiSuite {
+  /** Simple table layout to use for tests. The row keys are hashed. */
+  val simpleLayout: KijiTableLayout = layout(KijiTableLayouts.SIMPLE_TWO_COLUMNS)
+
+  test("a word-concat job that reads from a Kiji table is run using Scalding's local mode") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    /** Input tuples to use for word count tests. */
+    def wordCountInput(uri: String): List[(EntityId, Seq[FlowCell[String]])] = {
+      List((EntityId("row01"), slice("family:column1",(1L, "hello"), (2L, "world"),(3L, "hello"),
+          (4L, "hello"))))
+    }
+
+    /**
+     * Validates output from WordConcatJob
+     *
+     * @param outputBuffer containing data that output buffer has in it after the job has been run.
+     */
+    def validateWordConcat(outputBuffer: Buffer[(String, Int)]) {
+      val outMap = outputBuffer.toMap
+      assert(1 === outMap("hellohelloworldhello"))
+    }
+
+    // Build test job.
+    JobTest(new WordConcatJob(_))
+      .arg("input", uri)
+      .arg("output", "outputFile")
+      .source(
+          KijiInput(
+            uri,
+            Map(ColumnInputSpec("family:column1", all, paging = PagingSpec.Cells(3)) -> 'word)),
+          wordCountInput(uri))
+      .sink(Tsv("outputFile"))(validateWordConcat)
+      // Run the test job.
+      .runHadoop
+      .finish
+  }
+
+  test("a word-count job that reads from a Kiji table is run using Scalding's local mode") {
+    // Create test Kiji table.
+    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
+      table.getURI().toString()
+    }
+
+    /** Input tuples to use for word count tests. */
+    def wordCountInput(uri: String): List[(EntityId, Seq[FlowCell[String]])] = {
+      List((EntityId("row01"), slice("family:column1",(1L, "hello"), (2L, "world"),(3L, "hello"),
+        (4L, "hello"))))
+    }
+
+    /**
+     * Validates output from WordConcatJob
+     *
+     * @param outputBuffer containing data that output buffer has in it after the job has been run.
+     */
+    def validateWordConcat(outputBuffer: Buffer[(String, Int)]) {
+      val outMap = outputBuffer.toMap
+      assert(1 === outMap("hellohelloworldhello"))
+    }
+
+    def validateWordCount(outputBuffer: Buffer[(String, Int)]) {
+      val outMap = outputBuffer.toMap
+
+      // Validate that the output is as expected.
+      assert(3 === outMap("hello"))
+      assert(1 === outMap("world"))
+    }
+
+    val column1 = ColumnInputSpec(
+        column = "family:column1",
+        maxVersions = all,
+        paging = PagingSpec.Cells(3)
+    )
+
+    // Build test job.
+    JobTest(new WordCountFlatMapJob(_))
+        .arg("input", uri)
+        .arg("output", "outputFile")
+        .source(
+            KijiInput(
+                uri,
+                Map(column1 -> 'word)),
+            wordCountInput(uri))
+        .sink(Tsv("outputFile"))(validateWordCount)
+        // Run the test job.
+        .runHadoop
+        .finish
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/ReaderSchemaSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/ReaderSchemaSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..0773fc221b22a845ea431890cb0cd7c55cf09445
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/ReaderSchemaSuite.scala
@@ -0,0 +1,253 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import com.twitter.scalding.Args
+import com.twitter.scalding.Mode
+import org.apache.avro.generic.GenericEnumSymbol
+import org.apache.avro.generic.GenericFixed
+import org.apache.avro.generic.GenericRecord
+import org.apache.hadoop.conf.Configuration
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.avro.SimpleRecord
+import org.kiji.express.flow.SchemaSpec.Generic
+import org.kiji.express.flow.SchemaSpec.Specific
+import org.kiji.express.flow.SchemaSpec.Writer
+import org.kiji.schema.{ EntityId => JEntityId }
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiClientTest
+import org.kiji.schema.KijiDataRequest
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiTableReader
+import org.kiji.schema.KijiTableWriter
+import org.kiji.schema.layout.KijiTableLayout
+
+@RunWith(classOf[JUnitRunner])
+class ReaderSchemaSuite extends KijiClientTest with KijiSuite {
+  import org.kiji.express.flow.util.AvroTypesComplete._
+  setupKijiTest()
+  val kiji: Kiji = createTestKiji()
+  val layout: KijiTableLayout = layout("layout/avro-types-complete.json")
+  val table: KijiTable = {
+    kiji.createTable(layout.getDesc)
+    kiji.openTable(layout.getName)
+  }
+  val conf: Configuration = getConf
+  val uri: String = table.getURI.toString
+  val reader: KijiTableReader = table.openTableReader()
+  val writer: KijiTableWriter = table.openTableWriter()
+
+  private def entityId(s: String): JEntityId = { table.getEntityId(s) }
+
+  private def writeValue(eid: String, column: String, value: Any) {
+    writer.put(entityId(eid), family, column, value)
+    writer.flush()
+  }
+
+  private def getValue[T](eid: String, column: String): T = {
+    val get = reader.get(entityId(eid), KijiDataRequest.create(family, column))
+    require(get.containsColumn(family, column)) // Require the cell exists for null case
+    get.getMostRecentValue(family, column)
+  }
+
+  private def testExpressReadWrite[T](
+      column: String,
+      value: Any,
+      schemaSpec: SchemaSpec,
+      overrideSchema: Option[SchemaSpec] = None
+  ) {
+    val readEid = column + "-in"
+    val writeEid = column + "-out"
+    writeValue(readEid, column, value)
+
+    val outputSchema = overrideSchema.getOrElse(schemaSpec)
+
+    val inputCol = QualifiedColumnInputSpec(family, column, schemaSpec = schemaSpec)
+    val outputCol = QualifiedColumnOutputSpec(family, column, outputSchema)
+
+    val args = Args("--hdfs")
+    Mode.mode = Mode(args, conf)
+    new ReadWriteJob[T](uri, inputCol, outputCol, writeEid, args).run
+    assert(value === getValue[T](writeEid, column))
+  }
+
+  test("A KijiJob can read a counter column with the writer schema.") {
+    testExpressReadWrite[Long](counterColumn, longs.head, Writer)
+  }
+
+  test("A KijiJob can read a raw bytes column with the writer schema.") {
+    testExpressReadWrite[Array[Byte]](rawColumn, bytes.head, Writer)
+  }
+
+  test("A KijiJob can read a null column with the writer schema.") {
+    testExpressReadWrite[Null](nullColumn, null, Writer)
+  }
+
+  test("A KijiJob can read a null column with a generic reader schema.") {
+    testExpressReadWrite[Null](nullColumn, null, Generic(nullSchema))
+  }
+
+  test("A KijiJob can read a boolean column with the writer schema.") {
+    testExpressReadWrite[Boolean](booleanColumn, booleans.head, Writer)
+  }
+
+  test("A KijiJob can read a boolean column with a generic reader schema.") {
+    testExpressReadWrite[Boolean](booleanColumn, booleans.head, Generic(booleanSchema))
+  }
+
+  test("A KijiJob can read an int column with the writer schema.") {
+    testExpressReadWrite[Int](intColumn, ints.head, Writer)
+  }
+
+  test("A KijiJob can read an int column with a generic reader schema.") {
+    testExpressReadWrite[Int](intColumn, ints.head, Generic(intSchema))
+  }
+
+  test("A KijiJob can read a long column with the writer schema.") {
+    testExpressReadWrite[Long](longColumn, longs.head, Writer)
+  }
+
+  test("A KijiJob can read a long column with a generic reader schema.") {
+    testExpressReadWrite[Long](longColumn, longs.head, Generic(longSchema))
+  }
+
+  test("A KijiJob can read a float column with the writer schema.") {
+    testExpressReadWrite[Float](floatColumn, floats.head, Writer)
+  }
+
+  test("A KijiJob can read a float column with a generic reader schema.") {
+    testExpressReadWrite[Float](floatColumn, floats.head, Generic(floatSchema))
+  }
+
+  test("A KijiJob can read a double column with the writer schema.") {
+    testExpressReadWrite[Double](doubleColumn, doubles.head, Writer)
+  }
+
+  test("A KijiJob can read a double column with a generic reader schema.") {
+    testExpressReadWrite[Double](doubleColumn, doubles.head, Generic(doubleSchema))
+  }
+
+  /** TODO: reenable when Schema-594 is fixed. */
+  ignore("A KijiJob can read a bytes column with the writer schema.") {
+    testExpressReadWrite[Array[Byte]](bytesColumn, bytes.head, Writer)
+  }
+
+  /** TODO: reenable when Schema-594 is fixed. */
+  ignore("A KijiJob can read a bytes column with a generic reader schema.") {
+    testExpressReadWrite[Array[Byte]](bytesColumn, bytes.head, Generic(bytesSchema))
+  }
+
+  test("A KijiJob can read a string column with the writer schema.") {
+    testExpressReadWrite[String](stringColumn, strings.head, Writer)
+  }
+
+  test("A KijiJob can read a string column with a generic reader schema.") {
+    testExpressReadWrite[String](stringColumn, strings.head, Generic(stringSchema))
+  }
+
+  test("A KijiJob can read a specific record column with the writer schema.") {
+    testExpressReadWrite[SimpleRecord](specificColumn, specificRecords.head, Writer)
+  }
+
+  test("A KijiJob can read a specific record column with a generic reader schema.") {
+    testExpressReadWrite[SimpleRecord](specificColumn, specificRecords.head,
+        Generic(specificSchema))
+  }
+
+  test("A KijiJob can read a specific record column with a specific reader schema.") {
+    testExpressReadWrite[SimpleRecord](specificColumn, specificRecords.head,
+      Specific(classOf[SimpleRecord]))
+  }
+
+  test("A KijiJob can read a generic record column with the writer schema.") {
+    testExpressReadWrite[GenericRecord](genericColumn, genericRecords.head, Writer)
+  }
+
+  test("A KijiJob can read a generic record column with a generic reader schema.") {
+    testExpressReadWrite[GenericRecord](genericColumn, genericRecords.head, Generic(genericSchema))
+  }
+
+  test("A KijiJob can read an enum column with the writer schema.") {
+    testExpressReadWrite[GenericEnumSymbol](enumColumn, enums.head, Writer,
+        Some(Generic(enumSchema)))
+  }
+
+  test("A KijiJob can read an enum column with a generic reader schema.") {
+    testExpressReadWrite[String](enumColumn, enums.head, Generic(enumSchema))
+  }
+
+  test("A KijiJob can read an array column with the writer schema.") {
+    testExpressReadWrite[List[String]](arrayColumn, avroArrays.head, Writer,
+        Some(Generic(arraySchema)))
+  }
+
+  test("A KijiJob can read an array column with a generic reader schema.") {
+    testExpressReadWrite[List[String]](arrayColumn, avroArrays.head, Generic(arraySchema))
+  }
+
+  test("A KijiJob can read a union column with the writer schema [INT].") {
+    testExpressReadWrite[Any](unionColumn, ints.head, Writer, Some(Generic(unionSchema)))
+  }
+
+  test("A KijiJob can read a union column with the writer schema [STRING].") {
+    testExpressReadWrite[Any](unionColumn, strings.head, Writer, Some(Generic(unionSchema)))
+  }
+
+  test("A KijiJob can read a fixed column with the writer schema [INT].") {
+    testExpressReadWrite[GenericFixed](fixedColumn, fixeds.head, Writer,
+        Some(Generic(fixedSchema)))
+  }
+
+  test("A KijiJob can read a fixed column with a generic reader schema.") {
+    testExpressReadWrite[GenericFixed](fixedColumn, fixeds.head, Generic(fixedSchema))
+  }
+}
+
+// Must be its own top-level class for mystical serialization reasons
+class ReadWriteJob[T](
+    uri: String,
+    input: ColumnInputSpec,
+    output: ColumnOutputSpec,
+    writeEid: String,
+    args: Args
+) extends KijiJob(args) {
+
+  /**
+   * Unwraps the latest value from an iterable of cells and verifies that the type is as expected.
+   *
+   * @param slice containing the value to unwrap.
+   * @return unwrapped value of type T.
+   */
+  private def unwrap(slice: Seq[FlowCell[T]]): (T, Long) = {
+    require(slice.size == 1)
+    val cell = slice.head
+    (cell.datum, cell.version)
+  }
+
+  KijiInput(uri, Map(input -> 'slice))
+      .read
+      .mapTo('slice -> ('value, 'time))(unwrap)
+      .map('value -> 'entityId) { _: T => EntityId(writeEid)}
+      .write(KijiOutput(uri, 'time, Map('value -> output)))
+}
+
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/TimeRangeSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/TimeRangeSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..782b4f3d92ff8754f4ad334b46da33c72ba3088e
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/TimeRangeSuite.scala
@@ -0,0 +1,74 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.schema.KConstants
+
+@RunWith(classOf[JUnitRunner])
+class TimeRangeSuite extends FunSuite {
+  test("TimeRange fails fast when invalid time range arguments are specified.") {
+    val thrown: IllegalArgumentException = intercept[IllegalArgumentException] {
+      Between(10L, 1L)
+    }
+
+    val expectedMessage = "requirement failed: Invalid time range specified: (%d, %d)"
+        .format(10L, 1L)
+    assert(thrown.getMessage == expectedMessage)
+  }
+
+  test("All constructs a TimeRange correctly") {
+    val range: TimeRange = All
+
+    assert(range.begin == KConstants.BEGINNING_OF_TIME)
+    assert(range.end == KConstants.END_OF_TIME)
+  }
+
+  test("At constructs a TimeRange correctly") {
+    val range: TimeRange = At(42L)
+
+    assert(range.begin == 42L)
+    assert(range.end == 42L)
+  }
+
+  test("After constructs a TimeRange correctly") {
+    val range: TimeRange = After(42L)
+
+    assert(range.begin == 42L)
+    assert(range.end == KConstants.END_OF_TIME)
+  }
+
+  test("Before constructs a TimeRange correctly") {
+    val range: TimeRange = Before(42L)
+
+    assert(range.begin == KConstants.BEGINNING_OF_TIME)
+    assert(range.end == 42L)
+  }
+
+  test("Between constructs a TimeRange correctly") {
+    val range: TimeRange = Between(10L, 42L)
+
+    assert(range.begin == 10L)
+    assert(range.end == 42L)
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/TransientSeqSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/TransientSeqSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..05f9c4031bf01b2ccff9f503c4be71ede96aa8a6
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/TransientSeqSuite.scala
@@ -0,0 +1,383 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import java.util.concurrent.atomic.AtomicLong
+
+import scala.collection.mutable
+
+import org.kiji.express.KijiSuite
+
+class TransientSeqSuite extends KijiSuite {
+
+  test("Paging through a TransientSeq twice will generate two iterators.") {
+    var counter = 0
+    def genItr(): Iterator[Int] = {
+      counter += 1
+      List().iterator
+    }
+
+    val seq = new TransientSeq(genItr)
+    for (_ <- seq) {}
+    for (_ <- seq) {}
+    assert(counter === 2)
+  }
+
+  test("TransientSeq will not reuse iterators when doing transformations.") {
+    var counter = 0
+    def genItr(): Iterator[Int] =
+      new Iterator[Int] {
+        val itr = (0 until 5).iterator
+        def hasNext: Boolean = itr.hasNext
+
+        def next(): Int = {
+          counter += 1
+          itr.next()
+        }
+      }
+
+    val seq = new TransientSeq(genItr)
+    for (_ <- seq) {}
+    for (_ <- seq) {}
+    assert(counter === 10)
+  }
+
+  test("TransientSeq does not hold onto the results of an iterator.") {
+    // The only (sure) way to test this is to make sure an OOME doesn't occur
+    val max = 250 * 1000 * 1000 // 1 GB of Ints
+    def genItr(): Iterator[Int] = (1 to max).iterator
+
+    val seq = new TransientSeq(genItr)
+    // val seq = genItr().toStream // Control case.  Will crash
+
+    // Make sure there is a side effect when running through the iterator so it won't get JIT'd away
+    var counter: Int = 0
+    for (i <- seq) { counter = i }
+    assert(counter === max)
+  }
+
+  test("Chaining transformations on a TransientSeq does not require an iterator.") {
+    var counter = 0
+    def genItr(): Iterator[Int] = {
+      counter += 1
+      (1 to Int.MaxValue).iterator
+    }
+
+    val tseq = new TransientSeq(genItr)
+        .map(x => x + 1)
+        .map(x => x * x)
+        .take(10)
+        .drop(3)
+        .filter(x => x % 2 == 0)
+
+    assert(counter === 0)
+  }
+
+  // C(i, n) is a complexity measure of the number of iterators requested from the TransientSeq, i,
+  // and the number of elements requested from all iterators, n.  N is the number of elements held
+  // by the underlying iterators of the TransientSeq.
+
+  /** Good. */
+  test("force is C(1, 1)") { assertComplexity(1, 1, tseq => tseq.force) }
+
+  /** Good. */
+  test("head is C(1, 1)") { assertComplexity(1, 1, tseq => tseq.head) }
+
+  /** Poor. See below. */
+  test("tail is C(1, 0)") { assertComplexity(1, 0, tseq => tseq.tail) }
+
+  /** This is egregious. */
+  test("tail * m is C(m, 0 + 1 + ... + m - 1)") {
+    assertComplexity(4, 6, tseq => tseq.tail.tail.tail.tail)
+  }
+
+  /** And this is even worse. */
+  test("tail * m then force is C(m + 1, 0 + 1 + m - 1 + m + 1)") {
+    assertComplexity(5, 11, tseq => tseq.tail.tail.tail.tail.force)
+  }
+
+  /** Good. */
+  test("force then tail * m is C(1, m + 1)") {
+    assertComplexity(1, 5, tseq => tseq.force.tail.tail.tail.tail)
+  }
+
+  /** Good. */
+  test("take(m) is C(0, 0)") { assertComplexity(0, 0, tseq => tseq.take(42)) }
+
+  /** Good. */
+  test("take(m) then force is C(1, 1)") { assertComplexity(1, 1, tseq => tseq.take(42).force) }
+
+  /** Good. */
+  test("drop(m) is C(0, 0)") { assertComplexity(0, 0, tseq => tseq.drop(42)) }
+
+  /** Good. */
+  test("drop(m) then force is C(1, m + 1)") { assertComplexity(1, 43, tseq => tseq.drop(42).force) }
+
+  /** Good. */
+  test("dropWhile is C(0, 0)") {
+    assertComplexity(0, 0, tseq => tseq.dropWhile(x => x < 100))
+  }
+
+  /** Good. */
+  test("dropWhile then force is C(1, O(N))") {
+    assertComplexity(1, 100, tseq => tseq.dropWhile(x => x < 100).force)
+  }
+
+  /** Good. */
+  test("immutable additions are C(0, 0)") {
+    assertComplexity(0, 0, tseq => tseq ++ List(1, 2, 3))
+    assertComplexity(0, 0, tseq => tseq ++: List(1, 2, 3))
+    assertComplexity(0, 0, tseq => tseq.+:(1))
+    assertComplexity(0, 0, tseq => tseq.:+(1))
+  }
+
+  def sum(a: Int, b: Int): Int = a + b
+
+  /** Good. */
+  test("fold is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).foldLeft(0)(sum))
+    assertComplexity(1, 10, tseq => tseq.take(10).foldRight(0)(sum))
+    assertComplexity(1, 10, tseq => tseq.take(10)./:(0)(sum))
+    assertComplexity(1, 10, tseq => tseq.take(10).:\(0)(sum))
+    assertComplexity(1, 10, tseq => tseq.take(10).aggregate(0)(sum, sum))
+  }
+
+  /** Poor. Use fold over reduce where possible. */
+  test("reduce left is C(2, N)") {
+    assertComplexity(2, 10, tseq => tseq.take(10).reduce(sum))
+    assertComplexity(2, 10, tseq => tseq.take(10).reduceLeft(sum))
+  }
+
+  /** OK, but unsafe because of memory usage. */
+  test("reduce right is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).reduceRight(sum))
+  }
+
+  /** Poor. */
+  test("reduce option adds 1 iterator access") {
+    assertComplexity(3, 10, tseq => tseq.take(10).reduceLeftOption(sum))
+    assertComplexity(2, 10, tseq => tseq.take(10).reduceRightOption(sum))
+  }
+
+  /** Good. */
+  test("flatten is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).map(x => List(x)).flatten)
+  }
+
+  /** Good. */
+  test("map is C(0, 0)") {
+    assertComplexity(0, 0, tseq => tseq.map(x => x + 1))
+    assertComplexity(0, 0, tseq => tseq.flatMap(x => List(x, x)))
+    assertComplexity(0, 0, tseq => tseq.reverseMap(x => x + 1))
+  }
+
+  /** Good. */
+  test("map then force is C(1, 1)") {
+    assertComplexity(1, 1, tseq => tseq.map(x => x + 1).force)
+    assertComplexity(1, 1, tseq => tseq.flatMap(x => List(x, x)).force)
+  }
+
+  /** OK, but unsafe because of memory usage. */
+  test("reverseMap then force is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).reverseMap(x => x + 1).force)
+  }
+
+  /** Good. */
+  test("map then toList is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).map(x => x + 1).toList)
+    assertComplexity(1, 10, tseq => tseq.take(10).flatMap(x => List(x, x)).toList)
+    assertComplexity(1, 10, tseq => tseq.take(10).reverseMap(x => x + 1).toList)
+  }
+
+  /** Good. */
+  test("filter is C(0, 0)") {
+    assertComplexity(0, 0, tseq => tseq.filter(x => x % 2 == 0))
+    assertComplexity(0, 0, tseq => tseq.filterNot(x => x % 2 == 0))
+    assertComplexity(0, 0, tseq => tseq.collect { case x if x % 2 == 0 => x })
+  }
+
+  /** Good. */
+  test("filter then force is C(1, x) where x is the index of the first non-filtered element") {
+    assertComplexity(1, 2, tseq => tseq.filter(x => x % 2 == 0).force)
+    assertComplexity(1, 1, tseq => tseq.filterNot(x => x % 2 == 0).force)
+  }
+
+  /** Good. */
+  test("filter then toList is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).filter(x => x % 2 == 0).toList)
+    assertComplexity(1, 10, tseq => tseq.take(10).filterNot(x => x % 2 == 0).toList)
+  }
+
+  /** Good. */
+  test("membership ops are C(1, O(N))") {
+    assertComplexity(1, 10, tseq => tseq.collectFirst { case x if x == 10 => x })
+    assertComplexity(1, 10, tseq => tseq.contains(10))
+    assertComplexity(1, 10, tseq => tseq.indexOf(10))
+    assertComplexity(1, 10, tseq => tseq.indexWhere(_ == 10))
+    assertComplexity(1, 10, tseq => tseq.indexWhere(_ == 10, 3))
+    assertComplexity(1, 10, tseq => tseq.exists(_ == 10))
+    assertComplexity(1, 10, tseq => tseq.find(_ == 10))
+  }
+
+  /** Good. */
+  test("size is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).size)
+    assertComplexity(1, 10, tseq => tseq.take(10).length)
+  }
+
+  /** Good. */
+  test("copy ops are C(1, L) where L is the number of elements to copy") {
+    assertComplexity(1, 10, tseq => tseq.copyToArray(Array.ofDim[Int](10)))
+    assertComplexity(1, 10, tseq => tseq.copyToArray(Array.ofDim[Int](10), 0))
+    assertComplexity(1, 10, tseq => tseq.copyToArray(Array.ofDim[Int](10), 0, 10))
+    assertComplexity(1, 10, tseq => tseq.take(10).copyToBuffer(mutable.Buffer[Int]()))
+  }
+
+  /** Poor. */
+  test("force then updated is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).updated(2, 99))
+  }
+
+  /** Good. */
+  test("corresponds is C(1, O(Min(N, L))) where L is the length of the other Seq") {
+    assertComplexity(1, 5, tseq => tseq.corresponds(1 to 5)(_ equals _))
+  }
+
+  /** Poor. Not lazy, will not complete without the take. */
+  test("combinations is C(2, 2 * N)") {
+    assertComplexity(2, 20, tseq => tseq.take(10).combinations(3))
+  }
+
+  /** Poor. Not lazy, will not complete without the take. */
+  test("permutations is C(2, N)") {
+    assertComplexity(2, 10, tseq => tseq.take(10).permutations)
+  }
+
+  /** OK, but unsafe because of memory usage. */
+  test("groupBy is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).groupBy(x => x % 4 == 0))
+  }
+
+  /** Good. */
+  test("grouped is C(1, 0)") {
+    assertComplexity(1, 0, tseq => tseq.grouped(2))
+  }
+
+  /** Good. */
+  test("zip is C(0, 0)") {
+    assertComplexity(0, 0, tseq => tseq.zip(1 to 100))
+    assertComplexity(0, 0, tseq => tseq.zipAll(1 to 100, 99, 99))
+    assertComplexity(0, 0, tseq => tseq.zipWithIndex)
+  }
+
+  /** Poor. Not lazy, will not complete without the take. */
+  test("unzip is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).zipWithIndex.unzip)
+  }
+
+  /** OK, but unsafe because of memory usage. */
+  test("sorted is C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).sorted)
+  }
+
+  /** OK, but unsafe because of memory usage. */
+  test("set operations are C(1, N)") {
+    assertComplexity(1, 10, tseq => tseq.take(10).union(List(1, 2, 3, 99)))
+    assertComplexity(1, 10, tseq => tseq.take(10).intersect(List(1, 2, 3, 99)))
+    assertComplexity(1, 10, tseq => tseq.take(10).diff(List(1, 2, 3, 99)))
+  }
+
+  /** Good. */
+  test("patch is C(0, 0)") {
+    assertComplexity(0, 0, tseq => tseq.take(12).patch(0, List(10), 10))
+  }
+
+  /** Good. */
+  test("startsWith is C(1, _)") {
+    assertComplexity(1, 1, tseq => tseq.startsWith(List(10)))
+    assertComplexity(1, 4, tseq => tseq.startsWith(List(10), 3))
+  }
+
+  /** Poor. */
+  test("endsWith is C(2, 2 * N)") {
+    assertComplexity(2, 20, tseq => tseq.take(10).endsWith(List(10)))
+  }
+
+  /** Extremely poor. */
+  test("some operations are unsupported.") {
+    intercept[UnsupportedOperationException] {
+      assertComplexity(1, 10, tseq => tseq.take(12).containsSlice(List(9, 10)))
+    }
+    intercept[UnsupportedOperationException] {
+      assertComplexity(1, 10, tseq => tseq.take(10).distinct)
+    }
+    intercept[UnsupportedOperationException] {
+      assertComplexity(1, 10, tseq => tseq.take(12).indexOfSlice(List(10)))
+    }
+    intercept[UnsupportedOperationException] {
+      assertComplexity(1, 10, tseq => tseq.take(10).takeRight(2))
+    }
+    intercept[UnsupportedOperationException] {
+      assertComplexity(1, 100, tseq => tseq.take(10).dropRight(2))
+    }
+    intercept[UnsupportedOperationException] {
+      assertComplexity(1, 10, tseq => tseq.take(10).lastIndexOfSlice(List(10)))
+    }
+    intercept[UnsupportedOperationException] {
+      assertComplexity(1, 10, tseq => tseq.take(12).lastIndexOfSlice(List(10), 10))
+    }
+  }
+
+  test("unsupported operations are supported after force.") {
+    assertComplexity(1, 10, tseq => tseq.force.take(12).containsSlice(List(9, 10)))
+    assertComplexity(1, 10, tseq => tseq.force.take(10).distinct.toList)
+    assertComplexity(1, 10, tseq => tseq.force.take(12).indexOfSlice(List(10)))
+    assertComplexity(1, 10, tseq => tseq.force.take(10).takeRight(2))
+    assertComplexity(1, 10, tseq => tseq.force.take(10).dropRight(2))
+    assertComplexity(1, 10, tseq => tseq.force.take(10).lastIndexOfSlice(List(10)))
+    assertComplexity(1, 10, tseq => tseq.force.take(10).lastIndexOfSlice(List(10), 10))
+  }
+
+  def countingItr(): (AtomicLong, AtomicLong, () => Iterator[Int]) = {
+    val i = new AtomicLong() // number of iterators requested.
+    val n = new AtomicLong() // number of elements requested across all iterators.
+    def genItr(): Iterator[Int] = {
+      i.incrementAndGet()
+      new Iterator[Int] {
+        val itr = (1 to Int.MaxValue).iterator
+        def hasNext: Boolean = itr.hasNext
+        def next(): Int = { n.incrementAndGet() ; itr.next() }
+      }
+    }
+    (i, n , genItr)
+  }
+
+  def countingTSeq(): (AtomicLong, AtomicLong, TransientSeq[Int]) = {
+    val (i, n, genItr) = countingItr()
+    (i, n, new TransientSeq(genItr))
+  }
+
+  def assertComplexity(i: Long, n: Long, op: TransientSeq[Int] => Unit): Unit = {
+    val (iActual, nActual, tseq) = countingTSeq()
+    op(tseq)
+    assert(i === iActual.get, "iterator count")
+    assert(n === nActual.get, "element count")
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/WriterSchemaSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/WriterSchemaSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..1f9b312c144217a169be1af78939412851906b90
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/WriterSchemaSuite.scala
@@ -0,0 +1,312 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow
+
+import scala.collection.JavaConversions
+
+import cascading.tuple.Fields
+import com.twitter.scalding.Args
+import com.twitter.scalding.IterableSource
+import com.twitter.scalding.Mode
+import com.twitter.scalding.TupleConverter
+import com.twitter.scalding.TupleSetter
+import org.apache.avro.Schema
+import org.apache.avro.generic.GenericData
+import org.apache.avro.generic.GenericData.Fixed
+import org.apache.hadoop.conf.Configuration
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.flow.SchemaSpec.Writer
+import org.kiji.express.flow.util.AvroTypesComplete
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiClientTest
+import org.kiji.schema.KijiColumnName
+import org.kiji.schema.KijiDataRequest
+import org.kiji.schema.KijiTable
+import org.kiji.schema.KijiTableReader
+import org.kiji.schema.KijiTableWriter
+import org.kiji.schema.{EntityId => SchemaEntityId}
+
+
+@RunWith(classOf[JUnitRunner])
+class WriterSchemaSuite extends KijiClientTest with KijiSuite {
+  import WriterSchemaSuite._
+  import AvroTypesComplete._
+
+  // TODO: These non-test things can be moved to the companion object after SCHEMA-539 fix
+  setupKijiTest()
+  val kiji: Kiji = createTestKiji()
+  val table: KijiTable = {
+    kiji.createTable(AvroTypesComplete.layout.getDesc)
+    kiji.openTable(AvroTypesComplete.layout.getName)
+  }
+  val conf: Configuration = getConf
+  val uri = table.getURI.toString
+  val reader: KijiTableReader = table.openTableReader()
+  val writer: KijiTableWriter = table.openTableWriter()
+
+  /**
+   * Get value from HBase.
+   * @param eid string of row
+   * @param column column containing requested value
+   * @tparam T expected type of value
+   * @return the value
+   */
+  def getValue[T](eid: String, column: KijiColumnName): T = {
+    def entityId(s: String): SchemaEntityId = table.getEntityId(s)
+    val (family, qualifier) = column.getFamily -> column.getQualifier
+    val get = reader.get(entityId(eid), KijiDataRequest.create(family, qualifier))
+    require(get.containsColumn(family, qualifier)) // Require the cell exists for null case
+    get.getMostRecentValue(family, qualifier)
+  }
+
+  /**
+   * Verify that the inputs have been persisted into the Kiji column.  Checks that the types and
+   * values match.
+   * @param inputs to check against
+   * @param column column that the inputs are stored in
+   * @tparam T expected return type of value in HBase
+   */
+  def verify[T](inputs: Iterable[(EntityId, T)],
+                column: KijiColumnName,
+                verifier: (T, T) => Unit): Unit = {
+    inputs.foreach { input: (EntityId, T) =>
+      val (eid, value) = input
+      val retrieved: T = getValue(eid.components.head.toString, column)
+      verifier(value, retrieved)
+    }
+  }
+
+  def valueVerifier[T](input: T, retrieved: T): Unit = {
+    assert(input === retrieved)
+  }
+
+  def nullVerifier(input: Any, retrieved: Any): Unit = {
+    assert(input === null)
+    assert(retrieved === null)
+  }
+
+  def arrayVerifier[T](input: T, retrieved: T): Unit = {
+    assert(retrieved.isInstanceOf[GenericData.Array[_]])
+    val ret = JavaConversions.JListWrapper(retrieved.asInstanceOf[GenericData.Array[_]]).toSeq
+    assert(input.asInstanceOf[Iterable[_]].toSeq === ret)
+  }
+
+  def fixedVerifier[T](input: T, retrieved: T): Unit = {
+    assert(retrieved.isInstanceOf[Fixed])
+    assert(input === retrieved.asInstanceOf[Fixed].bytes())
+  }
+
+  def enumVerifier[T](schema: Schema)(input: T, retrieved: T): Unit = {
+    assert(retrieved.isInstanceOf[GenericData.EnumSymbol])
+    assert(
+      retrieved.asInstanceOf[GenericData.EnumSymbol] ===
+        new GenericData().createEnum(input.toString, schema))
+  }
+
+  /**                                                                         bfffff
+   * Write provided values with express into an HBase column with options as specified in output,
+   * and verify that the values have been persisted correctly.
+   * @param values to test
+   * @param output options to write with
+   * @tparam T type of values to write
+   * @return
+   */
+  def testWrite[T](values: Iterable[T],
+                   output: ColumnOutputSpec,
+                   verifier: (T, T) => Unit =  valueVerifier _) {
+    val outputSource = KijiOutput(uri, Map('value -> output))
+    val inputs = eids.zip(values)
+    expressWrite(conf, new Fields("entityId", "value"), inputs, outputSource)
+    verify(inputs, output.columnName, verifier)
+  }
+
+  test("A KijiJob can write to a counter column with a Writer schema spec.") {
+    testWrite(longs, QualifiedColumnOutputSpec(family, counterColumn, Writer))
+  }
+
+  test("A KijiJob can write to a raw bytes column with a Writer schema spec.")    {
+    testWrite(bytes, QualifiedColumnOutputSpec(family, rawColumn, Writer))
+  }
+
+  test("A KijiJob can write to an Avro null column with a Generic schema spec.") {
+    testWrite(nulls, QualifiedColumnOutputSpec(family, nullColumn, nullSchema), nullVerifier)
+  }
+
+  test("A KijiJob can write to an Avro null column with a Writer schema spec.") {
+    testWrite(nulls, QualifiedColumnOutputSpec(family, nullColumn), nullVerifier)
+  }
+
+  test("A KijiJob can write to an Avro boolean column with a Generic schema spec.") {
+    testWrite(booleans, QualifiedColumnOutputSpec(family, booleanColumn, booleanSchema))
+  }
+
+  test("A KijiJob can write to an Avro boolean column with a Writer schema spec.") {
+    testWrite(booleans, QualifiedColumnOutputSpec(family, booleanColumn, Writer))
+  }
+
+  test("A KijiJob can write to an Avro int column with a Generic schema spec.") {
+    testWrite(ints, QualifiedColumnOutputSpec(family, intColumn, intSchema))
+  }
+
+  test("A KijiJob can write to an Avro int column with a Writer schema spec.") {
+    testWrite(ints, QualifiedColumnOutputSpec(family, intColumn, Writer))
+  }
+
+  test("A KijiJob can write to an Avro long column with a Generic schema spec.") {
+    testWrite(longs, QualifiedColumnOutputSpec(family, longColumn, longSchema))
+  }
+
+  test("A KijiJob can write to an Avro long column with a Writer schema spec.") {
+    testWrite(longs, QualifiedColumnOutputSpec(family, longColumn, Writer))
+  }
+
+  test("A KijiJob can write ints to an Avro long column with an int schema.") {
+    testWrite(ints, QualifiedColumnOutputSpec(family, longColumn, intSchema))
+  }
+
+  test("A KijiJob can write to an Avro float column with a Generic schema spec.") {
+    testWrite(floats, QualifiedColumnOutputSpec(family, floatColumn, floatSchema))
+  }
+
+  test("A KijiJob can write to an Avro float column with a Writer schema spec.") {
+    testWrite(floats, QualifiedColumnOutputSpec(family, floatColumn, Writer))
+  }
+
+  test("A KijiJob can write to an Avro double column with a Generic schema spec.") {
+    testWrite(doubles, QualifiedColumnOutputSpec(family, doubleColumn, doubleSchema))
+  }
+
+  test("A KijiJob can write to an Avro double column with a Writer schema spec.") {
+    testWrite(doubles, QualifiedColumnOutputSpec(family, doubleColumn, Writer))
+  }
+
+  test("A KijiJob can write floats to an Avro double column with a float schema.") {
+    testWrite(floats, QualifiedColumnOutputSpec(family, doubleColumn, intSchema))
+  }
+
+  /** TODO: reenable when Schema-594 is fixed. */
+  ignore("A KijiJob can write to an Avro bytes column with a Generic schema spec.") {
+    testWrite(bytes, QualifiedColumnOutputSpec(family, bytesColumn, bytesSchema))
+  }
+
+  /** TODO: reenable when Schema-594 is fixed. */
+  ignore("A KijiJob can write to an Avro bytes column with a Writer schema spec.") {
+    testWrite(bytes, QualifiedColumnOutputSpec(family, bytesColumn, Writer))
+  }
+
+  test("A KijiJob can write to an Avro string column with a Generic schema spec.") {
+    testWrite(strings, QualifiedColumnOutputSpec(family, stringColumn, stringSchema))
+  }
+
+  test("A KijiJob can write to an Avro string column with a Writer schema spec.") {
+    testWrite(strings, QualifiedColumnOutputSpec(family, stringColumn, Writer))
+  }
+
+  test("A KijiJob can write to an Avro specific record column with a Generic schema spec.") {
+    testWrite(specificRecords, QualifiedColumnOutputSpec(family, specificColumn, specificSchema))
+  }
+
+  test("A KijiJob can write to an Avro specific record column with a Writer schema spec.") {
+    testWrite(specificRecords, QualifiedColumnOutputSpec(family, specificColumn, Writer))
+  }
+
+  test("A KijiJob can write to a generic record column with a Generic schema spec.") {
+    testWrite(genericRecords, QualifiedColumnOutputSpec(family, genericColumn, genericSchema))
+  }
+
+  test("A KijiJob can write to a generic record column with a Writer schema spec.") {
+    testWrite(genericRecords, QualifiedColumnOutputSpec(family, genericColumn, Writer))
+  }
+
+  test("A KijiJob can write to an enum column with a Generic schema spec.") {
+    testWrite(enums, QualifiedColumnOutputSpec(family, enumColumn, enumSchema))
+  }
+
+  test("A KijiJob can write to an enum column with a Writer schema spec.") {
+    testWrite(enums, QualifiedColumnOutputSpec(family, enumColumn, Writer))
+  }
+
+  test("A KijiJob can write a string to an enum column with a Generic schema spec.") {
+    testWrite(enumStrings, QualifiedColumnOutputSpec(family, enumColumn, enumSchema),
+      enumVerifier(enumSchema))
+  }
+
+  test("A KijiJob can write an avro array to an array column with a Generic schema spec.") {
+    testWrite(avroArrays, QualifiedColumnOutputSpec(family, arrayColumn, arraySchema))
+  }
+
+  test("A KijiJob can write an avro array to an array column with a Writer schema spec."){
+    testWrite(avroArrays, QualifiedColumnOutputSpec(family, arrayColumn, Writer))
+  }
+
+  test("A KijiJob can write an Iterable to an array column with a Generic schema spec.") {
+    testWrite(arrays, QualifiedColumnOutputSpec(family, arrayColumn, arraySchema), arrayVerifier)
+  }
+
+  test("A KijiJob can write to a union column with a Generic schema spec.") {
+    testWrite(unions, QualifiedColumnOutputSpec(family, unionColumn, unionSchema))
+  }
+
+  test("A KijiJob can write to a union column with a Writer schema spec.") {
+    testWrite(unions, QualifiedColumnOutputSpec(family, unionColumn, Writer))
+  }
+
+  test("A KijiJob can write to a fixed column with a Generic schema spec.") {
+    testWrite(fixeds, QualifiedColumnOutputSpec(family, fixedColumn, fixedSchema))
+  }
+
+  test("A KijiJob can write to a fixed column with a Writer schema spec.") {
+    testWrite(fixeds, QualifiedColumnOutputSpec(family, fixedColumn, Writer))
+  }
+
+  test("A KijiJob can write a byte array to a fixed column with a Generic schema spec.") {
+    testWrite(fixedByteArrays, QualifiedColumnOutputSpec(family, fixedColumn, fixedSchema),
+      fixedVerifier)
+  }
+}
+
+object WriterSchemaSuite {
+  /**
+   * Writes inputs to outputSource with Express.
+   * @param fs fields contained in the input tuples.
+   * @param inputs contains tuples to write to HBase with Express.
+   * @param outputSource KijiSource with options for how to write values to HBase.
+   * @param setter necessary for some implicit shenanigans.  Don't explicitly pass in.
+   * @tparam A type of values to be written.
+   */
+  def expressWrite[A](conf: Configuration,
+                      fs: Fields,
+                      inputs: Iterable[A],
+                      outputSource: KijiSource)
+                     (implicit setter: TupleSetter[A]): Boolean = {
+    val args = Args("--hdfs")
+    Mode.mode = Mode(args, conf) // HDFS mode
+    new IdentityJob(fs, inputs, outputSource, args).run
+  }
+}
+
+// Must be its own top-level class for mystical serialization reasons
+class IdentityJob[A](fs: Fields, inputs: Iterable[A], output: KijiSource, args: Args)
+                    (implicit setter: TupleSetter[A]) extends KijiJob(args) {
+  IterableSource(inputs, fs)(setter, implicitly[TupleConverter[A]]).write(output)
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/framework/AvroSerializerSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/framework/AvroSerializerSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..073cef7891b8d6df02f7cefd589b62602e20e52d
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/framework/AvroSerializerSuite.scala
@@ -0,0 +1,133 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import scala.collection.JavaConverters.seqAsJavaListConverter
+
+import cascading.kryo.KryoFactory
+import com.esotericsoftware.kryo.Kryo
+import com.esotericsoftware.kryo.io.Input
+import com.esotericsoftware.kryo.io.Output
+import org.apache.avro.Schema
+import org.apache.avro.generic.GenericContainer
+import org.apache.avro.generic.GenericRecordBuilder
+import org.apache.avro.specific.SpecificRecord
+import org.apache.hadoop.hbase.HBaseConfiguration
+import org.scalatest.FunSuite
+
+import org.kiji.express.avro.SimpleRecord
+import org.kiji.express.flow.framework.serialization.AvroSpecificSerializer
+import org.kiji.express.flow.framework.serialization.AvroGenericSerializer
+import org.kiji.express.flow.framework.serialization.AvroSchemaSerializer
+
+class AvroSerializerSuite
+    extends FunSuite {
+  def serDeTest[I](inputName: String, serdeName: String, input: => I)(operation: I => I) {
+    test("Serialization/Deserialization of a %s using %s".format(inputName, serdeName)) {
+      val expected: I = input
+      val actual: I = operation(expected)
+
+      assert(expected === actual)
+    }
+  }
+
+  val recordSchema: Schema = {
+    val fields = Seq(
+        new Schema.Field("field1", Schema.create(Schema.Type.INT), "First test field.", null),
+        new Schema.Field("field2", Schema.create(Schema.Type.STRING), "First test field.", null),
+        new Schema.Field("field3", Schema.create(Schema.Type.FLOAT), "First test field.", null))
+
+    val record = Schema.createRecord("TestRecord", "", "", false)
+    record.setFields(fields.asJava)
+
+    record
+  }
+
+  val genericRecord: GenericContainer = {
+    new GenericRecordBuilder(recordSchema)
+        .set("field1", 42)
+        .set("field2", "foo")
+        .set("field3", 3.14f)
+        .build()
+  }
+
+  val specificRecord: SpecificRecord = {
+    SimpleRecord
+        .newBuilder()
+        .setL(42L)
+        .setO("foo")
+        .setS("bar")
+        .build()
+  }
+
+  serDeTest("Schema", "Avro", recordSchema) { actual =>
+    // Use cascading.kryo to mimic scalding's actual behavior.
+    val kryo = new Kryo()
+    val kryoFactory = new KryoFactory(HBaseConfiguration.create())
+    val registrations = Seq(
+        new KryoFactory.ClassPair(classOf[Schema], classOf[AvroSchemaSerializer]))
+    kryoFactory.setHierarchyRegistrations(registrations.asJava)
+    kryoFactory.populateKryo(kryo)
+
+    // Serialize the schema.
+    val output = new Output(1024)
+    kryo.writeObject(output, actual)
+
+    // Deserialize the schema.
+    val input = new Input(output.getBuffer)
+    kryo.readObject(input, classOf[Schema])
+  }
+
+  serDeTest("GenericRecord", "Avro", genericRecord) { actual =>
+    // Use cascading.kryo to mimic scalding's actual behavior.
+    val kryo = new Kryo()
+    val kryoFactory = new KryoFactory(HBaseConfiguration.create())
+    val registrations = Seq(
+        new KryoFactory.ClassPair(classOf[GenericContainer], classOf[AvroGenericSerializer]))
+    kryoFactory.setHierarchyRegistrations(registrations.asJava)
+    kryoFactory.populateKryo(kryo)
+
+    // Serialize the schema.
+    val output = new Output(1024)
+    kryo.writeObject(output, actual)
+
+    // Deserialize the schema.
+    val input = new Input(output.getBuffer)
+    kryo.readObject(input, classOf[GenericContainer])
+  }
+
+  serDeTest("SpecificRecord", "Avro", specificRecord) { actual =>
+    // Use cascading.kryo to mimic scalding's actual behavior.
+    val kryo = new Kryo()
+    val kryoFactory = new KryoFactory(HBaseConfiguration.create())
+    val registrations = Seq(
+        new KryoFactory.ClassPair(classOf[SpecificRecord], classOf[AvroSpecificSerializer]))
+    kryoFactory.setHierarchyRegistrations(registrations.asJava)
+    kryoFactory.populateKryo(kryo)
+
+    // Serialize the schema.
+    val output = new Output(1024)
+    kryo.writeObject(output, actual)
+
+    // Deserialize the schema.
+    val input = new Input(output.getBuffer)
+    kryo.readObject(input, classOf[SimpleRecord])
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiKeySuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiKeySuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..676d3c5829f8fb371dccaaf6bb65ee42f8e05c19
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiKeySuite.scala
@@ -0,0 +1,41 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.schema.EntityIdFactory
+import org.kiji.schema.avro.RowKeyEncoding
+import org.kiji.schema.avro.RowKeyFormat
+
+@RunWith(classOf[JUnitRunner])
+class KijiKeySuite extends FunSuite {
+  test("KijiKey should get the same EntityId you put in.") {
+    val entityIdFactory = EntityIdFactory.getFactory(
+      RowKeyFormat.newBuilder().setEncoding(RowKeyEncoding.RAW).build())
+    val testId = entityIdFactory.getEntityId("foob")
+    val testKey = new KijiKey()
+    testKey.set(testId)
+
+    assert(testId == testKey.get())
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiSchemeSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiSchemeSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..b04ea8531bc89ee41bb33e6c1f6fbcc1e1e51ac9
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiSchemeSuite.scala
@@ -0,0 +1,102 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import cascading.tuple.Tuple
+import cascading.tuple.TupleEntry
+import org.apache.avro.generic.GenericRecord
+import org.apache.avro.generic.GenericRecordBuilder
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.hbase.HBaseConfiguration
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.flow.All
+import org.kiji.express.flow.FlowCell
+import org.kiji.express.flow.ColumnInputSpec
+import org.kiji.express.flow.EntityId
+import org.kiji.express.flow.QualifiedColumnOutputSpec
+import org.kiji.express.flow.util.GenericCellSpecs
+import org.kiji.schema.EntityIdFactory
+import org.kiji.schema.avro.HashSpec
+import org.kiji.schema.avro.HashType
+
+@RunWith(classOf[JUnitRunner])
+class KijiSchemeSuite extends KijiSuite {
+  test("putTuple and rowToTuple can write and read a generic AvroRecord.") {
+    // Set up the table.
+    val configuration: Configuration = HBaseConfiguration.create()
+    val tableLayout = layout("layout/avro-types.json")
+    val table = makeTestKijiTable(tableLayout)
+    val kiji = table.getKiji
+    val uri = table.getURI
+    val writer = table.openTableWriter()
+    val reader = table.getReaderFactory.openTableReader(GenericCellSpecs(table))
+
+    // Set up the columns and fields.
+    val columnsOutput = Map("columnSymbol" -> QualifiedColumnOutputSpec("family:column3"))
+    val columnsInput = Map("columnSymbol" -> ColumnInputSpec("family:column3"))
+    val sourceFields = KijiScheme.buildSourceFields(columnsOutput.keys)
+
+    // Create a dummy record with an entity ID to put in the table.
+    val dummyEid = EntityId("dummy")
+    val record: GenericRecord = {
+      val builder = new GenericRecordBuilder(HashSpec.getClassSchema)
+      builder.set("hash_type", HashType.MD5)
+      builder.set("hash_size", 13)
+      builder.set("suppress_key_materialization", false)
+      builder.build()
+    }
+    val writeValue = new TupleEntry(sourceFields, new Tuple(dummyEid, record))
+
+    val eidFactory = EntityIdFactory.getFactory(tableLayout)
+
+    // Put the tuple.
+    KijiScheme.putTuple(
+        columnsOutput,
+        uri,
+        kiji,
+        None,
+        writeValue,
+        writer,
+        tableLayout,
+        configuration)
+
+    // Read the tuple back.
+    val rowData = reader.get(
+        dummyEid.toJavaEntityId(eidFactory),
+        KijiScheme.buildRequest(All, columnsInput.values))
+    val readValue: Tuple = KijiScheme.rowToTuple(
+        columnsInput,
+        sourceFields,
+        None,
+        rowData,
+        uri,
+        configuration)
+
+    val readRecord = readValue.getObject(1).asInstanceOf[Seq[FlowCell[_]]].head.datum
+    assert(record === readRecord)
+
+    reader.close()
+    writer.close()
+    table.release()
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiTableSplitSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiTableSplitSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..32ff0f4f3e76b617bf9bcba29bb34055f6ccdf87
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiTableSplitSuite.scala
@@ -0,0 +1,44 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import org.apache.hadoop.hbase.mapreduce.TableSplit
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+
+@RunWith(classOf[JUnitRunner])
+class KijiTableSplitSuite extends FunSuite {
+  val hTableSplit =
+      new TableSplit("name".getBytes, "startrow".getBytes, "endrow".getBytes, "location")
+  val kTableSplit = new KijiTableSplit(hTableSplit)
+
+  test("KijiTableSplit should have the same startrow as the hTableSplit.") {
+    assert(hTableSplit.getStartRow.toSeq == kTableSplit.getStartRow.toSeq)
+  }
+
+  test("KijiTableSplit should have the same endrow as the hTableSplit.") {
+    assert(hTableSplit.getEndRow.toSeq ==  kTableSplit.getEndRow.toSeq)
+  }
+
+  test("KijiTableSplit should have the same locations as the hTableSplit.") {
+    assert(hTableSplit.getLocations.toSeq == kTableSplit.getLocations.toSeq)
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiTapSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiTapSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..ad819ed0d7aadc346477bd284ae54a9e7908db26
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiTapSuite.scala
@@ -0,0 +1,140 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import org.apache.hadoop.hbase.HBaseConfiguration
+import org.apache.hadoop.mapred.JobConf
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.flow.All
+import org.kiji.express.flow.ColumnInputSpec
+import org.kiji.express.flow.InvalidKijiTapException
+import org.kiji.schema.KijiURI
+import org.kiji.schema.layout.KijiTableLayout
+
+@RunWith(classOf[JUnitRunner])
+class KijiTapSuite extends KijiSuite {
+  val instanceName: String = "test_KijiTap_instance"
+  val testKijiTableLayout: KijiTableLayout = layout("layout/avro-types.json")
+  val config: JobConf = new JobConf(HBaseConfiguration.create())
+
+  test("KijiTap validates a valid instance/table/column.") {
+    val testTable = makeTestKijiTable(testKijiTableLayout, instanceName)
+    val kijiURI = testTable.getURI
+
+    val testScheme: KijiScheme = new KijiScheme(
+        timeRange = All,
+        timestampField = None,
+        inputColumns = Map(
+            "dummy_field1" -> ColumnInputSpec("searches"),
+            "dummy_field2" -> ColumnInputSpec("family:column1")))
+
+    val testTap: KijiTap = new KijiTap(kijiURI, testScheme)
+
+    testTap.validate(config)
+  }
+
+  test("KijiTap validates a nonexistent instance.") {
+    val testTable = makeTestKijiTable(testKijiTableLayout, instanceName)
+    val kijiURI = testTable.getURI
+
+    val testScheme: KijiScheme = new KijiScheme(
+        timeRange = All,
+        timestampField = None,
+        inputColumns = Map(
+            "dummy_field1" -> ColumnInputSpec("searches"),
+            "dummy_field2" -> ColumnInputSpec("family:column1")))
+
+    val testURI: KijiURI = KijiURI.newBuilder(kijiURI)
+        .withInstanceName("nonexistent_instance")
+        .build()
+
+    val testTap: KijiTap = new KijiTap(testURI, testScheme)
+
+    intercept[InvalidKijiTapException] {
+      testTap.validate(config)
+    }
+  }
+
+  test("KijiTap validates a nonexistent table.") {
+    val testTable = makeTestKijiTable(testKijiTableLayout, instanceName)
+    val kijiURI = testTable.getURI
+
+    val testScheme: KijiScheme = new KijiScheme(
+        timeRange = All,
+        timestampField = None,
+        inputColumns = Map(
+            "dummy_field1" -> ColumnInputSpec("searches"),
+            "dummy_field2" -> ColumnInputSpec("family:column1")))
+
+    val testURI: KijiURI = KijiURI.newBuilder(kijiURI)
+        .withTableName("nonexistent_table")
+        .build()
+
+    val testTap: KijiTap = new KijiTap(testURI, testScheme)
+
+    intercept[InvalidKijiTapException] {
+      testTap.validate(config)
+    }
+  }
+
+  test("KijiTap validates a nonexistent column.") {
+    val testTable = makeTestKijiTable(testKijiTableLayout, instanceName)
+    val kijiURI = testTable.getURI
+
+    val testScheme: KijiScheme = new KijiScheme(
+        timeRange = All,
+        timestampField = None,
+        inputColumns = Map(
+            "dummy_field1" -> ColumnInputSpec("searches"),
+            "dummy_field2" -> ColumnInputSpec("family:nonexistent")))
+
+    val testTap: KijiTap = new KijiTap(kijiURI, testScheme)
+
+    val exception = intercept[InvalidKijiTapException] {
+      testTap.validate(config)
+    }
+
+    assert(exception.getMessage.contains("nonexistent"))
+  }
+
+  test("KijiTap validates multiple nonexistent columns.") {
+    val testTable = makeTestKijiTable(testKijiTableLayout, instanceName)
+    val kijiURI = testTable.getURI
+
+    val testScheme: KijiScheme = new KijiScheme(
+        timeRange = All,
+        timestampField = None,
+        inputColumns = Map(
+            "dummy_field1" -> ColumnInputSpec("nonexistent1"),
+            "dummy_field2" -> ColumnInputSpec("family:nonexistent2")))
+
+    val testTap: KijiTap = new KijiTap(kijiURI, testScheme)
+
+    val exception = intercept[InvalidKijiTapException] {
+      testTap.validate(config)
+    }
+
+    assert(exception.getMessage.contains("nonexistent1"))
+    assert(exception.getMessage.contains("nonexistent2"))
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiValueSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiValueSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..662653ecf9f3e8ad3dae952a4af6a9459f74bcde
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/framework/KijiValueSuite.scala
@@ -0,0 +1,39 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework
+
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+import org.scalatest.mock.EasyMockSugar
+
+import org.kiji.schema.KijiRowData
+
+@RunWith(classOf[JUnitRunner])
+class KijiValueSuite extends FunSuite with EasyMockSugar {
+  test("KijiValue should get the same RowData you put in.") {
+    val value = new KijiValue()
+    val row = mock[KijiRowData]
+
+    value.set(row)
+
+    assert(row == value.get())
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/framework/hfile/HFileKijiJobIntegration.scala b/kiji-express/src/test/scala/org/kiji/express/flow/framework/hfile/HFileKijiJobIntegration.scala
new file mode 100644
index 0000000000000000000000000000000000000000..8e63ac86670b860a9aedb614923399989b6121c3
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/framework/hfile/HFileKijiJobIntegration.scala
@@ -0,0 +1,291 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.hfile
+
+import java.io.File
+
+import com.twitter.scalding.Args
+import com.twitter.scalding.TextLine
+import com.twitter.scalding.Tool
+import com.twitter.scalding.Tsv
+import org.apache.commons.io.FileUtils
+import org.apache.hadoop.fs.Path
+import org.junit.After
+import org.junit.Assert
+import org.junit.Before
+import org.junit.Test
+
+import org.kiji.express.flow.ColumnFamilyOutputSpec
+import org.kiji.express.flow.EntityId
+import org.kiji.express.flow.util.Resources
+import org.kiji.mapreduce.HFileLoader
+import org.kiji.schema.Kiji
+import org.kiji.schema.KijiDataRequest
+import org.kiji.schema.KijiDataRequestBuilder
+import org.kiji.schema.KijiTable
+import org.kiji.schema.layout.KijiTableLayouts
+import org.kiji.schema.testutil.AbstractKijiIntegrationTest
+
+class HFileKijiJobIntegration extends AbstractKijiIntegrationTest {
+
+  private var mKiji: Kiji = null
+
+  @Before
+  def setupTest {
+    val desc = KijiTableLayouts.getLayout("layout/avro-types-1.3.json")
+    mKiji = Kiji.Factory.open(getKijiURI())
+    mKiji.createTable(desc)
+  }
+
+  @After
+  def cleanupTest {
+    mKiji.release()
+  }
+
+  @Test
+  def testShouldBulkLoadMapReduceJob {
+    Resources.withKijiTable(mKiji, "table") { table =>
+      val tempHFileFolder = mTempDir.newFolder()
+      FileUtils.deleteDirectory(tempHFileFolder)
+
+      val toolRunnerArgs = Array(
+        classOf[SimpleAverageJob].getName(),
+        "--input",
+        "src/test/resources/data/input_lines.txt",
+        "--output",
+        table.getURI().toString(),
+        "--hfile-output",
+        tempHFileFolder.toString(),
+        "--hdfs")
+
+      Tool.main(toolRunnerArgs)
+
+      bulkLoad(tempHFileFolder, table)
+
+      Resources.withKijiTableReader(table) { myReader =>
+        val request = KijiDataRequest.create("family", "double_column")
+        val result = myReader.get(table.getEntityId("key1"), request)
+
+        Assert.assertEquals(20.0, result.getMostRecentValue("family", "double_column"), 0.0d)
+      }
+    }
+  }
+
+  @Test
+  def testShouldBulkLoadMapOnlyJob {
+    Resources.withKijiTable(mKiji, "table") { table =>
+      val tempHFileFolder = mTempDir.newFolder()
+      FileUtils.deleteDirectory(tempHFileFolder)
+
+      val toolRunnerArgs = Array(
+        classOf[SimpleLoaderJob].getName(),
+        "--input",
+        "src/test/resources/data/input_lines.txt",
+        "--output",
+        table.getURI().toString(),
+        "--hfile-output",
+        tempHFileFolder.toString(),
+        "--hdfs")
+
+      Tool.main(toolRunnerArgs)
+
+      bulkLoad(tempHFileFolder, table)
+
+      Resources.withKijiTableReader(table) { myReader =>
+        val colBuilder = KijiDataRequestBuilder.ColumnsDef
+          .create()
+          .withMaxVersions(10).add("family", "double_column")
+
+        val request = KijiDataRequest.builder().addColumns(colBuilder).build()
+        val result = myReader.get(table.getEntityId("key1"), request)
+        val cells = result.getCells("family", "double_column")
+
+        Assert.assertEquals(3, cells.size())
+        Assert.assertEquals(30.0, result.getMostRecentValue("family", "double_column"), 0.0d)
+      }
+    }
+  }
+
+  @Test
+  def testShouldBulkLoadMapOnlyJobWithAnotherOutput {
+    Resources.withKijiTable(mKiji, "table") { table =>
+      val tempHFileFolder = mTempDir.newFolder()
+      FileUtils.deleteDirectory(tempHFileFolder)
+
+      val tempTsvFolder = mTempDir.newFolder()
+      FileUtils.deleteDirectory(tempTsvFolder)
+
+      val toolRunnerArgs = Array(
+        classOf[SimpleLoaderMultiOutputJob].getName(),
+        "--input",
+        "src/test/resources/data/input_lines.txt",
+        "--output",
+        table.getURI().toString(),
+        "--tsv_output",
+        tempTsvFolder.toString(),
+        "--hfile-output",
+        tempHFileFolder.toString(),
+        "--hdfs")
+
+      Tool.main(toolRunnerArgs)
+
+      bulkLoad(tempHFileFolder, table)
+
+      Resources.withKijiTableReader(table) { myReader =>
+        val colBuilder = KijiDataRequestBuilder.ColumnsDef
+          .create()
+          .withMaxVersions(10).add("family", "double_column")
+
+        val request = KijiDataRequest.builder().addColumns(colBuilder).build()
+        val result = myReader.get(table.getEntityId("key1"), request)
+        val cells = result.getCells("family", "double_column")
+
+        Assert.assertEquals(3, cells.size())
+        Assert.assertEquals(30.0, result.getMostRecentValue("family", "double_column"), 0.0d)
+      }
+    }
+  }
+
+  @Test
+  def testShouldBulkLoadIntoMapFamily {
+    Resources.withKijiTable(mKiji, "table") { table =>
+      val tempHFileFolder = mTempDir.newFolder()
+      FileUtils.deleteDirectory(tempHFileFolder)
+
+      val tempTsvFolder = mTempDir.newFolder()
+      FileUtils.deleteDirectory(tempTsvFolder)
+
+      val toolRunnerArgs = Array(
+        classOf[SimpleLoaderMapTypeFamilyJob].getName(),
+        "--input",
+        "src/test/resources/data/input_lines.txt",
+        "--output",
+        table.getURI().toString(),
+        "--hfile-output",
+        tempHFileFolder.toString(),
+        "--hdfs")
+
+      Tool.main(toolRunnerArgs)
+
+      bulkLoad(tempHFileFolder, table)
+
+      Resources.withKijiTableReader(table) { myReader =>
+        val colBuilder = KijiDataRequestBuilder.ColumnsDef
+          .create()
+          .withMaxVersions(10).addFamily("searches_dev")
+
+        val request = KijiDataRequest.builder().addColumns(colBuilder).build()
+        val result = myReader.get(table.getEntityId("key1"), request)
+        val cells = result.getCells("searches_dev")
+
+        Assert.assertEquals(3, cells.size())
+      }
+    }
+  }
+
+  private def bulkLoad(hFilePath: File, table: KijiTable) {
+    val hFileLoader = HFileLoader.create(super.getConf())
+    hFileLoader.load(new Path(hFilePath.toString()), table)
+  }
+}
+
+class SimpleAverageJob(args: Args) extends HFileKijiJob(args) {
+
+  // Parse arguments
+  val inputUri: String = args("input")
+  val outputUri: String = args("output")
+  val hFileOutput = args("hfile-output")
+
+  // Read each line. Split on " " which should yield string, value
+  // string part eventually is the entity_id, value will be averaged in the end.
+
+  TextLine(inputUri)
+    .map('line -> ('entityId, 'numViews)) { line: String =>
+      val parts = line.split(" ")
+      (EntityId(parts(0)), parts(1).toInt)
+    }
+    .groupBy('entityId) { _.average('numViews) }
+    .write(HFileKijiOutput(outputUri, hFileOutput, ('numViews -> "family:double_column")))
+}
+
+class SimpleLoaderJob(args: Args) extends HFileKijiJob(args) {
+
+  // Parse arguments
+  val inputUri: String = args("input")
+  val outputUri: String = args("output")
+  val hFileOutput = args("hfile-output")
+
+  // Read each line. Generate an entityId and numViews. The entityId here is duplicated
+  // so there should be multiple versions of each in HBase.
+  TextLine(inputUri)
+    .read
+    .mapTo('line -> ('entityId, 'numViews, 'ts)) { line: String =>
+      val parts = line.split(" ")
+      Thread.sleep(2) // Force a sleep so that we get unique timestamps
+      (EntityId(parts(0)), parts(1).toDouble, System.currentTimeMillis())
+    }
+    .write(HFileKijiOutput(outputUri, hFileOutput, 'ts, ('numViews -> "family:double_column")))
+}
+
+class SimpleLoaderMapTypeFamilyJob(args: Args) extends HFileKijiJob(args) {
+
+  // Parse arguments
+  val inputUri: String = args("input")
+  val outputUri: String = args("output")
+  val hFileOutput = args("hfile-output")
+
+  @transient
+  lazy val outputCols = Map('numViews -> ColumnFamilyOutputSpec("searches_dev",
+      qualifierSelector='numViews))
+
+  // Read each line. Generate an entityId and numViews. The entityId here is duplicated
+  // so there should be multiple versions of each in HBase.
+  TextLine(inputUri)
+    .read
+    .mapTo('line -> ('entityId, 'numViews, 'ts)) { line: String =>
+      val parts = line.split(" ")
+      Thread.sleep(2) // Force a sleep so that we get unique timestamps
+      (EntityId(parts(0)), parts(1).toInt, System.currentTimeMillis())
+    }
+    .write(HFileKijiOutput(outputUri, hFileOutput, 'ts, outputCols))
+}
+
+class SimpleLoaderMultiOutputJob(args: Args) extends HFileKijiJob(args) {
+
+  // Parse arguments
+  val inputUri: String = args("input")
+  val outputUri: String = args("output")
+  val tsvOutputURI: String = args("tsv_output")
+  val hFileOutput = args("hfile-output")
+
+  // Read each line. Generate an entityId and numViews. The entityId here is duplicated
+  // so there should be multiple versions of each in HBase.
+  val computePipe = TextLine(inputUri)
+    .read
+    .mapTo('line -> ('entityId, 'numViews, 'ts)) { line: String =>
+      val parts = line.split(" ")
+      Thread.sleep(2) // Force a sleep so that we get unique timestamps
+      (EntityId(parts(0)), parts(1).toDouble, System.currentTimeMillis())
+    }
+
+  computePipe.write(HFileKijiOutput(outputUri, hFileOutput, 'ts,
+      ('numViews -> "family:double_column")))
+  computePipe.write(Tsv(tsvOutputURI))
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/framework/serialization/KijiLockerSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/framework/serialization/KijiLockerSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..415791ab37d01d9aa5a8a52ec29837811d6491eb
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/framework/serialization/KijiLockerSuite.scala
@@ -0,0 +1,55 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.framework.serialization
+
+import java.io.ObjectOutputStream
+import java.io.ObjectInputStream
+import java.io.ByteArrayOutputStream
+import java.io.ByteArrayInputStream
+
+import org.apache.avro.Schema
+import org.scalatest.junit.JUnitRunner
+import org.junit.runner.RunWith
+
+import org.kiji.express.KijiSuite
+
+@RunWith(classOf[JUnitRunner])
+class KijiLockerSuite extends KijiSuite {
+
+  test("KijiLocker can serialize and deserialize a Schema.") {
+    val schema = Schema.create(Schema.Type.STRING)
+
+    val copy = lockerRoundtrip(schema)
+
+    assert(schema === copy)
+    assert(schema.toString(false) === copy.toString(false))
+  }
+
+  def lockerRoundtrip[T <: AnyRef](value: T): T = {
+    val baos = new ByteArrayOutputStream
+    val oos = new ObjectOutputStream(baos)
+    oos.writeObject(KijiLocker(value))
+
+    val ois = new ObjectInputStream(new ByteArrayInputStream(baos.toByteArray))
+
+    ois.readObject().asInstanceOf[KijiLocker[T]].get
+  }
+}
+
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/tool/TmpJarsToolSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/tool/TmpJarsToolSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..212979a3db0e0427dd76b631846d2cede5fddddc
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/tool/TmpJarsToolSuite.scala
@@ -0,0 +1,119 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.tool
+
+import java.io.File
+
+import com.google.common.io.Files
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+
+@RunWith(classOf[JUnitRunner])
+class TmpJarsToolSuite extends FunSuite {
+
+  test("entryToFile transforms string paths to files.") {
+    assert(new File("/is/a/classpath/entry") == TmpJarsTool.entryToFile("/is/a/classpath/entry"))
+  }
+
+  def makeGlobTestDirectory(): File = {
+    // Make a temporary directory and put a directory, a .jar file, and a .txt file in it.
+    val tempDirectory = Files.createTempDir()
+    val subDirectory = new File(tempDirectory, "subdir")
+    subDirectory.mkdir()
+    val jarFile = new File(tempDirectory, "aJar.jar")
+    jarFile.createNewFile()
+    val txtFile = new File(tempDirectory, "aText.txt")
+    txtFile.createNewFile()
+    tempDirectory
+  }
+
+  test("globToFiles will unglob a file that is a glob (whose name is *).") {
+    val globTestDir = makeGlobTestDirectory()
+    val globTest = new File(globTestDir, "*")
+    val unglobbedFiles = TmpJarsTool.globToFiles(globTest)
+    assert(2 == unglobbedFiles.size)
+
+    assert(unglobbedFiles.contains(new File(globTestDir, "aJar.jar")))
+    assert(unglobbedFiles.contains(new File(globTestDir, "aText.txt")))
+    assert(!unglobbedFiles.contains(new File(globTestDir, "subdir")))
+  }
+
+  test("globToFiles will leave a file whose name is not * alone.") {
+    val testDir = new File("/i/am/a/file")
+    val unGlobbedFiles = TmpJarsTool.globToFiles(testDir)
+    assert(1 == unGlobbedFiles.size)
+    assert(testDir == unGlobbedFiles(0))
+  }
+
+  test("isJar says yes to a .jar file.") {
+    assert(TmpJarsTool.isJar(new File("/i/am/a/jarFile.jar")))
+  }
+
+  test("isJar says no to a file that isn't a .jar file.") {
+    assert(!TmpJarsTool.isJar(new File("/i/am/not/a/jarFile.txt")))
+  }
+
+  test("getJarsFromClasspath will ignore empty classpath entries.") {
+    val classpath = ":/i/am/aJar.jar::/i/am/anotherJar.jar:"
+    val jars = TmpJarsTool.getJarsFromClasspath(classpath)
+    assert(2 == jars.size)
+    assert(jars.contains(new File("/i/am/aJar.jar")))
+    assert(jars.contains(new File("/i/am/anotherJar.jar")))
+  }
+
+  test("getJarsFromClasspath will ignore directories.") {
+    val globTestDir = makeGlobTestDirectory()
+    val classpath = globTestDir.getAbsolutePath + ":" + new File("/i/am/aJar.jar")
+    val jars = TmpJarsTool.getJarsFromClasspath(classpath)
+    assert(1 == jars.size)
+    assert(jars.contains(new File("/i/am/aJar.jar")))
+  }
+
+  test("getJarsFromClasspath will unglob glob entries and leave regular files alone.") {
+    val globTestDir = makeGlobTestDirectory()
+    val globTest = new File(globTestDir, "*")
+    val classpath = globTest.getAbsolutePath + ":" + "/i/am/aJar.jar"
+    val jars = TmpJarsTool.getJarsFromClasspath(classpath)
+    assert(2 == jars.size)
+    assert(jars.contains(new File(globTestDir, "aJar.jar")))
+    assert(jars.contains(new File("/i/am/aJar.jar")))
+  }
+
+  test("getJarsFromClasspath will ignore unJars.") {
+    val classpath = "/i/am/aJar.jar:/i/am/text.txt"
+    val jars = TmpJarsTool.getJarsFromClasspath(classpath)
+    assert(1 == jars.size)
+    assert(jars.contains(new File("/i/am/aJar.jar")))
+  }
+
+  test("getjarsFromClasspath will transform a classpath into jar files accessible from that "
+      + "classpath.") {
+    val globTestDir = makeGlobTestDirectory()
+    val globTest = new File(globTestDir, "*")
+    val anotherTmpDir = Files.createTempDir()
+    val classpath = globTest.getAbsolutePath +
+        ":" + "/i/am/aJar.jar" + ":" + "/i/am/text.txt" + ":" + anotherTmpDir.getAbsolutePath
+    val jars = TmpJarsTool.getJarsFromClasspath(classpath)
+    assert(2 == jars.size)
+    assert(jars.contains(new File(globTestDir, "aJar.jar")))
+    assert(jars.contains(new File("/i/am/aJar.jar")))
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/util/AvroTupleConversionsSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/util/AvroTupleConversionsSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..6396df245ba8c8900d2a3b0e6f44f5528d259597
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/util/AvroTupleConversionsSuite.scala
@@ -0,0 +1,56 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import cascading.tuple.Fields
+import cascading.tuple.Tuple
+import cascading.tuple.TupleEntry
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.avro.NameFormats
+
+@RunWith(classOf[JUnitRunner])
+class AvroTupleConversionsSuite
+    extends KijiSuite {
+
+  test("Specific records get packed correctly with any variable name style.") {
+    val inputs = Map(
+        "snake_case_ugh" -> 13,
+        "CamelCaseEvenWorse" -> 14,
+        "camelPupCase" -> 15,
+        "SCREAMING_SNAKE_CASE_YAH" -> 16)
+    val fields = new Fields(inputs.keys.toSeq:_*)
+
+    val converter = AvroSpecificTupleConverter[NameFormats](fields,
+        implicitly[Manifest[NameFormats]])
+
+    val tuple = new Tuple()
+    inputs.values.foreach(tuple.addInteger)
+
+    val record: NameFormats = converter.apply(new TupleEntry(fields, tuple))
+
+
+    inputs.foreach { case (k, v) => assert(v === record.get(k)) }
+
+  }
+
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/util/AvroTypesComplete.scala b/kiji-express/src/test/scala/org/kiji/express/flow/util/AvroTypesComplete.scala
new file mode 100644
index 0000000000000000000000000000000000000000..5230bf6fefd3906b00877d8e8f0f1b3ccad4066a
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/util/AvroTypesComplete.scala
@@ -0,0 +1,137 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import scala.util.Random
+import scala.collection.JavaConverters.seqAsJavaListConverter
+
+import org.apache.avro.Schema
+import org.apache.avro.generic.GenericArray
+import org.apache.avro.generic.GenericData
+import org.apache.avro.generic.GenericData.Fixed
+import org.apache.avro.generic.GenericEnumSymbol
+import org.apache.avro.generic.GenericFixed
+import org.apache.avro.generic.GenericRecord
+import org.apache.avro.generic.GenericRecordBuilder
+
+import org.kiji.express.avro.SimpleRecord
+import org.kiji.express.flow.EntityId
+import org.kiji.schema.layout.KijiTableLayout
+import org.kiji.schema.layout.KijiTableLayouts
+
+/** Utils for testing against the avro-types-complete layout. */
+object AvroTypesComplete {
+
+  val layout = KijiTableLayout.newLayout(
+      KijiTableLayouts.getLayout("layout/avro-types-complete.json"))
+
+  val family = "strict"
+
+  /** Column names. */
+  val counterColumn = "counter"
+  val rawColumn = "raw"
+  val nullColumn = "null"
+  val booleanColumn = "boolean"
+  val intColumn = "int"
+  val longColumn = "long"
+  val floatColumn = "float"
+  val doubleColumn = "double"
+  val bytesColumn = "bytes"
+  val stringColumn = "string"
+  val specificColumn = "specific"
+  val genericColumn = "generic"
+  val enumColumn = "enum"
+  val arrayColumn = "array"
+  val mapColumn = "map"
+  val unionColumn = "union"
+  val fixedColumn = "fixed"
+
+  private val schemaParser = new Schema.Parser()
+
+  /** Column schemas. */
+  val nullSchema = Schema.create(Schema.Type.NULL)
+  val booleanSchema = Schema.create(Schema.Type.BOOLEAN)
+  val intSchema = Schema.create(Schema.Type.INT)
+  val longSchema = Schema.create(Schema.Type.LONG)
+  val floatSchema = Schema.create(Schema.Type.FLOAT)
+  val doubleSchema = Schema.create(Schema.Type.DOUBLE)
+  val bytesSchema = Schema.create(Schema.Type.BYTES)
+  val stringSchema = schemaParser.parse(
+      "{ \"type\": \"string\", \"avro.java.string\": \"String\" }")
+  val specificSchema = SimpleRecord.getClassSchema
+  val genericSchema = schemaParser.parse(
+      "{\"type\": \"record\", \"name\": \"Vector\", \"fields\": [" +
+          "{\"name\": \"length\", \"type\": \"int\"}," +
+          " {\"name\": \"angle\", \"type\": \"float\"}]}")
+  val enumSchema = schemaParser.parse(
+      "{\"type\": \"enum\", \"name\": \"Direction\", \"symbols\":" +
+          " [\"NORTH\", \"EAST\", \"SOUTH\", \"WEST\"]}")
+  val arraySchema = schemaParser.parse(
+    "{\"type\": \"array\", \"items\": { \"type\": \"string\", \"avro.java.string\": \"String\" }}")
+  val mapSchema = schemaParser.parse("{\"type\": \"map\", \"values\": \"int\"}")
+  val unionSchema = schemaParser.parse("[\"string\", \"int\"]")
+  val fixedSchema = schemaParser.parse("{\"type\": \"fixed\", \"size\": 10, \"name\": \"hash\"}")
+
+  /** Record builders. */
+  val specificBuilder = SimpleRecord.newBuilder()
+  val genericBuilder = new GenericRecordBuilder(genericSchema)
+  val genericData = new GenericData()
+
+
+  /** Value generators. */
+  val rand = new Random
+  val base: Iterable[_] = Range(0, 10) // Determines the number of inputs per test
+  val nulls = base.map { _ => null }
+  def booleans: Iterable[Boolean] = base.map { _ => rand.nextBoolean() }
+  def ints: Iterable[Int] = base.map { _ => rand.nextInt() }
+  def longs: Iterable[Long] = base.map { _ => rand.nextLong() }
+  def floats: Iterable[Float] = base.map { _ => rand.nextFloat() }
+  def doubles: Iterable[Double] = base.map { _ => rand.nextDouble() }
+  def bytes: Iterable[Array[Byte]] = base.map { _ =>
+    val ary = Array.ofDim[Byte](32)
+    rand.nextBytes(ary)
+    ary
+  }
+  def strings: Iterable[String] = base.map { _ => rand.nextString(32) }
+  def specificRecords: Iterable[SimpleRecord] = longs.zip(strings)
+      .map { fields => specificBuilder.setL(fields._1).setS(fields._2).build() }
+  def genericRecords: Iterable[GenericRecord] = ints.zip(floats)
+      .map { fields => genericBuilder.set("length", fields._1).set("angle", fields._2).build() }
+  def enumValues: Vector[String] = Vector("NORTH", "EAST", "SOUTH", "WEST")
+  def enums: Iterable[GenericEnumSymbol] = base.map { _ =>
+    genericData.createEnum(enumValues(rand.nextInt(4)), enumSchema).asInstanceOf[GenericEnumSymbol]}
+  def enumStrings: Iterable[String] = base.map { _ => enumValues(rand.nextInt(4))}
+  def arrays: Iterable[Iterable[String]] = base.map { _ => strings }
+  def avroArrays: Iterable[GenericArray[String]] = arrays.map { strings =>
+    new GenericData.Array(arraySchema, strings.toSeq.asJava)
+  }
+  def maps: Iterable[Map[String, Int]] = base.map { _ => strings.zip(ints).toMap }
+  def unions: Iterable[Any] = booleans.zip(strings.zip(ints)).map { t =>
+    val (bool, (string, int)) = t
+    if (bool) string else int
+  }
+  def fixedByteArrays: Iterable[Array[Byte]] = base.map { _ =>
+    val ary = Array.ofDim[Byte](10)
+    rand.nextBytes(ary)
+    ary
+  }
+  def fixeds: Iterable[GenericFixed] = fixedByteArrays.map { bs => new Fixed(fixedSchema, bs) }
+  def eids: Iterable[EntityId] = strings.map(EntityId(_))
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/util/AvroUtilWithSchemaSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/util/AvroUtilWithSchemaSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..6aaf1283e403916d89c615155be163dc6afa6540
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/util/AvroUtilWithSchemaSuite.scala
@@ -0,0 +1,71 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import scala.collection.JavaConverters.seqAsJavaListConverter
+
+import org.apache.avro.Schema
+import org.apache.avro.SchemaBuilder
+import org.apache.avro.generic.GenericData
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+
+@RunWith(classOf[JUnitRunner])
+class AvroUtilWithSchemaSuite extends FunSuite {
+
+  test("avroToScala will convert an Avro GenericArray to a Scala equivalent") {
+    val list = List("foo", "bar", "baz")
+    val schema = SchemaBuilder.array.items(Schema.create(Schema.Type.STRING))
+    val glist = new GenericData.Array(schema, list.asJava)
+    assert(list === AvroUtil.avroToScala(glist))
+  }
+
+  test("avroToScala will convert a GenericFixed to a Array[Byte]") {
+    val bytes = "foo bar, baz?".getBytes
+    val schema = SchemaBuilder.fixed("name").size(bytes.length)
+    val fixed = new GenericData.Fixed(schema, bytes)
+    assert(bytes === AvroUtil.avroToScala(fixed))
+  }
+
+  test("avroToScala will convert a GenericEnumSymbol to the String equivalent") {
+    val name = "CACTUS"
+    val schema = SchemaBuilder.enumeration("name").symbols("CACTUS", "CHERRY", "PIZZA")
+    val enum = new GenericData.EnumSymbol(schema, name)
+    assert(name === AvroUtil.avroToScala(enum))
+  }
+
+  /** TODO: implement once Avro generic map object exists. */
+  ignore("avroToScala will convert a GenericMap to the Scala equivalent.") { }
+
+  test("avroToScala will recursively convert a GenericArray to the Scala equivalents.") {
+    val names = List("CACTUS", "CHERRY", "CHERRY", "PIZZA")
+    val enumSchema = SchemaBuilder.enumeration("name").symbols("CACTUS", "CHERRY", "PIZZA")
+    val listSchema = SchemaBuilder.array.items(enumSchema)
+
+    val glist = new GenericData.Array(listSchema,
+        names.map(new GenericData.EnumSymbol(enumSchema, _)).asJava)
+
+    assert(names === AvroUtil.avroToScala(glist))
+  }
+
+  /** TODO: implement once Avro generic map object exists. */
+  ignore("avroToScala will recursively convert a GenericMap's values to the Scala equivalent.") { }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/util/CellMathUtilSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/util/CellMathUtilSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..0cb65800eeda5b1c4c6decef5fc563368cc081ec
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/util/CellMathUtilSuite.scala
@@ -0,0 +1,73 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+import org.scalatest.matchers.ShouldMatchers
+
+import org.kiji.express.flow.FlowCell
+
+@RunWith(classOf[JUnitRunner])
+class CellMathUtilSuite
+    extends FunSuite
+    with ShouldMatchers {
+  val mapCell0: FlowCell[Long] = FlowCell("info", "a", 0L, 0L)
+  val mapCell1: FlowCell[Long] = FlowCell("info", "a", 1L, 1L)
+  val mapCell2: FlowCell[Long] = FlowCell("info", "b", 2L, 0L)
+  val mapCell3: FlowCell[Long] = FlowCell("info", "b", 3L, 3L)
+  val mapCell4: FlowCell[Long] = FlowCell("info", "c", 4L, 0L)
+  val mapCellSeq: List[FlowCell[Long]] = List(
+      mapCell4,
+      mapCell3,
+      mapCell2,
+      mapCell1,
+      mapCell0
+  )
+
+  test("CellMathUtils should properly sum simple types") {
+    assert(4 === CellMathUtil.sum(mapCellSeq))
+  }
+
+  test("CellMathUtils should properly compute the squared sum of simple types") {
+    assert(10.0 === CellMathUtil.sumSquares(mapCellSeq))
+  }
+
+  test("CellMathUtils should properly compute the average of simple types") {
+    assert(0.8 === CellMathUtil.mean(mapCellSeq))
+  }
+
+  test("CellMathUtils should properly compute the standard deviation of simple types") {
+    CellMathUtil.stddev(mapCellSeq) should be (1.16619 plusOrMinus 0.1)
+  }
+
+  test("CellMathUtils should properly compute the variance of simple types") {
+    CellMathUtil.variance(mapCellSeq) should be (1.36 plusOrMinus 0.1)
+  }
+
+  test("CellMathUtils should properly find the minimum of simple types") {
+    assert(0.0 === CellMathUtil.min(mapCellSeq))
+  }
+
+  test("CellMathUtils should properly find the maximum of simple types") {
+    assert(3.0 === CellMathUtil.max(mapCellSeq))
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/util/EntityIdFactoryCacheSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/util/EntityIdFactoryCacheSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..93afd0fad5eecef738b9dc0958f3196234a9fc82
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/util/EntityIdFactoryCacheSuite.scala
@@ -0,0 +1,94 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+
+import org.apache.hadoop.hbase.HBaseConfiguration
+import org.junit.runner.RunWith
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.KijiSuite
+import org.kiji.express.flow.util.Resources._
+import org.kiji.schema.KijiTable
+import org.kiji.schema.layout.KijiTableLayout
+import org.kiji.schema.layout.KijiTableLayouts
+
+/**
+ * Unit tests for [[org.kiji.express.flow.util.EntityIdFactoryCache]]
+ */
+@RunWith(classOf[JUnitRunner])
+class EntityIdFactoryCacheSuite extends KijiSuite {
+  test("Test serializing/deserializing configurations") {
+    val configuration = HBaseConfiguration.create()
+    configuration.set("blah", "hi")
+
+    val deserializedConf =
+        EntityIdFactoryCache.deserializeConf(EntityIdFactoryCache.serializeConf(configuration))
+
+    assert(deserializedConf.get("blah") === configuration.get("blah"))
+  }
+
+  test("Test for caching for Row Key Format given Table Uri") {
+    /** Table layout to use for tests. */
+    val tableLayout = KijiTableLayout.newLayout(
+        KijiTableLayouts.getLayout(KijiTableLayouts.FORMATTED_RKF))
+    val uri: String = doAndRelease(makeTestKijiTable(tableLayout)) { table: KijiTable =>
+      table.getURI.toString
+    }
+    val configuration = HBaseConfiguration.create()
+
+    val eidFactory = EntityIdFactoryCache.getFactory(uri, configuration)
+
+    val components = new java.util.ArrayList[Object]()
+    components.add("a")
+    components.add("b")
+    components.add("c")
+    components.add(new java.lang.Integer(1))
+    components.add(new java.lang.Long(7))
+
+    // If this works, we successfully created a Formatted EntityId
+    // which means we were returned the right EID factory.
+    val eid = eidFactory.getEntityId(components)
+    assert(components == eid.getComponents)
+
+    val eidFactoryCached = EntityIdFactoryCache.getFactory(uri, configuration)
+    assert(eidFactory === eidFactoryCached)
+  }
+
+  test("Test cache for two equivalent configurations and table URIs memoizes correctly.") {
+    /** Table layout to use for tests. */
+    val tableLayout = KijiTableLayout.newLayout(
+      KijiTableLayouts.getLayout(KijiTableLayouts.FORMATTED_RKF))
+    val uri: String = doAndRelease(makeTestKijiTable(tableLayout)) { table: KijiTable =>
+      table.getURI.toString
+    }
+    val configuration = HBaseConfiguration.create()
+    configuration.addResource("blah")
+
+    val configurationCopy = HBaseConfiguration.create()
+    configurationCopy.addResource("blah")
+
+    val eidFactory1 = EntityIdFactoryCache.getFactory(uri, configuration)
+
+    val eidFactory2 = EntityIdFactoryCache.getFactory(uri, configurationCopy)
+
+    assert(eidFactory1 === eidFactory2)
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/util/MemoizeSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/util/MemoizeSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..541365400378726b51ca79193ff1c496dd60960d
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/util/MemoizeSuite.scala
@@ -0,0 +1,41 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+
+@RunWith(classOf[JUnitRunner])
+class MemoizeSuite extends FunSuite {
+  test("Test for memoization") {
+    // outer class without equals defined
+    class Outer(val data: Int)
+
+    def strSqLen(s: String): Outer = new Outer(s.length * s.length)
+    val strSqLenMemoized = Memoize(strSqLen)
+    val a = strSqLenMemoized("hello Memo")
+    val b = strSqLen("hello Memo")
+    assert(a.data == b.data)
+    // should go to cache for result
+    val c = strSqLenMemoized("hello Memo")
+    assert(a == c)
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/util/PipeRunner.scala b/kiji-express/src/test/scala/org/kiji/express/flow/util/PipeRunner.scala
new file mode 100644
index 0000000000000000000000000000000000000000..d19b291fd690c148462d485f457dce0c6561c70d
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/util/PipeRunner.scala
@@ -0,0 +1,129 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import java.lang.reflect.Constructor
+
+import scala.collection.mutable
+
+import cascading.flow.Flow
+import cascading.pipe.Pipe
+import cascading.tuple.Fields
+import cascading.tuple.Tuple
+import cascading.tuple.TupleEntry
+import com.twitter.scalding.Args
+import com.twitter.scalding.Job
+import com.twitter.scalding.JobTest
+import com.twitter.scalding.Mode
+import com.twitter.scalding.Read
+import com.twitter.scalding.Tsv
+import com.twitter.scalding.TupleConverter
+import com.twitter.scalding.TupleSetter
+import com.twitter.scalding.Write
+
+/**
+ * Module for providing a test framework around testing Express pipes.  Allows testing of pipes
+ * independent of the Source and Sink.  See `runPipe`.
+ */
+object PipeRunner {
+
+  /**
+   * Unpacks [[scala.Product]] instances into [[cascading.tuple.Tuple]] instances.
+   * @param n number of fields expected in product type
+   */
+  private class ProductTupleSetter[T](n: Int) extends TupleSetter[T] {
+    def arity: Int = n
+
+    def apply(t: T): Tuple = {
+      if (n == 1) {
+        new Tuple(t.asInstanceOf[Object])
+      } else {
+        val p: Product = t.asInstanceOf[Product]
+        require(p.productArity == n)
+        val tuple = new Tuple()
+        p.productIterator.foreach(tuple.add)
+        tuple
+      }
+    }
+  }
+
+  /**
+   * Converts [[cascading.tuple.TupleEntry]]s into scala Tuples via frightening black majik.
+   */
+  private class ProductTupleConverter[O] extends TupleConverter[O] {
+
+    override def arity: Int = -1
+
+    var ctors: Map[Int, Constructor[_]] = Map()
+    private def tupleCtor(arity: Int) = {
+      if (!ctors.contains(arity)) {
+        ctors += arity -> Class.forName("scala.Tuple" + arity).getConstructors.apply(0)
+      }
+      ctors(arity)
+    }
+
+    override def apply(entry: TupleEntry): O = {
+      val arity = entry.size()
+      if (arity == 1) {
+        entry.getObject(0).asInstanceOf[O]
+      } else {
+        tupleCtor(entry.size())
+            .newInstance(Range(0, entry.size()).map(entry.getObject):_*)
+            .asInstanceOf[O]
+      }
+    }
+  }
+
+  /**
+   * Simulates running a pipe with given inputs containing given fields.  Returns a
+   * [[scala.collection.immutable.List]] of result values.  The returned value type is not known
+   * statically; therefore runtime type errors on the result are possible.
+   *
+   * @tparam I Type of input values.
+   * @tparam O Type of output values.  Statically resolves to [[scala.Nothing]].
+   * @param pipe Pipe to run.
+   * @param fields Field names of input tuples.
+   * @param input Iterable of input tuples.
+   * @return List of output values.
+   */
+  def runPipe[I, O](pipe: Pipe, fields: Fields, input: Iterable[I]): List[O] = {
+    val source = Tsv("input", fields)
+    val sink = Tsv("output")
+
+    implicit val setter: TupleSetter[I] = new ProductTupleSetter(fields.size())
+    implicit val converter: TupleConverter[O] = new ProductTupleConverter[O]
+
+    class InnerJob(args: Args) extends Job(args) {
+      override def buildFlow(implicit mode : Mode): Flow[_]  = {
+        validateSources(mode)
+        mode.newFlowConnector(config).connect(
+          pipe.getName, source.createTap(Read), sink.createTap(Write), pipe)
+      }
+    }
+
+    var buffer: mutable.Buffer[O] = null
+    val jobTest = JobTest(new InnerJob(_))
+        .source(source, input)
+        .sink(sink) { b: mutable.Buffer[O] => buffer = b}
+
+    jobTest.run
+    buffer.toList
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/flow/util/ResourcesSuite.scala b/kiji-express/src/test/scala/org/kiji/express/flow/util/ResourcesSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..7baa72774943dbd4141ea0c032adc064836ffc9e
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/flow/util/ResourcesSuite.scala
@@ -0,0 +1,95 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.flow.util
+
+import java.io.Closeable
+
+import org.junit.runner.RunWith
+import org.scalatest.FunSuite
+import org.scalatest.junit.JUnitRunner
+
+import org.kiji.express.flow.util.Resources._
+
+@RunWith(classOf[JUnitRunner])
+class ResourcesSuite extends FunSuite {
+  test("doAnd runs an operation") {
+    var isClosed = false
+    def resource: Closeable = new Closeable {
+      def close() {
+        isClosed = true
+      }
+    }
+    def after(r: Closeable) { r.close() }
+    doAnd(resource, after) { _ => () }
+
+    assert(isClosed)
+  }
+
+  test("doAnd catches one normal exception") {
+    var isClosed = false
+    def resource: Closeable = new Closeable {
+      def close() {
+        isClosed = true
+      }
+    }
+    def after(r: Closeable) { r.close() }
+    val result = try {
+      doAnd(resource, after) { _ => sys.error("test") }
+      false
+    } catch {
+      case err: RuntimeException => true
+    }
+
+    assert(isClosed)
+    assert(result)
+  }
+
+  test("doAnd catches one exception while closing") {
+    def resource: Closeable = new Closeable {
+      def close() {
+        sys.error("test")
+      }
+    }
+    def after(r: Closeable) { r.close() }
+    val result = try {
+      doAnd(resource, after) { _ => () }
+      false
+    } catch {
+      case err: RuntimeException => true
+    }
+
+    assert(result)
+  }
+
+  test("doAnd catches two exceptions") {
+    def resource: Closeable = new Closeable {
+      def close() {
+        sys.error("test")
+      }
+    }
+    def after(r: Closeable) { r.close() }
+    val result = try {
+      doAnd(resource, after) { _ => sys.error("t") }
+      false
+    } catch {
+      case CompoundException(_, Seq(_, _)) => true
+    }
+  }
+}
diff --git a/kiji-express/src/test/scala/org/kiji/express/repl/ExpressShellSuite.scala b/kiji-express/src/test/scala/org/kiji/express/repl/ExpressShellSuite.scala
new file mode 100644
index 0000000000000000000000000000000000000000..68476ed065351169e65300926bebd5e4e3d9a3cb
--- /dev/null
+++ b/kiji-express/src/test/scala/org/kiji/express/repl/ExpressShellSuite.scala
@@ -0,0 +1,38 @@
+/**
+ * (c) Copyright 2013 WibiData, Inc.
+ *
+ * See the NOTICE file distributed with this work for additional
+ * information regarding copyright ownership.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kiji.express.repl
+
+import org.junit.runner.RunWith
+import org.scalatest._
+import org.scalatest.junit.JUnitRunner
+
+/**
+ * Tests the functionality of [[org.kiji.express.repl.ExpressShell]].
+ */
+@RunWith(classOf[JUnitRunner])
+class ExpressShellSuite extends FunSuite {
+  test("The shell can start a REPL using command line arguments.") {
+    pending
+  }
+
+  test("The shell runner can write a jar file containing classes in a virtual directory.") {
+    pending
+  }
+}
diff --git a/pom.xml b/pom.xml
index 162def687b77fa10d19d8c52d28bb4ccd051b7be..b69828119f5720f40337324c6ca76217d547b1dc 100644
--- a/pom.xml
+++ b/pom.xml
@@ -1,12 +1,32 @@
 <?xml version="1.0" encoding="UTF-8"?>
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+  <!--
+    (c) Copyright 2013 WibiData, Inc.
+
+    See the NOTICE file distributed with this work for additional
+    information regarding copyright ownership.
+
+    Licensed under the Apache License, Version 2.0 (the "License");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+        http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+  -->
   <modelVersion>4.0.0</modelVersion>
 
+  <name>KijiExpress top-level project</name>
+  <inceptionYear>2013</inceptionYear>
+
   <groupId>org.kiji.express</groupId>
-  <artifactId>kiji-express</artifactId>
+  <artifactId>kiji-express-root</artifactId>
   <version>0.14.0-SNAPSHOT</version>
-  <packaging>jar</packaging>
-  <inceptionYear>2013</inceptionYear>
+  <packaging>pom</packaging>
 
   <licenses>
     <license>
@@ -33,298 +53,237 @@
     <elephant-bird.version>4.2</elephant-bird.version>
   </properties>
 
-  <dependencies>
-    <!--
-      Dependencies, alphabetized by groupId:artifactId:type:version
-    -->
-    <dependency>
-      <groupId>com.twitter</groupId>
-      <artifactId>scalding-core_${scala.version}</artifactId>
-      <version>${scalding.version}</version>
-      <scope>compile</scope>
-      <exclusions>
-        <exclusion>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-core</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>com.esotericsoftware.kryo</groupId>
-          <artifactId>kryo</artifactId>
-        </exclusion>
-      </exclusions>
-    </dependency>
-    <dependency>
-      <groupId>com.esotericsoftware.kryo</groupId>
-      <artifactId>kryo</artifactId>
-      <version>2.21</version>
-      <scope>compile</scope>
-    </dependency>
-    <dependency>
-      <groupId>com.twitter</groupId>
-      <artifactId>scalding-args_${scala.version}</artifactId>
-      <version>${scalding.version}</version>
-      <scope>compile</scope>
-    </dependency>
-    <dependency>
-      <groupId>com.twitter</groupId>
-      <artifactId>util-eval_${scala.version}</artifactId>
-      <version>${twitter-util.version}</version>
-      <scope>compile</scope>
-    </dependency>
-    <dependency>
-      <groupId>com.twitter.elephantbird</groupId>
-      <artifactId>elephant-bird-hadoop-compat</artifactId>
-      <version>${elephant-bird.version}</version>
-    </dependency>
-    <dependency>
-      <groupId>com.twitter.elephantbird</groupId>
-      <artifactId>elephant-bird-core</artifactId>
-      <version>${elephant-bird.version}</version>
-    </dependency>
-    <dependency>
-      <groupId>log4j</groupId>
-      <artifactId>log4j</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.avro</groupId>
-      <artifactId>avro</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-core</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hbase</groupId>
-      <artifactId>hbase</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.easymock</groupId>
-      <artifactId>easymock</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.kiji.annotations</groupId>
-      <artifactId>annotations</artifactId>
-      <scope>compile</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.kiji.mapreduce</groupId>
-      <artifactId>kiji-mapreduce</artifactId>
-      <version>${kiji-mapreduce.version}</version>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.kiji.schema</groupId>
-      <artifactId>kiji-schema</artifactId>
-      <version>${kiji-schema.version}</version>
-      <scope>provided</scope>
-    </dependency>
-    <!--
-      Keep this dependency in compile scope so that users can depend on the KijiExpress testing
-      framework by only depending on the KijiExpress test jar.
-    -->
-    <dependency>
-      <groupId>org.kiji.schema</groupId>
-      <artifactId>kiji-schema</artifactId>
-      <version>${kiji-schema.version}</version>
-      <type>test-jar</type>
-      <scope>compile</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.kiji.schema-shell</groupId>
-      <artifactId>kiji-schema-shell</artifactId>
-      <version>${kiji-schema-shell.version}</version>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.kiji.schema-shell</groupId>
-      <artifactId>kiji-schema-shell</artifactId>
-      <version>${kiji-schema-shell.version}</version>
-      <scope>test</scope>
-      <type>test-jar</type>
-    </dependency>
-    <!--
-      Keep this dependency in compile scope so that users can depend on the KijiExpress testing
-      framework by only depending on the KijiExpress test jar.
-    -->
-    <dependency>
-      <groupId>org.kiji.testing</groupId>
-      <artifactId>fake-hbase</artifactId>
-      <version>${fake-hbase.version}</version>
-      <scope>compile</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hbase</groupId>
-      <artifactId>hbase</artifactId>
-      <scope>test</scope>
-      <type>test-jar</type>
-    </dependency>
-    <dependency>
-      <groupId>org.scalatest</groupId>
-      <artifactId>scalatest_${scala.version}</artifactId>
-      <version>${scalatest.version}</version>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.scala-lang</groupId>
-      <artifactId>jline</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.scala-lang</groupId>
-      <artifactId>scala-compiler</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.scala-lang</groupId>
-      <artifactId>scala-library</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.slf4j</groupId>
-      <artifactId>slf4j-api</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.slf4j</groupId>
-      <artifactId>slf4j-log4j12</artifactId>
-      <scope>provided</scope>
-    </dependency>
-  </dependencies>
+  <modules>
+    <module>kiji-express</module>
+  </modules>
+
+  <dependencyManagement>
+    <dependencies>
+      <!--
+	Dependencies, alphabetized by groupId:artifactId:type:version
+      -->
+      <dependency>
+	<groupId>com.twitter</groupId>
+	<artifactId>scalding-core_${scala.version}</artifactId>
+	<version>${scalding.version}</version>
+	<scope>compile</scope>
+	<exclusions>
+	  <exclusion>
+	    <groupId>org.apache.hadoop</groupId>
+	    <artifactId>hadoop-core</artifactId>
+	  </exclusion>
+	  <exclusion>
+	    <groupId>com.esotericsoftware.kryo</groupId>
+	    <artifactId>kryo</artifactId>
+	  </exclusion>
+	</exclusions>
+      </dependency>
+      <dependency>
+	<groupId>com.esotericsoftware.kryo</groupId>
+	<artifactId>kryo</artifactId>
+	<version>2.21</version>
+	<scope>compile</scope>
+      </dependency>
+      <dependency>
+	<groupId>com.twitter</groupId>
+	<artifactId>scalding-args_${scala.version}</artifactId>
+	<version>${scalding.version}</version>
+	<scope>compile</scope>
+      </dependency>
+      <dependency>
+	<groupId>com.twitter</groupId>
+	<artifactId>util-eval_${scala.version}</artifactId>
+	<version>${twitter-util.version}</version>
+	<scope>compile</scope>
+      </dependency>
+      <dependency>
+	<groupId>com.twitter.elephantbird</groupId>
+	<artifactId>elephant-bird-hadoop-compat</artifactId>
+	<version>${elephant-bird.version}</version>
+      </dependency>
+      <dependency>
+	<groupId>com.twitter.elephantbird</groupId>
+	<artifactId>elephant-bird-core</artifactId>
+	<version>${elephant-bird.version}</version>
+      </dependency>
+      <dependency>
+	<groupId>org.kiji.mapreduce</groupId>
+	<artifactId>kiji-mapreduce</artifactId>
+	<version>${kiji-mapreduce.version}</version>
+	<scope>provided</scope>
+      </dependency>
+      <dependency>
+	<groupId>org.kiji.schema</groupId>
+	<artifactId>kiji-schema</artifactId>
+	<version>${kiji-schema.version}</version>
+	<scope>provided</scope>
+      </dependency>
+      <!--
+	Keep this dependency in compile scope so that users can depend on the KijiExpress testing
+	framework by only depending on the KijiExpress test jar.
+      -->
+      <dependency>
+	<groupId>org.kiji.schema</groupId>
+	<artifactId>kiji-schema</artifactId>
+	<version>${kiji-schema.version}</version>
+	<type>test-jar</type>
+	<scope>compile</scope>
+      </dependency>
+      <dependency>
+	<groupId>org.kiji.schema-shell</groupId>
+	<artifactId>kiji-schema-shell</artifactId>
+	<version>${kiji-schema-shell.version}</version>
+      </dependency>
+      <dependency>
+	<groupId>org.kiji.schema-shell</groupId>
+	<artifactId>kiji-schema-shell</artifactId>
+	<version>${kiji-schema-shell.version}</version>
+	<type>test-jar</type>
+      </dependency>
+      <!--
+	Keep this dependency in compile scope so that users can depend on the KijiExpress testing
+	framework by only depending on the KijiExpress test jar.
+      -->
+      <dependency>
+	<groupId>org.kiji.testing</groupId>
+	<artifactId>fake-hbase</artifactId>
+	<version>${fake-hbase.version}</version>
+      </dependency>
+      <dependency>
+	<groupId>org.scalatest</groupId>
+	<artifactId>scalatest_${scala.version}</artifactId>
+	<version>${scalatest.version}</version>
+	<scope>test</scope>
+      </dependency>
+    </dependencies>
+  </dependencyManagement>
 
   <build>
-    <plugins>
-      <plugin>
-        <!--
-          Express depends on the oldest version of KijiSchema / KIJIMR that
-          supports it.
-          To test whether the latest Kiji dependency SNAPSHOTs have introduced
-          a regression:
+    <pluginManagement>
+      <plugins>
+        <plugin>
+	  <!--
+	    Express depends on the oldest version of KijiSchema / KIJIMR that
+	    supports it.
+	    To test whether the latest Kiji dependency SNAPSHOTs have introduced
+            a regression:
 
-          Run the following commands in your shell
+            Run the following commands in your shell
 
             mvn versions:update-properties -N # Update the kiji-*.version properties
                                               # in the top-level pom file; don't recurse.
             mvn clean verify                  # Uses the new snapshot versions
             mvn versions:revert               # Revert the pom file to before update-properties.
                                     # Do not check in new Kiji SNAPSHOT dependencies in the pom.
-        -->
-        <groupId>org.codehaus.mojo</groupId>
-        <artifactId>versions-maven-plugin</artifactId>
-        <configuration>
-          <includeProperties>
-            kiji-schema.version,kiji-schema-shell.version,kiji-mapreduce.version
-          </includeProperties>
-        </configuration>
-        <dependencies>
-          <dependency>
-            <groupId>org.kiji.schema</groupId>
-            <artifactId>kiji-schema</artifactId>
-            <version>${kiji-schema.version}</version>
-          </dependency>
-          <dependency>
-            <groupId>org.kiji.schema-shell</groupId>
-            <artifactId>kiji-schema-shell</artifactId>
-            <version>${kiji-schema-shell.version}</version>
-          </dependency>
-          <dependency>
-            <groupId>org.kiji.mapreduce</groupId>
-            <artifactId>kiji-mapreduce</artifactId>
-            <version>${kiji-mapreduce.version}</version>
-          </dependency>
-        </dependencies>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-jar-plugin</artifactId>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-assembly-plugin</artifactId>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.avro</groupId>
-        <artifactId>avro-maven-plugin</artifactId>
-        <configuration>
-          <!-- Avro string compiled to java.lang.String instead of CharSequence -->
-          <stringType>String</stringType>
-        </configuration>
-      </plugin>
-      <plugin>
-        <groupId>com.google.code.maven-replacer-plugin</groupId>
-        <artifactId>maven-replacer-plugin</artifactId>
-      </plugin>
-      <plugin>
-        <groupId>net.alchim31.maven</groupId>
-        <artifactId>scala-maven-plugin</artifactId>
-        <version>3.1.3</version>
-        <executions>
-          <execution>
-            <id>compilation</id>
-            <goals>
-              <goal>compile</goal>
-              <goal>testCompile</goal>
-            </goals>
-            <configuration>
-              <args>
-                <arg>-unchecked</arg>
-                <arg>-deprecation</arg>
-                <arg>-explaintypes</arg>
-              </args>
-              <sendJavaToScalac>false</sendJavaToScalac>
-              <jvmArgs>
-                <jvmArg>-Xmx2048m</jvmArg>
-              </jvmArgs>
-            </configuration>
-          </execution>
-          <execution>
-            <id>scaladocs</id>
-            <phase>prepare-package</phase>
-            <goals>
-              <goal>doc</goal>
-            </goals>
-            <configuration>
-              <outputDirectory>${project.build.directory}/apidocs</outputDirectory>
-              <reportOutputDirectory>${project.build.directory}/apidocs</reportOutputDirectory>
-              <jvmArgs>
-                <jvmArg>-Xmx2048m</jvmArg>
-              </jvmArgs>
-            </configuration>
-          </execution>
-        </executions>
-      </plugin>
-      <plugin>
-        <groupId>org.scalastyle</groupId>
-        <artifactId>scalastyle-maven-plugin</artifactId>
-        <version>0.3.2</version>
-        <configuration>
-          <failOnViolation>true</failOnViolation>
-          <includeTestSourceDirectory>true</includeTestSourceDirectory>
-          <failOnWarning>false</failOnWarning>
-          <sourceDirectory>${basedir}/src/main/scala</sourceDirectory>
-          <testSourceDirectory>${basedir}/src/test/scala</testSourceDirectory>
-          <configLocation>${basedir}/src/main/scalastyle/scalastyle_config.xml</configLocation>
-        </configuration>
-        <executions>
-          <execution>
-            <goals>
-              <goal>check</goal>
-            </goals>
-          </execution>
-        </executions>
-      </plugin>
+          -->
+	  <groupId>org.codehaus.mojo</groupId>
+	  <artifactId>versions-maven-plugin</artifactId>
+	  <configuration>
+	    <includeProperties>
+	      kiji-schema.version,kiji-schema-shell.version,kiji-mapreduce.version
+	    </includeProperties>
+	  </configuration>
+	  <dependencies>
+	    <dependency>
+	      <groupId>org.kiji.schema</groupId>
+	      <artifactId>kiji-schema</artifactId>
+	      <version>${kiji-schema.version}</version>
+	    </dependency>
+	    <dependency>
+	      <groupId>org.kiji.schema-shell</groupId>
+	      <artifactId>kiji-schema-shell</artifactId>
+	      <version>${kiji-schema-shell.version}</version>
+	    </dependency>
+	    <dependency>
+	      <groupId>org.kiji.mapreduce</groupId>
+	      <artifactId>kiji-mapreduce</artifactId>
+	      <version>${kiji-mapreduce.version}</version>
+	    </dependency>
+	  </dependencies>
+	</plugin>
+	<plugin>
+	  <groupId>org.apache.avro</groupId>
+	  <artifactId>avro-maven-plugin</artifactId>
+	  <configuration>
+	    <!-- Avro string compiled to java.lang.String instead of CharSequence -->
+	    <stringType>String</stringType>
+	  </configuration>
+	</plugin>
+	<plugin>
+	  <groupId>net.alchim31.maven</groupId>
+	  <artifactId>scala-maven-plugin</artifactId>
+	  <version>3.1.3</version>
+	  <executions>
+	    <execution>
+	      <id>compilation</id>
+	      <goals>
+		<goal>compile</goal>
+		<goal>testCompile</goal>
+	      </goals>
+	      <configuration>
+		<args>
+		  <arg>-unchecked</arg>
+		  <arg>-deprecation</arg>
+		  <arg>-explaintypes</arg>
+		</args>
+		<sendJavaToScalac>false</sendJavaToScalac>
+		<jvmArgs>
+		  <jvmArg>-Xmx2048m</jvmArg>
+		</jvmArgs>
+	      </configuration>
+	    </execution>
+	    <execution>
+	      <id>scaladocs</id>
+	      <phase>prepare-package</phase>
+	      <goals>
+		<goal>doc</goal>
+	      </goals>
+	      <configuration>
+		<outputDirectory>${project.build.directory}/apidocs</outputDirectory>
+		<reportOutputDirectory>${project.build.directory}/apidocs</reportOutputDirectory>
+		<jvmArgs>
+		  <jvmArg>-Xmx2048m</jvmArg>
+		</jvmArgs>
+	      </configuration>
+	    </execution>
+	  </executions>
+	</plugin>
+	<plugin>
+	  <groupId>org.scalastyle</groupId>
+	  <artifactId>scalastyle-maven-plugin</artifactId>
+	  <version>0.3.2</version>
+	  <configuration>
+	    <failOnViolation>true</failOnViolation>
+	    <includeTestSourceDirectory>true</includeTestSourceDirectory>
+	    <failOnWarning>false</failOnWarning>
+	    <sourceDirectory>${basedir}/src/main/scala</sourceDirectory>
+	    <testSourceDirectory>${basedir}/src/test/scala</testSourceDirectory>
+	    <configLocation>${basedir}/src/main/scalastyle/scalastyle_config.xml</configLocation>
+	  </configuration>
+	  <executions>
+	    <execution>
+	      <goals>
+		<goal>check</goal>
+	      </goals>
+	    </execution>
+	  </executions>
+	</plugin>
 
-      <!-- ScalaTest -->
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-surefire-plugin</artifactId>
-        <configuration>
-          <argLine>-Xmx2G -XX:MaxPermSize=512m</argLine>
-          <includes>
-            <include>**/*Suite.scala</include>
-            <include>%regex[.*/.*Suite.class]</include>
-          </includes>
-          <forkMode>once</forkMode>
-        </configuration>
-      </plugin>
-    </plugins>
+	<!-- ScalaTest -->
+	<plugin>
+	  <groupId>org.apache.maven.plugins</groupId>
+	  <artifactId>maven-surefire-plugin</artifactId>
+	  <configuration>
+	    <argLine>-Xmx2G -XX:MaxPermSize=512m</argLine>
+	    <includes>
+	      <include>**/*Suite.scala</include>
+	      <include>%regex[.*/.*Suite.class]</include>
+	    </includes>
+	    <forkMode>once</forkMode>
+	  </configuration>
+	</plugin>
+      </plugins>
+    </pluginManagement>
   </build>
 
   <repositories>
@@ -358,4 +317,5 @@
     <url>scm:git:git@github.com:kijiproject/kiji-express.git</url>
     <developerConnection>scm:git:git@github.com:kijiproject/kiji-express.git</developerConnection>
   </scm>
+
 </project>
diff --git a/src/main/assembly/release.xml b/src/main/assembly/release.xml
deleted file mode 100644
index 53abe52c135cfde4023516a0fe9eb17d91edee2e..0000000000000000000000000000000000000000
--- a/src/main/assembly/release.xml
+++ /dev/null
@@ -1,90 +0,0 @@
-<!-- Assembly configuration for the release bundle. -->
-<assembly
-    xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2"
-    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-    xsi:schemaLocation="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2
-                        http://maven.apache.org/xsd/assembly-1.1.2.xsd">
-  <!--
-    (c) Copyright 2013 WibiData, Inc.
-
-    See the NOTICE file distributed with this work for additional
-    information regarding copyright ownership.
-
-    Licensed under the Apache License, Version 2.0 (the "License");
-    you may not use this file except in compliance with the License.
-    You may obtain a copy of the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
-  -->
-  <id>release</id>
-  <formats>
-    <format>dir</format>
-    <format>tar.gz</format>
-  </formats>
-
-  <includeBaseDirectory>true</includeBaseDirectory>
-
-  <fileSets>
-    <fileSet>
-      <useDefaultExcludes>true</useDefaultExcludes>
-      <outputDirectory></outputDirectory>
-      <directory></directory>
-      <fileMode>0644</fileMode>
-      <includes>
-        <include>NOTICE.txt</include>
-        <include>LICENSE.txt</include>
-        <include>README.md</include>
-        <include>RELEASE_NOTES.txt</include>
-      </includes>
-      <filtered>true</filtered>
-    </fileSet>
-    <fileSet>
-      <useDefaultExcludes>false</useDefaultExcludes>
-      <outputDirectory>bin</outputDirectory>
-      <directory>src/main/scripts</directory>
-      <fileMode>0755</fileMode>
-      <excludes>
-        <exclude>*~</exclude>
-        <exclude>*.swp</exclude>
-      </excludes>
-      <filtered>false</filtered>
-    </fileSet>
-    <fileSet>
-      <useDefaultExcludes>false</useDefaultExcludes>
-      <outputDirectory>conf</outputDirectory>
-      <directory>src/main/conf</directory>
-      <fileMode>0644</fileMode>
-      <excludes>
-        <exclude>*~</exclude>
-        <exclude>*.swp</exclude>
-      </excludes>
-      <filtered>true</filtered>
-    </fileSet>
-    <fileSet>
-      <!-- scala api documentation -->
-      <useDefaultExcludes>false</useDefaultExcludes>
-      <outputDirectory>docs/apidocs</outputDirectory>
-      <directory>target/apidocs</directory>
-      <fileMode>0644</fileMode>
-    </fileSet>
-  </fileSets>
-
-  <dependencySets>
-    <dependencySet>
-      <outputDirectory>lib</outputDirectory>
-      <scope>runtime</scope>
-      <useTransitiveFiltering>true</useTransitiveFiltering>
-      <fileMode>0644</fileMode>
-      <excludes>
-        <exclude>org.kiji.schema:kiji-schema</exclude>
-      </excludes>
-    </dependencySet>
-  </dependencySets>
-
-</assembly>
diff --git a/src/main/conf/log4j.properties b/src/main/conf/log4j.properties
deleted file mode 100644
index a0fdb229aa458ce8f558b23eb5f7b2756f0c332e..0000000000000000000000000000000000000000
--- a/src/main/conf/log4j.properties
+++ /dev/null
@@ -1,38 +0,0 @@
-#
-#   (c) Copyright 2013 WibiData, Inc.
-#
-#   See the NOTICE file distributed with this work for additional
-#   information regarding copyright ownership.
-#
-#   Licensed under the Apache License, Version 2.0 (the "License");
-#   you may not use this file except in compliance with the License.
-#   You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-
-log4j.rootLogger=${kiji.logger}
-
-# By default, log INFO to the console.
-kiji.logger=INFO,console
-
-# Console appender:
-log4j.appender.console=org.apache.log4j.ConsoleAppender
-log4j.appender.console.target=System.err
-log4j.appender.console.layout=org.apache.log4j.PatternLayout
-log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c: %m%n
-
-# Quiet down zookeeper, it's too noisy:
-log4j.logger.org.apache.zookeeper=WARN
-log4j.logger.org.apache.hadoop.hbase.zookeeper=WARN
-log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=ERROR
-
-# Loggers in the Kiji framework:
-log4j.logger.org.kiji.schema=INFO
-log4j.logger.org.kiji.mapreduce=INFO
-log4j.logger.org.kiji.express=INFO
diff --git a/src/main/scala/org/kiji/express/flow/ColumnFilterSpec.scala b/src/main/scala/org/kiji/express/flow/ColumnFilterSpec.scala
deleted file mode 100644
index c3fe8ae2a416b6b41c98f1e5e10e3922a093ccd8..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/ColumnFilterSpec.scala
+++ /dev/null
@@ -1,183 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.filter.Filters
-import org.kiji.schema.filter.KijiColumnFilter
-import org.kiji.schema.filter.KijiColumnRangeFilter
-import org.kiji.schema.filter.RegexQualifierColumnFilter
-
-/**
- * An extendable trait used for column filters in Express, which correspond to Kiji and HBase column
- * filters.
- *
- * Filters are implemented via HBase filters, not on the client side, so they can cut down on the
- * amount of data transferred over your network.
- *
- * These can be used in [[org.kiji.express.flow.ColumnInputSpec]].  Currently the filters provided
- * are only useful for [[org.kiji.express.flow.ColumnFamilyInputSpec]], because they are filters for
- * qualifiers when an entire column family is requested.  In the future, there may be filters
- * provided that can filter on the data returned from a fully qualified column.
- *
- * By default, no filter is applied, but you can specify your own.  Only data that pass these
- * filters will be requested and populated into the tuple.  Two column filters are currently
- * provided: [[org.kiji.express.flow.ColumnRangeFilterSpec]] and
- * [[org.kiji.express.flow.RegexQualifierFilterSpec]].  Both of these filter the data
- * returned from a ColumnFamilyInputSpec by qualifier in some way.  These filters can be composed
- * with [[org.kiji.express.flow.AndFilterSpec]] and [[org.kiji.express.flow.OrFilterSpec]].
- *
- * To specify a range of qualifiers for the cells that should be returned.
- * {{{
- *     ColumnRangeFilterSpec(
- *         minimum = “c”,
- *         maximum = “m”,
- *         minimumIncluded = true,
- *         maximumIncluded = false)
- * }}}
- *
- * A `ColumnInputSpec` with the above filter specified will return all data from all  columns with
- * qualifiers “c” and later, up to but not including “m”.  You can omit any of the parameters, for
- * instance, you can write ``ColumnRangeFilterSpec(minimum = “m”, minimumIncluded = true)` to
- * specify columns with qualifiers “m” and later.
- *
- * To specify a regex for the qualifier names that you want data from:
- * {{{
- *     RegexQualifierFilterSpec(“http://.*”)
- * }}}
- * In this example, only data from columns whose qualifier names start with “http://” are returned.
- *
- * See the Sun Java documentation for regular expressions:
- * http://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html
- *
- * To compose filters using `AndFilterSpec`:
- * {{{
- *     AndFilterSpec(List(mRegexFilter, mQualifierFilter))
- * }}}
- * The `AndFilterSpec` composes a list of `FilterSpec`s, returning only data from columns that
- * satisfy all the filters in the `AndFilterSpec`.
- *
- * Analogously, you can compose them with `OrFilterSpec`:
- * {{{
- *     OrFilterSpec(List(mRegexFilter, mQualifierFilter))
- * }}}
- * This returns only data from columns that satisfy at least one of the filters.
- *
- * `OrFilterSpec` and `AndFilterSpec` can themselves be composed.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-sealed trait ColumnFilterSpec {
-  /** @return a KijiColumnFilter that corresponds to the Express column filter. */
-  private[kiji] def toKijiColumnFilter: KijiColumnFilter
-}
-
-/**
- * An Express column filter which combines a list of column filters using a logical "and" operator.
- *
- * See the scaladocs for [[org.kiji.express.flow.ColumnFilterSpec]] for information on other
- * filters.
- *
- * @param filters to combine with a logical "and" operation.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class AndFilterSpec(filters: Seq[ColumnFilterSpec])
-    extends ColumnFilterSpec {
-  private[kiji] override def toKijiColumnFilter: KijiColumnFilter = {
-    val schemaFilters = filters
-        .map { filter: ColumnFilterSpec => filter.toKijiColumnFilter }
-        .toArray
-
-    Filters.and(schemaFilters: _*)
-  }
-}
-
-/**
- * An Express column filter which combines a list of column filters using a logical "or" operator.
- *
- * See the scaladocs for [[org.kiji.express.flow.ColumnFilterSpec]] for information on other
- * filters.
- *
- * @param filters to combine with a logical "or" operation.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class OrFilterSpec(filters: Seq[ColumnFilterSpec])
-    extends ColumnFilterSpec {
-  private[kiji] override def toKijiColumnFilter: KijiColumnFilter = {
-    val orParams = filters
-        .map { filter: ColumnFilterSpec => filter.toKijiColumnFilter }
-        .toArray
-
-    Filters.or(orParams: _*)
-  }
-}
-
-/**
- * An Express column filter based on the given minimum and maximum qualifier bounds.
- *
- * See the scaladocs for [[org.kiji.express.flow.ColumnFilterSpec]] for information on other
- * filters.
- *
- * @param minimum qualifier bound.
- * @param maximum qualifier bound.
- * @param minimumIncluded determines if the lower bound is inclusive.
- * @param maximumIncluded determines if the upper bound is inclusive.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class ColumnRangeFilterSpec(
-    minimum: Option[String] = None,
-    maximum: Option[String] = None,
-    minimumIncluded: Boolean = true,
-    maximumIncluded: Boolean = false)
-    extends ColumnFilterSpec {
-  private[kiji] override def toKijiColumnFilter: KijiColumnFilter = {
-    new KijiColumnRangeFilter(
-        minimum.getOrElse { null },
-        minimumIncluded,
-        maximum.getOrElse { null },
-        maximumIncluded)
-  }
-}
-
-/**
- * An Express column filter which matches a regular expression against the full qualifier.
- *
- * See the scaladocs for [[org.kiji.express.flow.ColumnFilterSpec]] for information on other
- * filters.
- *
- * @param regex to match on.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class RegexQualifierFilterSpec(regex: String)
-    extends ColumnFilterSpec {
-  private[kiji] override def toKijiColumnFilter: KijiColumnFilter =
-      new RegexQualifierColumnFilter(regex)
-}
diff --git a/src/main/scala/org/kiji/express/flow/ColumnInputSpec.scala b/src/main/scala/org/kiji/express/flow/ColumnInputSpec.scala
deleted file mode 100644
index 66848857437e1d8addec51cad6789b8dfbea178c..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/ColumnInputSpec.scala
+++ /dev/null
@@ -1,396 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.apache.avro.Schema
-import org.apache.avro.specific.SpecificRecord
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiInvalidNameException
-
-/**
- * A request for data from a Kiji table. Provides access to options common to all types of column
- * input specs. There are two types of column input specs:
- * <ul>
- *   <li>
- *     [[org.kiji.express.flow.QualifiedColumnInputSpec]] - Requests versions of cells from an
- *     fully-qualified column.
- *   </li>
- *   <li>
- *     [[org.kiji.express.flow.ColumnFamilyInputSpec]] - Requests versions of cells from columns in
- *     a column family.
- *   </li>
- * </ul>
- *
- * Requested data will be represented as a sequence of flow cells (`Seq[FlowCell[T] ]`).
- *
- * Note: Subclasses of ColumnInputSpec are case classes that override ColumnInputSpec's abstract
- * methods (e.g., `schema`) with `val`s.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-sealed trait ColumnInputSpec {
-  /**
-   * Maximum number of cells to retrieve starting from the most recent cell. By default, only the
-   * most recent cell is retrieved.
-   *
-   * @return the maximum number of cells to retrieve.
-   */
-  def maxVersions: Int
-
-  /**
-   * Filter that a cell must pass in order to be retrieved. If `None`, no filter is used.
-   *
-   * @return `Some(filter)` if specified or `None`.
-   */
-  def filter: Option[ColumnFilterSpec]
-
-  /**
-   * Specifies the maximum number of cells to maintain in memory when paging through a column.
-   *
-   * @return the paging specification for this column.
-   */
-  def paging: PagingSpec
-
-  /**
-   * Specifies the schema that should be applied to the requested data.
-   *
-   * @return the schema that should be used for reading.
-   */
-  def schemaSpec: SchemaSpec
-
-  /**
-   * Column family of the requested data.
-   *
-   * @return the column family of the requested data.
-   */
-  def family: String
-
-  /**
-   * The [[org.kiji.schema.KijiColumnName]] of the requested data.
-   *
-   * @return the column name of the requested data.
-   */
-  def columnName: KijiColumnName
-}
-
-/**
- * Provides convenience factory methods for creating [[org.kiji.express.flow.ColumnInputSpec]]
- * instances.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-object ColumnInputSpec {
-  /**
-   * A request for data from a Kiji table column. The input spec will be for a qualified column if
-   * the column parameter contains a ':', otherwise the input will assumed to be for a column family
-   * (column family names cannot contain ';' characters).
-   *
-   * @param column name of the requested data.
-   * @param maxVersions to read back from the requested column (default is only most recent).
-   * @param filter to use when reading back cells (default is `None`).
-   * @param paging options specifying the maximum number of cells to retrieve from Kiji per page.
-   * @param schemaSpec specifies the schema to use when reading cells. Defaults to
-   *     [[org.kiji.express.flow.SchemaSpec.Writer]].
-   * @return a new column input spec with supplied options.
-   */
-  def apply(
-      column: String,
-      maxVersions: Int = latest,
-      filter: Option[ColumnFilterSpec] = None,
-      paging: PagingSpec = PagingSpec.Off,
-      schemaSpec: SchemaSpec = SchemaSpec.Writer
-  ): ColumnInputSpec = {
-    column.split(':') match {
-      case Array(family, qualifier) =>
-          QualifiedColumnInputSpec(
-              family,
-              qualifier,
-              maxVersions,
-              filter,
-              paging,
-              schemaSpec
-          )
-      case Array(family) =>
-          ColumnFamilyInputSpec(
-              family,
-              maxVersions,
-              filter,
-              paging,
-              schemaSpec
-          )
-      case _ => throw new IllegalArgumentException("column name must contain 'family:qualifier'" +
-        " for a group-type, or 'family' for a map-type column.")
-    }
-  }
-
-  /**
-   * A request for data from a Kiji table column. The input spec will be for a qualified column if
-   * the column parameter contains a ':', otherwise the input will assumed to be for a column family
-   * (column family names cannot contain ';' characters). Data will be read back as the specified
-   * avro class.
-   *
-   * @param column name of the requested data.
-   * @param specificRecord class to read from the column.
-   * @return a new column input spec with supplied options.
-   */
-  def apply(
-      column: String,
-      specificRecord: Class[_ <: SpecificRecord]
-  ): ColumnInputSpec = {
-    ColumnInputSpec(column, schemaSpec = SchemaSpec.Specific(specificRecord))
-  }
-
-  /**
-   * A request for data from a Kiji table column. The input spec will be for a qualified column if
-   * the column parameter contains a ':', otherwise the input will assumed to be for a column family
-   * (column family names cannot contain ';' characters). Data will be read back applying the
-   * specified avro schema.
-   *
-   * @param column name of the requested data.
-   * @param schema to apply to the data.
-   * @return a new column input spec with supplied options.
-   */
-  def apply(
-      column: String,
-      schema: Schema
-  ): ColumnInputSpec = {
-    ColumnInputSpec(column, schemaSpec = SchemaSpec.Generic(schema))
-  }
-}
-
-/**
- * Specifies a request for versions of cells from a fully-qualified column.
- *
- * Basic column example:
- * {{{
- *   // Request the latest version of data stored in the "info:name" column.
- *   val myColumnSpec: QualifiedColumnInputSpec =
- *       QualifiedColumnInputSpec(
- *           family = "info",
- *           qualifier = "name",
- *           maxVersions = 1
- *       )
- * }}}
- *
- * Paging can be enabled on a column input specification causing blocks of cells to be retrieved
- * from Kiji at a time:
- * {{{
- *   // Request cells from the "info:status" column retrieving 1000 cells per block.
- *   val myPagedColumn: QualifiedColumnInputSpec =
- *       QualifiedColumnInputSpec(
- *           family = "info",
- *           qualifier = "status",
- *           maxVersions = Int.MaxValue,
- *           paging = PagingSpec.Cells(1000)
- *       )
- * }}}
- *
- * If compiled avro classes are being used, a class that data should be read as can be specified:
- * {{{
- *   // Request cells from the "info:user" column containing User records.
- *   val myColumnSpec: QualifiedColumnInputSpec =
- *       QualifiedColumnInputSpec(
- *           family = "info",
- *           qualifier = "user",
- *           maxVersions = 1,
- *           schemaSpec = SchemaSpec.Specific(classOf[User])
- *       )
- * }}}
- *
- * @param family of columns the requested data belongs to.
- * @param qualifier of the column the requested data belongs to.
- * @param maxVersions to read back from the requested column (default is only most recent).
- * @param filter to use when reading back cells (default is `None`).
- * @param paging options specifying the maximum number of cells to retrieve from Kiji per page.
- * @param schemaSpec specifies the schema to use when reading cells. Defaults to
- *     [[org.kiji.express.flow.SchemaSpec.Writer]].
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class QualifiedColumnInputSpec(
-    family: String,
-    qualifier: String,
-    maxVersions: Int = latest,
-    filter: Option[ColumnFilterSpec] = None,
-    paging: PagingSpec = PagingSpec.Off,
-    schemaSpec: SchemaSpec = SchemaSpec.Writer
-) extends ColumnInputSpec {
-  override val columnName: KijiColumnName = new KijiColumnName(family, qualifier)
-}
-
-/**
- * Provides factory functions for creating [[org.kiji.express.flow.QualifiedColumnInputSpec]]
- * instances.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-object QualifiedColumnInputSpec {
-  /**
-   * Convenience function for creating a [[org.kiji.express.flow.QualifiedColumnInputSpec]] with
-   * a specific Avro record type.
-   *
-   * @param family of columns the requested data belongs to.
-   * @param qualifier of the column the requested data belongs to.
-   * @param specificRecord class to read from the column.
-   * @return a new column input spec with supplied options.
-   */
-  def apply(
-      family: String,
-      qualifier: String,
-      specificRecord: Class[_ <: SpecificRecord]
-  ): QualifiedColumnInputSpec = {
-    QualifiedColumnInputSpec(family, qualifier, schemaSpec = SchemaSpec.Specific(specificRecord))
-  }
-
-  /**
-   * Convenience function for creating a [[org.kiji.express.flow.QualifiedColumnInputSpec]] with
-   * a generic Avro type specified by a [[org.apache.avro.Schema]].
-   *
-   * @param family of columns the requested data belongs to.
-   * @param qualifier of the column the requested data belongs to.
-   * @param schema of generic Avro type to read from the column.
-   * @return a new column input spec with supplied options.
-   */
-  def apply(
-      family: String,
-      qualifier: String,
-      schema: Schema
-  ): QualifiedColumnInputSpec = {
-    QualifiedColumnInputSpec(family, qualifier, schemaSpec = SchemaSpec.Generic(schema))
-  }
-}
-
-/**
- * Specifies a request for versions of cells from a column family.
- *
- * Basic column family example:
- * {{{
- *   // Request the latest version of data stored in the "matrix" column family.
- *   val myColumnFamilySpec: ColumnFamilyInputSpec =
- *       ColumnFamilyInputSpec(
- *           family = "matrix",
- *           maxVersions = 1
- *       )
- * }}}
- *
- * Filters can be applied to the column qualifier of cells in a column family.
- * {{{
- *   // Request cells from the "hits" column that are from columns with qualifiers that begin with
- *   // the string "http://www.wibidata.com/".
- *   val myFilteredColumnSpec: ColumnFamilyInputSpec =
- *       ColumnFamilyInputSpec(
- *           family = "hits",
- *           maxVersions = Int.MaxValue,
- *           filter = RegexQualifierFilterSpec("http://www\.wibidata\.com/.*")
- *       )
- * }}}
- *
- * Paging can be enabled on a column input specification causing blocks of cells to be retrieved
- * from Kiji at a time:
- * {{{
- *   // Request cells from the "metadata" column family retrieving 1000 cells per block.
- *   val myPagedColumn: ColumnFamilyInputSpec =
- *       ColumnFamilyInputSpec(
- *           family = "metadata",
- *           maxVersions = Int.MaxValue,
- *           paging = PagingSpec.Cells(1000)
- *       )
- * }}}
- *
- * If compiled avro classes are being used, a class that data should be read as can be specified:
- * {{{
- *   // Request cells from the "users" column family containing User records.
- *   val myColumnSpec: ColumnFamilyInputSpec =
- *       ColumnFamilyInputSpec(
- *           family = "users",
- *           maxVersions = 1,
- *           schemaSpec = SchemaSpec.Specific(classOf[User])
- *       )
- * }}}
- *
- * @param family of columns the requested data belongs to.
- * @param maxVersions to read back from the requested column family (default is only most recent).
- * @param filter to use when reading back cells (default is `None`).
- * @param paging options specifying the maximum number of cells to retrieve from Kiji per page.
- * @param schemaSpec specifies the schema to use when reading cells. Defaults to
- *     [[org.kiji.express.flow.SchemaSpec.Writer]].
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class ColumnFamilyInputSpec(
-    family: String,
-    maxVersions: Int = latest,
-    filter: Option[ColumnFilterSpec] = None,
-    paging: PagingSpec = PagingSpec.Off,
-    schemaSpec: SchemaSpec = SchemaSpec.Writer
-) extends ColumnInputSpec {
-  if (family.contains(':')) {
-    throw new KijiInvalidNameException("Cannot have a ':' in family name for column family request")
-  }
-  override val columnName: KijiColumnName = new KijiColumnName(family)
-}
-
-/**
- * Provides factory functions for creating [[org.kiji.express.flow.ColumnFamilyInputSpec]]
- * instances.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-object ColumnFamilyInputSpec {
-  /**
-   * Convenience function for creating a [[org.kiji.express.flow.ColumnFamilyInputSpec]] with a
-   * specific Avro record type.
-   *
-   * @param family of columns the requested data belongs to.
-   * @param specificRecord class to read from the column.
-   * @return a new column input spec with supplied options.
-   */
-  def apply(
-      family: String,
-      specificRecord: Class[_ <: SpecificRecord]
-  ): ColumnFamilyInputSpec = {
-    ColumnFamilyInputSpec(family, schemaSpec = SchemaSpec.Specific(specificRecord))
-  }
-
-  /**
-   * Convenience function for creating a [[org.kiji.express.flow.ColumnFamilyInputSpec]] with a
-   * generic Avro type specified by a [[org.apache.avro.Schema]].
-   *
-   * @param family of columns the requested data belongs to.
-   * @param schema of Avro type to read from the column.
-   * @return a new column input spec with supplied options.
-   */
-  def apply(
-      family: String,
-      schema: Schema
-  ): ColumnFamilyInputSpec = {
-    ColumnFamilyInputSpec(family, schemaSpec = SchemaSpec.Generic(schema))
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/ColumnOutputSpec.scala b/src/main/scala/org/kiji/express/flow/ColumnOutputSpec.scala
deleted file mode 100644
index 05df82a032aceb6a872ee0a1b1aeb416855f3865..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/ColumnOutputSpec.scala
+++ /dev/null
@@ -1,272 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.apache.avro.Schema
-import org.apache.avro.specific.SpecificRecord
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.SchemaSpec.Generic
-import org.kiji.express.flow.SchemaSpec.Specific
-import org.kiji.express.flow.SchemaSpec.Writer
-import org.kiji.express.flow.util.AvroUtil
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiInvalidNameException
-
-/**
- * Interface for all column output specification objects. ColumnOutputSpec
- * implementations specify how to write individual fields in an Express flow to a Kiji column or
- * column family.
- *
- * Use the [[org.kiji.express.flow.QualifiedColumnOutputSpec]] to write a field to an individual
- * Kiji column.
- *
- * Use the [[org.kiji.express.flow.ColumnFamilyOutputSpec]] to write a field to a column
- * family, with a qualifier determined as part of the flow.  The qualifier should be written to a
- * field specified as part of the ColumnFamilyOutputSpec.
- *
- * Note that the subclasses of ColumnOutputSpec are case classes, and so they override
- * ColumnOutputSpec's abstract methods (e.g., schema) with vals.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-trait ColumnOutputSpec {
-
-  /**
-   * Family which this [[org.kiji.express.flow.ColumnOutputSpec]] belongs to.
-   *
-   * @return family name of output column.
-   */
-  def family: String
-
-  /**
-   * [[org.kiji.schema.KijiColumnName]] of this [[org.kiji.express.flow.ColumnOutputSpec]].
-   *
-   *  @return the name of the column to output to.
-   */
-  def columnName: KijiColumnName
-
-  /**
-   * Specifies the schema of data to be written to the column.
-   * @return the schema specification of data written to the column.
-   */
-  def schemaSpec: SchemaSpec
-
-  /**
-   * Make a best effort attempt to encode a provided value to a type that will be compatible with
-   * the column.  If no such conversion can be made, the original value will be returned.
-   */
-  private[express] def encode: Any => Any = {
-    schemaSpec.schema.map(AvroUtil.avroEncoder).getOrElse(identity)
-  }
-}
-
-/**
- * Specification for writing to a Kiji column.
- *
- * @param family of the output column.
- * @param qualifier of the output column.
- * @param schemaSpec The schema specification with which to write values. By default uses
- *     [[org.kiji.express.flow.SchemaSpec.Writer]].
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class QualifiedColumnOutputSpec(
-    family: String,
-    qualifier: String,
-    schemaSpec: SchemaSpec = Writer
-) extends ColumnOutputSpec {
-  override val columnName: KijiColumnName = new KijiColumnName(family, qualifier)
-}
-
-/**
- * Provides factory functions for creating [[org.kiji.express.flow.QualifiedColumnOutputSpec]]
- * instances.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-object QualifiedColumnOutputSpec {
-  /**
-   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]] with a
-   * generic Avro writer schema.
-   *
-   * @param family of the output column.
-   * @param qualifier of the output column.
-   * @param schema with which to write data.
-   */
-  def apply(
-    family: String,
-    qualifier: String,
-    schema: Schema
-  ): QualifiedColumnOutputSpec = {
-    QualifiedColumnOutputSpec(family, qualifier, Generic(schema))
-  }
-
-  /**
-   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]] with a
-   * specific Avro record writer schema.
-   *
-   * @param family of the output column.
-   * @param qualifier of the output column.
-   * @param specificClass of Avro record with which to write.
-   */
-  def apply(
-    family: String,
-    qualifier: String,
-    specificClass: Class[_ <: SpecificRecord]
-  ): QualifiedColumnOutputSpec = {
-    QualifiedColumnOutputSpec(family, qualifier, Specific(specificClass))
-  }
-
-  /**
-   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]].
-   * This constructor takes a column string which must contain the column family and qualifier
-   * in the form 'family:qualifier'.
-   *
-   * @param column The output family and column in format 'family:column'.
-   * @param schemaSpec specification with which to write data.
-   */
-  def apply(
-      column: String,
-      schemaSpec: SchemaSpec
-  ): QualifiedColumnOutputSpec = {
-    column.split(':').toList match {
-      case family :: qualifier :: Nil => QualifiedColumnOutputSpec(family, qualifier, schemaSpec)
-      case _ => throw new IllegalArgumentException(
-          "Must specify column to GroupTypeOutputColumnSpec in the format 'family:qualifier'.")
-    }
-  }
-
-  /**
-   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]] with
-   * a generic Avro writer schema. This constructor takes a column string which must contain the
-   * column family and qualifier in the form 'family:qualifier'.
-   *
-   * @param column The output family and column in format 'family:column'.
-   * @param schema with which to write data.
-   */
-  def apply(
-      column: String,
-      schema: Schema
-  ): QualifiedColumnOutputSpec = {
-    QualifiedColumnOutputSpec(column, Generic(schema))
-  }
-
-  /**
-   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]] with
-   * a generic Avro writer schema. This constructor takes a column string which must contain the
-   * column family and qualifier in the form 'family:qualifier'.
-   *
-   * @param column The output family and column in format 'family:column'.
-   * @param specificClass of Avro record with which to write.
-   */
-  def apply(
-    column: String,
-    specificClass: Class[_ <: SpecificRecord]
-  ): QualifiedColumnOutputSpec = {
-    QualifiedColumnOutputSpec(column, Specific(specificClass))
-  }
-
-  /**
-   * Factory function for creating a [[org.kiji.express.flow.QualifiedColumnOutputSpec]] with
-   * the [[org.kiji.express.flow.SchemaSpec.Writer]] schema spec. This constructor takes a column
-   * string which must contain the column family and qualifier in the form 'family:qualifier'.
-   *
-   * @param column The output family and column in format 'family:column'.
-   */
-  def apply(
-    column: String
-  ): QualifiedColumnOutputSpec = {
-    QualifiedColumnOutputSpec(column, Writer)
-  }
-}
-
-/**
- * Specification for writing to a Kiji column family.
- *
- * @param family of the output column.
- * @param qualifierSelector The field in the Express flow indicating the qualifier of the
- *     output column.
- * @param schemaSpec The schema spec to use for writing data. By default uses
- *     [[org.kiji.express.flow.SchemaSpec.Writer]].
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class ColumnFamilyOutputSpec(
-    family: String,
-    qualifierSelector: Symbol,
-    schemaSpec: SchemaSpec = Writer
-) extends ColumnOutputSpec {
-  if (family.contains(':')) {
-    throw new KijiInvalidNameException(
-        "family name in ColumnFamilyOutputSpec may not contain a ':'")
-  }
-  override def columnName: KijiColumnName = new KijiColumnName(family)
-}
-
-/**
- * Provides factory functions for creating [[org.kiji.express.flow.ColumnFamilyOutputSpec]]
- * instances.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-object ColumnFamilyOutputSpec {
-  /**
-   * Factory function for creating a [[org.kiji.express.flow.ColumnFamilyOutputSpec]] with a
-   * generic Avro writer schema.
-   *
-   * @param family of the output column.
-   * @param qualifierSelector The field in the Express flow indicating the qualifier of the
-   *     output column.
-   * @param schema The schema to use for writing values.
-   */
-  def apply(
-      family: String,
-      qualifierSelector: Symbol,
-      schema: Schema
-  ): ColumnFamilyOutputSpec = {
-    ColumnFamilyOutputSpec(family, qualifierSelector, Generic(schema))
-  }
-
-  /**
-   * Factory function for creating a [[org.kiji.express.flow.ColumnFamilyOutputSpec]] with a
-   * specific Avro record writer schema.
-   *
-   * @param family of the output column.
-   * @param qualifierSelector The field in the Express flow indicating the qualifier of the
-   *     output column.
-   * @param specificClass The specific record class to use for writes.
-   */
-  def apply(
-    family: String,
-    qualifierSelector: Symbol,
-    specificClass: Class[_ <: SpecificRecord]
-  ): ColumnFamilyOutputSpec = {
-    ColumnFamilyOutputSpec(family, qualifierSelector, Specific(specificClass))
-  }
-}
-
diff --git a/src/main/scala/org/kiji/express/flow/EntityId.scala b/src/main/scala/org/kiji/express/flow/EntityId.scala
deleted file mode 100644
index cb189a14e03c266dea917ae6b220f10f6eda6471..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/EntityId.scala
+++ /dev/null
@@ -1,293 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import java.lang.IllegalStateException
-
-import scala.collection.JavaConverters.asScalaBufferConverter
-import scala.collection.JavaConverters.seqAsJavaListConverter
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.EntityIdFactory
-import org.kiji.schema.{EntityId => JEntityId}
-
-/**
- * An entity ID, or row key, that can be used to address a row in a Kiji table. This is the
- * Express representation of a [[org.kiji.schema.EntityId]].
- *
- * When writing to a Kiji table using KijiExpress, the 'entityId field of each tuple must contain a
- * single [[org.kiji.express.flow.EntityId]].  The rest of the tuple fields will be written to the
- * table according to the field -> column mapping specified by the user in
- * [[org.kiji.express.flow.KijiOutput]], for the same row that the 'entityId indicated.
- *
- * Users can create EntityIds by passing in the objects that compose it. For example, if a Kiji
- * table uses formatted row keys composed of a string as their first component and a long as the
- * second, the user can create this as:
- * {{{
- * EntityId("myString", 1L)
- * }}}
- *
- * When reading from a Kiji table using [[org.kiji.express.flow.KijiInput]], each row is read into a
- * tuple, and the 'entityId field of each tuple is automatically populated with an instance of
- * [[org.kiji.express.flow.EntityId]] corresponding to the EntityID of that row.
- *
- * Users can retrieve the index'th element of an EntityId (0-based), as follows:
- * {{{
- * MyEntityId(index)
- * }}}
- *
- * EntityIds can either be [[org.kiji.express.flow.EntityId.MaterializedEntityId]] (in the case of
- * EntityIds from tables with formatted row keys, and all user-created EntityIds), or
- * [[org.kiji.express.flow.EntityId.HashedEntityId]] (in the case of EntityIds from tables with
- * hashed or materialization-suppressed row keys).
- *
- * Note for joining on EntityIds: MaterializedEntityIds can be compared with each other.
- * HashedEntityIds can be compared with other HashedEntityIds, but only their hash values are
- * compared.  You should only attempt to join on HashedEntityIds if they are from the same table.
- * If you join two pipes on EntityIds, and one of them comes straight from a table and contains
- * HashedEntityIds, while the other only has the components of the EntityIds, you can't compare an
- * EntityId constructed directly from the components with the HashedEntityId.  Instead, you need to
- * construct an EntityId containing the hash of the components according to the table your
- * HashedEntityId is from.  You can do this using [[org.kiji.schema.KijiTable#getEntityId]].
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-trait EntityId extends Product with Ordered[EntityId] {
-  /**
-   * Get the Java [[org.kiji.schema.EntityId]] associated with this Scala EntityId.
-   *
-   * @param eidFactory is the EntityIdFactory used to convert the underlying components to a Java
-   *     EntityId.
-   * @return the Java EntityId backing this EntityId.
-   */
-  def toJavaEntityId(eidFactory: EntityIdFactory): JEntityId
-
-  /**
-   * Get the index'th component of the EntityId.
-   *
-   * @param index of the component to retrieve.
-   * @return the component at index.
-   */
-  def apply(index: Int): Any = components(index)
-
-  override def productPrefix: String = "EntityId"
-
-  override def canEqual(that: Any): Boolean = that.isInstanceOf[EntityId]
-
-  /**
-   * Returns whether this is equal to `other`.  EntityIds are only comparable to other EntityIds.
-   *
-   * HashedEntityIds can be compared with other HashedEntityIds, but only their hash values are
-   * compared.  You should only attempt to join on HashedEntityIds if they are from the same table.
-   * If you join two pipes on EntityIds, and one of them comes straight from a table and contains
-   * HashedEntityIds, while the other only has the components of the EntityIds, you can't compare an
-   * EntityId constructed directly from the components with the HashedEntityId.  Instead, you need
-   * to construct an EntityId containing the hash of the components according to the table your
-   * HashedEntityId is from.  You can do this using [[org.kiji.schema.KijiTable#getEntityId]].
-   *
-   * @param other object to compare this to.
-   * @return whether the two objects are "equal" according to the definition in this scaladoc.
-   */
-  override def equals(other: Any): Boolean = {
-    if(other.isInstanceOf[EntityId]) {
-      compare(other.asInstanceOf[EntityId]) == 0
-    } else {
-      false
-    }
-  }
-
-  /**
-   * Returns the hashcode of the underlying entityId. For a materialized entity id it returns
-   * the hashcode of the underlying list of components. For a hashed entityId it, it returns
-   * the hashcode of the encoded byte array wrapped as a string.
-   *
-   * @return entityId hashcode.
-   */
-  override def hashCode(): Int
-
-  /**
-   * Returns the underlying components of this entityId. For a materialized entityId, this will
-   * be the list of components. For a hashed entityId, this will be a singleton list containing
-   * the encoded byte[].
-   *
-   * @return a list of the underlying components.
-   */
-  def components: Seq[AnyRef]
-
-  /**
-   * Returns the comparison result ( > 0, 0, < 0).
-   *
-   * MaterializedEntityIds can be compared with each other.
-   * HashedEntityIds can be compared with other HashedEntityIds, but only their hash values are
-   * compared.  You should only attempt to join on HashedEntityIds if they are from the same table.
-   * If you join two pipes on EntityIds, and one of them comes straight from a table and contains
-   * HashedEntityIds, while the other only has the components of the EntityIds, you can't compare an
-   * EntityId constructed directly from the components with the HashedEntityId.  Instead, you need
-   * to construct an EntityId containing the hash of the components according to the table your
-   * HashedEntityId is from.  You can do this using [[org.kiji.schema.KijiTable#getEntityId]].
-   *
-   * @return the comparison result ( > 0, 0, < 0).
-   */
-  override def compare(rhs: EntityId): Int = {
-    val zipped = this.components.zip(rhs.components)
-    // Compare each element lexicographically.
-    zipped.foreach {
-      case (mine, theirs) => {
-        try {
-          val compareResult =
-            if (mine.isInstanceOf[Array[Byte]] && theirs.isInstanceOf[Array[Byte]]) {
-              val myArray = mine.asInstanceOf[Array[Byte]]
-              val rhsArray = theirs.asInstanceOf[Array[Byte]]
-              new String(myArray).compareTo(new String(rhsArray))
-            } else {
-              mine.asInstanceOf[Comparable[Any]].compareTo(theirs)
-            }
-          if (compareResult != 0) {
-            // Return out of the function if these two elements are not equal.
-            return compareResult
-          }
-          // Otherwise, continue.
-        } catch {
-          case e: ClassCastException =>
-            throw new EntityIdFormatMismatchException(components, rhs.components)
-        }
-      }
-    }
-    // If all elements in "zipped" were equal, we compare the lengths.
-    this.components.length.compare(rhs.components.length)
-  }
-}
-
-/**
- * Companion object for EntityId. Provides factory methods and implementations for EntityIds.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-object EntityId {
-  /**
-   * Creates a KijiExpress EntityId from a Java EntityId.  This is used internally to convert
-   * between kiji-schema and kiji-express.
-   *
-   * Users should not need to use this method.
-   *
-   * @param entityId is the Java EntityId to convert.
-   */
-  @ApiAudience.Framework
-  def fromJavaEntityId(entityId: JEntityId): EntityId = {
-    val hbaseKey = entityId.getHBaseRowKey()
-
-    try {
-      val components = entityId
-        .getComponents
-        .asScala
-        .toSeq
-      MaterializedEntityId(components)
-    } catch {
-      // This is an exception thrown when we try to access components of an entityId which has
-      // materialization suppressed. E.g. Hashed EntityIds. So we are unable to retrieve components,
-      // but the behavior is legal.
-      case ise: IllegalStateException => {
-        HashedEntityId(hbaseKey)
-      }
-    }
-  }
-
-  /**
-   * Creates a new EntityId with the components specified by the user.
-   *
-   * @param components of the EntityId to create.
-   *
-   * @return the created entity id.
-   */
-  def apply(components: Any*): EntityId = {
-    MaterializedEntityId(components.toSeq.map { _.asInstanceOf[AnyRef] })
-  }
-
-  /**
-   * Creates a new EntityId given an array of bytes representing the raw
-   * HBase rowkey.
-   *
-   * @param encoded is the raw hbase rowkey.
-   *
-   * @return the created entity id.
-   */
-  def apply(encoded: Array[Byte]): EntityId = {
-    HashedEntityId(encoded)
-  }
-
-  /**
-   * An EntityId that does not provide access to its components.  It only contains the encoded hash.
-   *
-   * These are never user-created.  They are constructed by KijiExpress when reading from a table
-   * with row key format HASHED or with suppress-materialization enabled.
-   *
-   * @param encoded byte array representation of this EntityId.
-   */
-  @ApiAudience.Private
-  @ApiStability.Experimental
-  @Inheritance.Sealed
-  private[express] case class HashedEntityId(encoded: Array[Byte])
-      extends EntityId {
-
-    /** Lazily create a string encoding of this byte array for hash code purposes. **/
-    @transient
-    private lazy val stringEncoding = new String(encoded)
-
-    /** Lazily create a memoized list of components for the components method. **/
-    @transient
-    override lazy val components: Seq[AnyRef] = List(encoded)
-
-    override def hashCode(): Int = {
-      stringEncoding.hashCode
-    }
-
-    override def toJavaEntityId(eidFactory: EntityIdFactory): JEntityId = {
-      eidFactory.getEntityIdFromHBaseRowKey(components(0).asInstanceOf[Array[Byte]])
-    }
-  }
-
-  /**
-   * An EntityId that provides access to its components.  It can be constructed by the user:
-   *
-   * {{{EntityId(component1, component2, component3)}}}
-   *
-   * KijiExpress will return an instance of this class in the 'entityId field of the tuple if the
-   * row key format in the layout of the table supports returning the components of the EntityId.
-   *
-   * @param components of an EntityId.
-   */
-  @ApiAudience.Private
-  @ApiStability.Experimental
-  @Inheritance.Sealed
-  private[express] case class MaterializedEntityId(override val components: Seq[AnyRef])
-      extends EntityId {
-
-    override def toJavaEntityId(eidFactory: EntityIdFactory): JEntityId = {
-      eidFactory.getEntityId(components.asJava)
-    }
-
-    override def hashCode(): Int = {
-      components.hashCode
-    }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/EntityIdFormatMismatchException.scala b/src/main/scala/org/kiji/express/flow/EntityIdFormatMismatchException.scala
deleted file mode 100644
index 5d01ec47fe48f27e1466dbd24c89d8d61f370550..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/EntityIdFormatMismatchException.scala
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-
-/**
- * A runtime exception thrown when two EntityIds with different formats are compared.
- *
- * @param thisComponents components of one of the EntityIds that is mismatched.
- * @param thatComponents components of the other of the EntityIds that is mismatched.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-class EntityIdFormatMismatchException(thisComponents: Seq[Any],thatComponents: Seq[Any])
-    extends RuntimeException("Mismatched Formats: %s and  %s do not match.".format(
-        "Components: [%s]".format(thisComponents.map { _.getClass.getName }.mkString(",")),
-        "Components: [%s]".format(thatComponents.map { _.getClass.getName }.mkString(","))))
diff --git a/src/main/scala/org/kiji/express/flow/FlowCell.scala b/src/main/scala/org/kiji/express/flow/FlowCell.scala
deleted file mode 100644
index b79335740d092920037df978f8a024c25a238e48..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/FlowCell.scala
+++ /dev/null
@@ -1,123 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import scala.annotation.implicitNotFound
-
-import org.apache.hadoop.hbase.HConstants
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.util.AvroUtil
-import org.kiji.schema.KijiCell
-
-/**
- * A container for data from a Kiji table. Contains some datum tagged with a column family, column
- * qualifier, and version. Flow cells are provided when requesting a Kiji table column or column
- * family.
- *
- * Example of accesssing data stored within a flow cell:
- * {{{
- *   // Extracts the data stored within cell.
- *   val myData: T = cell.datum
- *
- *   // Extracts the family, qualifier, and version of the cell.
- *   val myFamily: String = cell.family
- *   val myQualifier: String = cell.qualifier
- *   val myVersion: Long = cell.version
- * }}}
- *
- * @tparam T is the type of the datum in the cell.
- * @param family of columns that this cell comes from.
- * @param qualifier of the column that this cell comes from.
- * @param version of the column data that this cell represents. Defaults to the latest timestamp.
- * @param datum stored in this cell.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-case class FlowCell[T] (
-    family: String,
-    qualifier: String,
-    version: Long = HConstants.LATEST_TIMESTAMP,
-    datum: T)
-
-/**
- * Companion object containing factory methods for creating flow cells and orderings for sorting
- * cells.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-object FlowCell {
-  /**
-   * Creates an object that contains the coordinates (family, qualifier, and version) of data in a
-   * Kiji table along with the data itself.
-   *
-   * @tparam T is the type of the datum that this cell contains.
-   * @param cell from a Kiji table produced by the Java API.
-   * @return a FlowCell for use in KijiExpress containing the same family, qualifier, version, and
-   *     datum as the provided KijiCell.
-   */
-  private[kiji] def apply[T](cell: KijiCell[T]): FlowCell[T] = {
-    new FlowCell[T](
-        cell.getFamily,
-        cell.getQualifier,
-        cell.getTimestamp.longValue,
-        AvroUtil.avroToScala(cell.getData).asInstanceOf[T])
-  }
-
-  /**
-   * Provides an implementation of the `scala.Ordering` trait that sorts
-   * [[org.kiji.express.flow.FlowCell]]s by value.
-   *
-   * @tparam T is the type of the datum in the [[org.kiji.express.flow.FlowCell]].
-   * @return an ordering that sorts cells by their value.
-   */
-  @implicitNotFound("The type of the datum in the cells is not Orderable. You may be trying to " +
-      "order a cell that contains a complex type (such as an avro record).")
-  implicit def valueOrder[T](implicit order: Ordering[T]): Ordering[FlowCell[T]] = {
-    Ordering.by { cell: FlowCell[T] => cell.datum }
-  }
-
-  /**
-   * Provides an implementation of the `scala.Ordering` trait that sorts
-   * [[org.kiji.express.flow.FlowCell]]s by version.
-   *
-   * @tparam T is the type of the datum in the [[org.kiji.express.flow.FlowCell]].
-   * @return an ordering that sorts cells by version.
-   */
-  def versionOrder[T]: Ordering[FlowCell[T]] = {
-    Ordering.by { cell: FlowCell[T] => cell.version }
-  }
-
-  /**
-   * Provides an implementation of the `scala.Ordering` trait that sorts
-   * [[org.kiji.express.flow.FlowCell]]s first by the cell's family and then by it's qualifier.
-   *
-   * @tparam T is the type of the datum in the [[org.kiji.express.flow.FlowCell]].
-   * @return an ordering that sorts cells by qualifier.
-   */
-  def qualifierOrder[T]: Ordering[FlowCell[T]] = {
-    Ordering.by { cell: FlowCell[T] =>
-      (cell.family, cell.qualifier)
-    }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/InvalidKijiTapException.scala b/src/main/scala/org/kiji/express/flow/InvalidKijiTapException.scala
deleted file mode 100644
index 74d4efa153f1a556b9d68e9482c64b606f748b99..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/InvalidKijiTapException.scala
+++ /dev/null
@@ -1,34 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-
-/**
- * This exception is thrown when a KijiTap cannot validate that the Kiji tables or columns it
- * requires exist.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final class InvalidKijiTapException(message: String)
-    extends RuntimeException(message)
diff --git a/src/main/scala/org/kiji/express/flow/KijiInput.scala b/src/main/scala/org/kiji/express/flow/KijiInput.scala
deleted file mode 100644
index 3bc3d502b9a02962d8fa8d90993a4a1b1687907b..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/KijiInput.scala
+++ /dev/null
@@ -1,198 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-
-/**
- * Factory methods for constructing [[org.kiji.express.flow.KijiSource]]s that will be used as
- * inputs to a KijiExpress flow. Two basic APIs are provided with differing complexity.
- *
- * Simple:
- * {{{
- *   // Create a KijiSource that reads from the table named `mytable` reading the columns
- *   // `info:column1` and `info:column2` to the fields `'column1` and `'column2`.
- *   KijiInput(
- *       tableUri = "kiji://localhost:2181/default/mytable",
- *       "info:column1" -> 'column1,
- *       "info:column2" -> 'column2)
- * }}}
- *
- * Verbose:
- * {{{
- *   // Create a KijiSource that reads from the table named `mytable` reading the columns
- *   // `info:column1` and `info:column2` to the fields `'column1` and `'column2`.
- *   KijiInput(
- *       tableUri = "kiji://localhost:2181/default/mytable",
- *       columns = Map(
- *           QualifiedColumnInputSpec("info", "column1") -> 'column1,
- *           QualifiedColumnInputSpec("info", "column2") -> 'column2)
- * }}}
- *
- * The verbose methods allow you to instantiate explicity
- * [[org.kiji.express.flow.QualifiedColumnInputSpec]] and
- * [[org.kiji.express.flow.ColumnInputSpec]] objects.
- * Use the verbose method to specify options for the input columns, e.g.,
- * {{{
- *   // Create a KijiSource that reads from the table named `mytable` reading the columns
- *   // `info:column1` and `info:column2` to the fields `'column1` and `'column2`.
- *   KijiInput(
- *       tableUri = "kiji://localhost:2181/default/mytable",
- *       columns = Map(
- *           QualifiedColumnInputSpec("info", "column1", maxVersions=5) -> 'column1,
- *           QualifiedColumnInputSpec("info", "column2", paging=Cells(10)) -> 'column2)
- * }}}
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-object KijiInput {
-  /** Default time range for KijiSource */
-  private val defaultTimeRange: TimeRange = All
-
-  /**
-   * An internal factory method for creating a [[org.kiji.express.flow.KijiSource]] for reading
-   * cells from a Kiji table.
-   *
-   * @param tableUri addressing a table in a Kiji instance.
-   * @param timeRange that cells must fall into to be retrieved.
-   * @param columns are a series of pairs mapping column input specs to tuple field names.
-   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
-   *     simply "family".
-   * @return a source for data in the Kiji table, whose row tuples will contain fields with cell
-   *     data from the requested columns and map-type column families.
-   */
-  private def applyAllArgsSym(
-      tableUri: String,
-      timeRange: TimeRange,
-      columns: (String, Symbol)*
-  ): KijiSource = {
-    val columnMap = columns
-        .map { case (col, field) => (field, ColumnInputSpec(col)) }
-        .toMap
-
-    new KijiSource(
-        tableUri,
-        timeRange,
-        None,
-        inputColumns = columnMap
-    )
-  }
-
-  /**
-   * An internal factory method for creating a [[org.kiji.express.flow.KijiSource]] for
-   * reading cells from a Kiji table.
-   *
-   * @param tableUri addressing a table in a Kiji instance.
-   * @param timeRange that cells must fall into to be retrieved.
-   * @param columns are a series of pairs mapping column input specs to tuple field names.
-   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
-   *     simply "family".
-   * @return a source for data in the Kiji table, whose row tuples will contain fields with
-   *     cell data from the requested columns and map-type column families.
-   */
-  private def applyAllArgsMap(
-      tableUri: String,
-      timeRange: TimeRange,
-      columns: Map[_ <: ColumnInputSpec, Symbol]
-  ): KijiSource = {
-    val columnMap = columns
-        .map { entry: (ColumnInputSpec, Symbol) => entry.swap }
-
-    new KijiSource(
-        tableUri,
-        timeRange,
-        None,
-        inputColumns = columnMap
-    )
-  }
-
-  /**
-   * A factory method for creating a KijiSource.
-   *
-   * @param tableUri addressing a table in a Kiji instance.
-   * @param columns are a series of pairs mapping column input specs to tuple field names.
-   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
-   *     simply "family".
-   * @return a source for data in the Kiji table, whose row tuples will contain fields with cell
-   *     data from the requested columns and map-type column families.
-   */
-  def apply(
-      tableUri: String,
-      columns: (String, Symbol)*
-  ): KijiSource = {
-    applyAllArgsSym(tableUri, defaultTimeRange, columns: _*)
-  }
-
-  /**
-   * A factory method for creating a KijiSource.
-   *
-   * @param tableUri addressing a table in a Kiji instance.
-   * @param timeRange that cells must fall into to be retrieved.
-   * @param columns are a series of pairs mapping column input specs to tuple field names.
-   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
-   *     simply "family".
-   * @return a source for data in the Kiji table, whose row tuples will contain fields with cell
-   *     data from the requested columns and map-type column families.
-   */
-  def apply(
-      tableUri: String,
-      timeRange: TimeRange,
-      columns: (String, Symbol)*
-  ): KijiSource = {
-    applyAllArgsSym(tableUri, timeRange, columns: _*)
-  }
-
-  /**
-   * A factory method for creating a KijiSource.
-   *
-   * @param tableUri addressing a table in a Kiji instance.
-   * @param columns are a series of pairs mapping column input specs to tuple field names.
-   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
-   *     simply "family".
-   * @return a source for data in the Kiji table, whose row tuples will contain fields with cell
-   *     data from the requested columns and map-type column families.
-   */
-  def apply(
-      tableUri: String,
-      columns: Map[_ <: ColumnInputSpec, Symbol]
-  ): KijiSource = {
-    applyAllArgsMap(tableUri, defaultTimeRange, columns)
-  }
-
-  /**
-   * A factory method for creating a KijiSource.
-   *
-   * @param tableUri addressing a table in a Kiji instance.
-   * @param timeRange that cells must fall into to be retrieved.
-   * @param columns are a series of pairs mapping column input specs to tuple field names.
-   *     Columns are specified as "family:qualifier" or, in the case of a column family input spec,
-   *     simply "family".
-   * @return a source for data in the Kiji table, whose row tuples will contain fields with cell
-   *     data from the requested columns and map-type column families.
-   */
-  def apply(
-      tableUri: String,
-      timeRange: TimeRange,
-      columns: Map[_ <: ColumnInputSpec, Symbol]
-  ): KijiSource = {
-    applyAllArgsMap(tableUri, timeRange, columns)
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/KijiJob.scala b/src/main/scala/org/kiji/express/flow/KijiJob.scala
deleted file mode 100644
index b2bb164646521b4add511879a65deec546b47785..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/KijiJob.scala
+++ /dev/null
@@ -1,115 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import scala.collection.JavaConverters.collectionAsScalaIterableConverter
-
-import java.util.Properties
-
-import cascading.flow.hadoop.util.HadoopUtil
-import cascading.tap.Tap
-import com.twitter.scalding.Args
-import com.twitter.scalding.HadoopTest
-import com.twitter.scalding.Hdfs
-import com.twitter.scalding.Job
-import com.twitter.scalding.Mode
-import org.apache.hadoop.conf.Configuration
-import org.apache.hadoop.hbase.HBaseConfiguration
-import org.apache.hadoop.mapred.JobConf
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.framework.KijiTap
-import org.kiji.express.flow.framework.LocalKijiTap
-import org.kiji.express.flow.util.AvroTupleConversions
-import org.kiji.express.flow.util.PipeConversions
-
-/**
- * KijiJob is KijiExpress's extension of Scalding's `Job`, and users should extend it when writing
- * their own jobs in KijiExpress.  It provides extra conversions that Express needs for KijiPipes.
- *
- * @param args to the job. These get parsed in from the command line by Scalding.  Within your own
- *     KijiJob, `args("input")` will evaluate to "SomeFile.txt" if your command line contained the
- *     argument `--input SomeFile.txt`
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Extensible
-class KijiJob(args: Args = Args(Nil))
-    extends Job(args)
-    with PipeConversions
-    with AvroTupleConversions {
-  override def validateSources(mode: Mode): Unit = {
-    val taps: List[Tap[_, _, _]] =
-        flowDef.getSources.values.asScala.toList ++
-        flowDef.getSinks.values.asScala.toList
-
-    // Retrieve the configuration
-    var conf: Configuration = HBaseConfiguration.create()
-    implicitly[Mode] match {
-      case Hdfs(_, configuration) => {
-        conf = configuration
-      }
-      case HadoopTest(configuration, _) => {
-        conf = configuration
-      }
-      case _ =>
-    }
-
-    // Validate that the Kiji parts of the sources (tables, columns) are valid and exist.
-    taps.foreach {
-      case kijiTap: KijiTap => kijiTap.validate(new JobConf(conf))
-      case localKijiTap: LocalKijiTap => {
-        val properties: Properties = new Properties()
-        properties.putAll(HadoopUtil.createProperties(conf))
-        localKijiTap.validate(properties)
-      }
-      case _ => // No Kiji parts to verify.
-    }
-
-    // Call any validation that scalding's Job class does.
-    super.validateSources(mode)
-  }
-
-  override def config(implicit mode: Mode): Map[AnyRef, AnyRef] = {
-    val baseConfig = super.config(mode)
-
-    // We configure as is done in Scalding's Job, but then append to mapred.child.java.opts to
-    // disable schema validation. This system property is only useful for KijiSchema v1.1. In newer
-    // versions of KijiSchema, this property has no effect.
-    val disableValidation = " -Dorg.kiji.schema.impl.AvroCellEncoder.SCHEMA_VALIDATION=DISABLED"
-    val oldJavaOptions = baseConfig
-        .get("mapred.child.java.opts")
-        .getOrElse("")
-
-    // Add support for our Kryo Avro serializers (see org.kiji.express.flow.framework.KryoKiji).
-    val oldSerializations = baseConfig("io.serializations").toString
-    require(oldSerializations.contains("com.twitter.scalding.serialization.KryoHadoop"))
-    val newSerializations = oldSerializations.replaceFirst(
-        "com.twitter.scalding.serialization.KryoHadoop",
-        "org.kiji.express.flow.framework.serialization.KryoKiji")
-
-    // Append all the new keys.
-    baseConfig +
-        ("mapred.child.java.opts" -> (oldJavaOptions + disableValidation)) +
-        ("io.serializations" -> newSerializations)
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/KijiOutput.scala b/src/main/scala/org/kiji/express/flow/KijiOutput.scala
deleted file mode 100644
index 0340bb4cb50cffa1c57f855060564e417dad51b3..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/KijiOutput.scala
+++ /dev/null
@@ -1,160 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-
-/**
- * Factory methods for constructing [[org.kiji.express.flow.KijiSource]]s that will be used as
- * outputs of a KijiExpress flow. Two basic APIs are provided with differing complexity.
- *
- * Simple:
- * {{{
- *   // Create a KijiOutput that writes to the table named `mytable` putting timestamps in the
- *   // `'timestamps` field and writing the fields `'column1` and `'column2` to the columns
- *   // `info:column1` and `info:column2`.
- *   KijiOutput(
- *       tableUri = "kiji://localhost:2181/default/mytable",
- *       timestampField = 'timestamps,
- *       'column1 -> "info:column1",
- *       'column2 -> "info:column2")
- * }}}
- *
- * Verbose:
- * {{{
- *   // Create a KijiOutput that writes to the table named `mytable` putting timestamps in the
- *   // `'timestamps` field and writing the fields `'column1` and `'column2` to the columns
- *   // `info:column1` and `info:column2`.
- *   KijiOutput(
- *       tableUri = "kiji://localhost:2181/default/mytable",
- *       timestampField = 'timestamps,
- *       columns = Map(
- *           // Enable paging for `info:column1`.
- *           'column1 -> QualifiedColumnOutputSpec("info", "column1"),
- *           'column2 -> QualifiedColumnOutputSpec("info", "column2")))
- * }}}
- *
- * The verbose methods allow you to instantiate explicity
- * [[org.kiji.express.flow.QualifiedColumnOutputSpec]] and
- * [[org.kiji.express.flow.ColumnOutputSpec]] objects.
- * Use the verbose method to specify options for the output columns, e.g.,
- * {{{
- *   // Create a KijiSource that reads from the table named `mytable` reading the columns
- *   // `info:column1` and `info:column2` to the fields `'column1` and `'column2`.
- *   KijiOutput(
- *       tableUri = "kiji://localhost:2181/default/mytable",
- *       timestampField = 'timestamps,
- *       columns = Map(
- *           QualifiedColumnOutputSpec("info", "column1", schemaId=Some(12)) -> 'column1,
- *           QualifiedColumnOutputSpec("info", "column2") -> 'column2)
- * }}}
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-object KijiOutput {
-  /**
-   * A factory method for instantiating [[org.kiji.express.flow.KijiSource]]s used as sinks. This
-   * method permits specifying the full range of read options for each column. Values written will
-   * be tagged with the current time at write.
-   *
-   * @param tableUri that addresses a table in a Kiji instance.
-   * @param columns is a mapping specifying what column to write each field value to.
-   * @return a source that can write tuple field values to columns of a Kiji table.
-   */
-  def apply(
-      tableUri: String,
-      columns: Map[Symbol, _ <: ColumnOutputSpec]
-  ): KijiSource = {
-    new KijiSource(
-        tableAddress = tableUri,
-        timeRange = All,
-        timestampField = None,
-        outputColumns = columns)
-  }
-
-  /**
-   * A factory method for instantiating [[org.kiji.express.flow.KijiSource]]s used as sinks. This
-   * method permits specifying the full range of read options for each column.
-   *
-   * @param tableUri that addresses a table in a Kiji instance.
-   * @param columns is a mapping specifying what column to write each field value to.
-   * @param timestampField is the name of a tuple field that will contain cell timestamps when the
-   *     source is used for writing.
-   * @return a source that can write tuple field values to columns of a Kiji table.
-   */
-  def apply(
-      tableUri: String,
-      timestampField: Symbol,
-      columns: Map[Symbol, _ <: ColumnOutputSpec]
-  ): KijiSource = {
-    require(timestampField != null)
-
-    new KijiSource(
-        tableAddress = tableUri,
-        timeRange = All,
-        timestampField = Some(timestampField),
-        outputColumns = columns)
-  }
-
-  /**
-   * A factory method for instantiating [[org.kiji.express.flow.KijiSource]]s used as sinks. Values
-   * written will be tagged with the current time at write.
-   *
-   * @param tableUri that addresses a table in a Kiji instance.
-   * @param columns are a series of pairs mapping tuple field names to Kiji column names. When
-   *     naming columns, use the format `"family:qualifier"`.
-   * @return a source that can write tuple field values to columns of a Kiji table.
-   */
-  def apply(
-      tableUri: String,
-      columns: (Symbol, String)*
-  ): KijiSource = {
-    val columnMap = columns
-        .toMap
-        .mapValues(QualifiedColumnOutputSpec(_))
-
-    KijiOutput(tableUri, columnMap)
-  }
-
-  /**
-   * A factory method for instantiating [[org.kiji.express.flow.KijiSource]]s used as sinks.
-   *
-   * @param tableUri that addresses a table in a Kiji instance.
-   * @param columns are a series of pairs mapping tuple field names to Kiji column names. When
-   *     naming columns, use the format `"family:qualifier"`.
-   * @param timestampField is the name of a tuple field that will contain cell timestamps when the
-   *     source is used for writing.
-   * @return a source that can write tuple field values to columns of a Kiji table.
-   */
-  def apply(
-      tableUri: String,
-      timestampField: Symbol,
-      columns: (Symbol, String)*
-  ): KijiSource = {
-    require(timestampField != null)
-
-    val columnMap = columns
-        .toMap
-        .mapValues(QualifiedColumnOutputSpec(_))
-
-    KijiOutput(tableUri, timestampField, columnMap)
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/KijiPipe.scala b/src/main/scala/org/kiji/express/flow/KijiPipe.scala
deleted file mode 100644
index ac8a3395e9bc9c4826dd4d51f679863f5fa4feac..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/KijiPipe.scala
+++ /dev/null
@@ -1,178 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import cascading.flow.Flow
-import cascading.pipe.Pipe
-import cascading.tuple.Fields
-import com.twitter.scalding.Args
-import com.twitter.scalding.Job
-import com.twitter.scalding.Mode
-import com.twitter.scalding.TupleConversions
-import com.twitter.scalding.TupleSetter
-import org.apache.avro.Schema
-import org.apache.avro.generic.GenericRecord
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.framework.serialization.KijiLocker
-import org.kiji.express.flow.util.AvroGenericTupleConverter
-import org.kiji.express.repl.ExpressShell
-import org.kiji.express.repl.Implicits
-import org.kiji.express.repl.Implicits.pipeToRichPipe
-
-/**
- * A class that adds Kiji-specific functionality to a Cascading pipe. This includes running pipes
- * outside of the context of a Scalding Job.
- *
- * A `KijiPipe` should be obtained by end-users during the course of authoring a Scalding flow via
- * an implicit conversion available in [[org.kiji.express.repl.Implicits]].
- *
- * @param pipe enriched with extra functionality.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-class KijiPipe(private[express] val pipe: Pipe) extends TupleConversions {
-  /**
-   * Gets a job that can be used to run the data pipeline.
-   *
-   * @param args that should be used to construct the job.
-   * @return a job that can be used to run the data pipeline.
-   */
-  private[express] def getJob(args: Args): Job = new KijiJob(args) {
-    // The job's constructor should evaluate to the pipe to run.
-    pipe
-
-    /**
-     *  The flow definition used by this job, which should be the same as that used by the user
-     *  when creating their pipe.
-     */
-    override implicit val flowDef = Implicits.flowDef
-
-    /**
-     * Obtains a configuration used when running the job.
-     *
-     * This overridden method uses the same configuration as a standard Scalding job,
-     * but adds options specific to KijiExpress, including adding a jar containing compiled REPL
-     * code to the distributed cache if the REPL is running.
-     *
-     * @param mode used to run the job (either local or hadoop).
-     * @return the configuration that should be used to run the job.
-     */
-    override def config(implicit mode: Mode): Map[AnyRef, AnyRef] = {
-      // Use the configuration from Scalding Job as our base.
-      val configuration = super.config(mode)
-
-      /** Appends a comma to the end of a string. */
-      def appendComma(str: Any): String = str.toString + ","
-
-      // If the REPL is running, we should add tmpjars passed in from the command line,
-      // and a jar of REPL code, to the distributed cache of jobs run through the REPL.
-      val replCodeJar = ExpressShell.createReplCodeJar()
-      val tmpJarsConfig =
-        if (replCodeJar.isDefined) {
-          Map("tmpjars" -> {
-              // Use tmpjars already in the configuration.
-              configuration
-                  .get("tmpjars")
-                  .map(appendComma)
-                  .getOrElse("") +
-              // And tmpjars passed to ExpressShell from the command line when started.
-              ExpressShell.tmpjars
-                  .map(appendComma)
-                  .getOrElse("") +
-              // And a jar of code compiled by the REPL.
-              "file://" + replCodeJar.get.getAbsolutePath
-          })
-        } else {
-          // No need to add the tmpjars to the configuration
-          Map[String, String]()
-        }
-
-      val userClassPathFirstConfig = Map("mapreduce.task.classpath.user.precedence" -> "true")
-
-      configuration ++ tmpJarsConfig ++ userClassPathFirstConfig
-    }
-
-    /**
-     * Builds a flow from the flow definition used when creating the pipeline run by this job.
-     *
-     * This overridden method operates the same as that of the super class,
-     * but clears the implicit flow definition defined in [[org.kiji.express.repl.Implicits]]
-     * after the flow has been built from the flow definition. This allows additional pipelines
-     * to be constructed and run after the pipeline encapsulated by this job.
-     *
-     * @param mode the mode in which the built flow will be run.
-     * @return the flow created from the flow definition.
-     */
-    override def buildFlow(implicit mode: Mode): Flow[_] = {
-      val flow = super.buildFlow(mode)
-      Implicits.resetFlowDef()
-      flow
-    }
-  }
-
-  /**
-   * Runs this pipe as a Scalding job.
-   */
-  def run() {
-    getJob(new Args(Map())).run(Mode.mode)
-
-    // Clear the REPL state after running a job.
-    Implicits.resetFlowDef()
-  }
-
-  /**
-   * Packs the specified fields into an Avro [[org.apache.avro.generic.GenericRecord]].  The
-   * provided field names must match the fields of the generic record specified by the schema.
-   *
-   * @param fields is the mapping of input fields (to be packed into the
-   *     [[org.apache.avro.generic.GenericRecord]]) to output field which will contain
-   *     the [[org.apache.avro.generic.GenericRecord]].
-   * @return a pipe containing all input fields, and an additional field containing an
-   *     [[org.apache.avro.generic.GenericRecord]].
-   */
-  def packGenericRecord(fields: (Fields, Fields))(schema: Schema): Pipe = {
-    require(fields._2.size == 1, "Cannot pack generic record to more than a single field.")
-    require(schema.getType == Schema.Type.RECORD, "Cannot pack non-record Avro type.")
-    pipe.map(fields) { input: GenericRecord => input } (
-      new AvroGenericTupleConverter(KijiLocker(schema)), implicitly[TupleSetter[GenericRecord]])
-  }
-
-  /**
-   * Packs the specified fields into an Avro [[org.apache.avro.generic.GenericRecord]] and drops
-   * other fields from the flow.  The provided field names must match the fields of the
-   * generic record specified by the schema.
-   *
-   * @param fields is the mapping of input fields (to be packed into the
-   *     [[org.apache.avro.generic.GenericRecord]]) to new output field which will
-   *     contain the [[org.apache.avro.generic.GenericRecord]].
-   * @return a pipe containing a single field with an Avro
-   *     [[org.apache.avro.generic.GenericRecord]].
-   */
-  def packGenericRecordTo(fields: (Fields, Fields))(schema: Schema): Pipe = {
-    require(fields._2.size == 1, "Cannot pack generic record to more than a single field.")
-    require(schema.getType == Schema.Type.RECORD, "Cannot pack to non-record Avro type.")
-    pipe.mapTo(fields) { input: GenericRecord => input } (
-      new AvroGenericTupleConverter(KijiLocker(schema)), implicitly[TupleSetter[GenericRecord]])
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/KijiSource.scala b/src/main/scala/org/kiji/express/flow/KijiSource.scala
deleted file mode 100644
index 3467b53507f0865adeb8887235d2685c17b6cfd0..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/KijiSource.scala
+++ /dev/null
@@ -1,518 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import scala.collection.JavaConverters.asScalaIteratorConverter
-import scala.collection.JavaConverters.mapAsJavaMapConverter
-import scala.collection.mutable.Buffer
-
-import java.io.OutputStream
-import java.util.Properties
-
-import cascading.flow.FlowProcess
-import cascading.flow.hadoop.util.HadoopUtil
-import cascading.scheme.Scheme
-import cascading.scheme.SinkCall
-import cascading.tap.Tap
-import cascading.tuple.Fields
-import cascading.tuple.Tuple
-import cascading.tuple.TupleEntry
-import com.google.common.base.Objects
-import com.twitter.scalding.AccessMode
-import com.twitter.scalding.Test
-import com.twitter.scalding.HadoopTest
-import com.twitter.scalding.Hdfs
-import com.twitter.scalding.Local
-import com.twitter.scalding.Mode
-import com.twitter.scalding.Read
-import com.twitter.scalding.Source
-import com.twitter.scalding.Write
-import org.apache.hadoop.conf.Configuration
-import org.apache.hadoop.hbase.HBaseConfiguration
-import org.apache.hadoop.mapred.JobConf
-import org.apache.hadoop.mapred.OutputCollector
-import org.apache.hadoop.mapred.RecordReader
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.express.flow.framework.KijiScheme
-import org.kiji.express.flow.framework.KijiTap
-import org.kiji.express.flow.framework.LocalKijiScheme
-import org.kiji.express.flow.framework.LocalKijiTap
-import org.kiji.express.flow.framework.OutputContext
-import org.kiji.express.flow.util.Resources._
-import org.kiji.mapreduce.framework.KijiConfKeys
-import org.kiji.schema.EntityIdFactory
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiDataRequest
-import org.kiji.schema.KijiRowData
-import org.kiji.schema.KijiRowScanner
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiTableWriter
-import org.kiji.schema.KijiURI
-import org.kiji.schema.layout.CellSpec
-
-/**
- * A read or write view of a Kiji table.
- *
- * A Scalding `Source` provides a view of a data source that can be read as Scalding tuples. It
- * is comprised of a Cascading tap [[cascading.tap.Tap]], which describes where the data is and how
- * to access it, and a Cascading Scheme [[cascading.scheme.Scheme]], which describes how to read
- * and interpret the data.
- *
- * When reading from a Kiji table, a `KijiSource` will provide a view of a Kiji table as a
- * collection of tuples that correspond to rows from the Kiji table. Which columns will be read
- * and how they are associated with tuple fields can be configured,
- * as well as the time span that cells retrieved must belong to.
- *
- * When writing to a Kiji table, a `KijiSource` views a Kiji table as a collection of tuples that
- * correspond to cells from the Kiji table. Each tuple to be written must provide a cell address
- * by specifying a Kiji `EntityID` in the tuple field `entityId`, a value to be written in a
- * configurable field, and (optionally) a timestamp in a configurable field.
- *
- * End-users cannot directly obtain instances of `KijiSource`. Instead,
- * they should use the factory methods provided as part of the [[org.kiji.express.flow]] module.
- *
- * @param tableAddress is a Kiji URI addressing the Kiji table to read or write to.
- * @param timeRange that cells read must belong to. Ignored when the source is used to write.
- * @param timestampField is the name of a tuple field that will contain cell timestamp when the
- *     source is used for writing. Specify `None` to write all
- *     cells at the current time.
- * @param inputColumns is a one-to-one mapping from field names to Kiji columns. The columns in the
- *     map will be read into their associated tuple fields.
- * @param outputColumns is a one-to-one mapping from field names to Kiji columns. Values from the
- *     tuple fields will be written to their associated column.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-final class KijiSource private[express] (
-    val tableAddress: String,
-    val timeRange: TimeRange,
-    val timestampField: Option[Symbol],
-    val inputColumns: Map[Symbol, ColumnInputSpec] = Map(),
-    val outputColumns: Map[Symbol, ColumnOutputSpec] = Map()
-) extends Source {
-  import KijiSource._
-
-  private type HadoopScheme = Scheme[JobConf, RecordReader[_, _], OutputCollector[_, _], _, _]
-
-  /** The URI of the target Kiji table. */
-  private val tableUri: KijiURI = KijiURI.newBuilder(tableAddress).build()
-
-  /** A Kiji scheme intended to be used with Scalding/Cascading's hdfs mode. */
-  private val kijiScheme: KijiScheme =
-      new KijiScheme(
-          timeRange,
-          timestampField,
-          convertKeysToStrings(inputColumns),
-          convertKeysToStrings(outputColumns)
-      )
-
-  /** A Kiji scheme intended to be used with Scalding/Cascading's local mode. */
-  private val localKijiScheme: LocalKijiScheme =
-      new LocalKijiScheme(
-          timeRange,
-          timestampField,
-          convertKeysToStrings(inputColumns),
-          convertKeysToStrings(outputColumns)
-      )
-
-  /**
-   * Creates a Scheme that writes to/reads from a Kiji table for usage with
-   * the hadoop runner.
-   */
-  override val hdfsScheme: HadoopScheme = kijiScheme
-      // This cast is required due to Scheme being defined with invariant type parameters.
-      .asInstanceOf[HadoopScheme]
-
-  /**
-   * Creates a Scheme that writes to/reads from a Kiji table for usage with
-   * the local runner.
-   */
-  override val localScheme: LocalScheme = localKijiScheme
-      // This cast is required due to Scheme being defined with invariant type parameters.
-      .asInstanceOf[LocalScheme]
-
-  /**
-   * Create a connection to the physical data source (also known as a Tap in Cascading)
-   * which, in this case, is a [[org.kiji.schema.KijiTable]].
-   *
-   * @param readOrWrite Specifies if this source is to be used for reading or writing.
-   * @param mode Specifies which job runner/flow planner is being used.
-   * @return A tap to use for this data source.
-   */
-  override def createTap(readOrWrite: AccessMode)(implicit mode: Mode): Tap[_, _, _] = {
-    /** Combination of normal input columns and input versions of the output columns (the latter are
-     * needed for reading back written results) */
-    def getInputColumnsForTesting: Map[String, ColumnInputSpec] = {
-      val testingInputColumnsFromReads = inputColumnSpecifyAllData(
-          convertKeysToStrings(inputColumns))
-      val testingInputColumnsFromWrites = inputColumnSpecifyAllData(
-          convertKeysToStrings(outputColumns)
-          .mapValues { x: ColumnOutputSpec => ColumnInputSpec(x.columnName.toString) })
-      testingInputColumnsFromReads ++ testingInputColumnsFromWrites
-    }
-
-    val tap: Tap[_, _, _] = mode match {
-      // Production taps.
-      case Hdfs(_,_) => new KijiTap(tableUri, kijiScheme).asInstanceOf[Tap[_, _, _]]
-      case Local(_) => new LocalKijiTap(tableUri, localKijiScheme).asInstanceOf[Tap[_, _, _]]
-
-      // Test taps.
-      case HadoopTest(conf, buffers) => {
-        readOrWrite match {
-          case Read => {
-            val scheme = kijiScheme
-            populateTestTable(tableUri, buffers(this), scheme.getSourceFields, conf)
-
-            new KijiTap(tableUri, scheme).asInstanceOf[Tap[_, _, _]]
-          }
-          case Write => {
-            val scheme = new TestKijiScheme(
-                timestampField,
-                getInputColumnsForTesting,
-                convertKeysToStrings(outputColumns))
-
-            new KijiTap(tableUri, scheme).asInstanceOf[Tap[_, _, _]]
-          }
-        }
-      }
-      case Test(buffers) => {
-        readOrWrite match {
-          // Use Kiji's local tap and scheme when reading.
-          case Read => {
-            val scheme = localKijiScheme
-            populateTestTable(
-                tableUri,
-                buffers(this),
-                scheme.getSourceFields,
-                HBaseConfiguration.create())
-
-            new LocalKijiTap(tableUri, scheme).asInstanceOf[Tap[_, _, _]]
-          }
-
-          // After performing a write, use TestLocalKijiScheme to populate the output buffer.
-          case Write => {
-            val scheme = new TestLocalKijiScheme(
-                buffers(this),
-                timeRange,
-                timestampField,
-                getInputColumnsForTesting,
-                convertKeysToStrings(outputColumns))
-
-            new LocalKijiTap(tableUri, scheme).asInstanceOf[Tap[_, _, _]]
-          }
-        }
-      }
-
-      // Delegate any other tap types to Source's default behaviour.
-      case _ => super.createTap(readOrWrite)(mode)
-    }
-
-    return tap
-  }
-
- override def toString: String = {
-   Objects
-       .toStringHelper(this)
-       .add("tableAddress", tableAddress)
-       .add("timeRange", timeRange)
-       .add("timestampField", timestampField)
-       .add("inputColumns", inputColumns)
-       .add("outputColumns", outputColumns)
-       .toString
-  }
-
-  override def equals(other: Any): Boolean = {
-    other match {
-      case source: KijiSource => {
-        Objects.equal(tableAddress, source.tableAddress) &&
-        Objects.equal(inputColumns, source.inputColumns) &&
-        Objects.equal(outputColumns, source.outputColumns) &&
-        Objects.equal(timestampField, source.timestampField) &&
-        Objects.equal(timeRange, source.timeRange)
-      }
-      case _ => false
-    }
-  }
-
-  override def hashCode(): Int =
-      Objects.hashCode(tableAddress, inputColumns, outputColumns, timestampField, timeRange)
-}
-
-/**
- * Contains a private, inner class used by [[org.kiji.express.flow.KijiSource]] when working with
- * tests.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-private[express] object KijiSource {
-  /**
-   * Convert scala columns definition into its corresponding java variety.
-   *
-   * @param columnMap Mapping from field name to Kiji column name.
-   * @return Java map from field name to column definition.
-   */
-  private[express] def convertKeysToStrings[T <: Any](columnMap: Map[Symbol, T])
-      : Map[String, T] = {
-    columnMap.map { case (symbol, column) => (symbol.name, column) }
-  }
-
-  /**
-   * Takes a buffer containing rows and writes them to the table at the specified uri.
-   *
-   * @param tableUri of the table to populate.
-   * @param rows Tuples to write to populate the table with.
-   * @param fields Field names for elements in the tuple.
-   * @param configuration defining the cluster to use.
-   */
-  private def populateTestTable(
-      tableUri: KijiURI,
-      rows: Buffer[Tuple],
-      fields: Fields,
-      configuration: Configuration) {
-    doAndRelease(Kiji.Factory.open(tableUri)) { kiji: Kiji =>
-      // Layout to get the default reader schemas from.
-      val layout = withKijiTable(tableUri, configuration) { table: KijiTable =>
-        table.getLayout
-      }
-
-      val eidFactory = EntityIdFactory.getFactory(layout)
-
-      // Write the desired rows to the table.
-      withKijiTableWriter(tableUri, configuration) { writer: KijiTableWriter =>
-        rows.foreach { row: Tuple =>
-          val tupleEntry = new TupleEntry(fields, row)
-          val iterator = fields.iterator()
-
-          // Get the entity id field.
-          val entityIdField = iterator.next().toString
-          val entityId = tupleEntry
-            .getObject(entityIdField)
-            .asInstanceOf[EntityId]
-
-          // Iterate through fields in the tuple, adding each one.
-          while (iterator.hasNext) {
-            val field = iterator.next().toString
-
-            // Get the timeline to be written.
-            val cells: Seq[FlowCell[Any]] = tupleEntry
-                .getObject(field)
-                .asInstanceOf[Seq[FlowCell[Any]]]
-
-            // Write the timeline to the table.
-            cells.foreach { cell: FlowCell[Any] =>
-              writer.put(
-                  entityId.toJavaEntityId(eidFactory),
-                  cell.family,
-                  cell.qualifier,
-                  cell.version,
-                  cell.datum
-              )
-            }
-          }
-        }
-      }
-    }
-  }
-
-  private[express] def newGetAllData(col: ColumnInputSpec): ColumnInputSpec = {
-    ColumnInputSpec(
-        col.columnName.toString,
-        Integer.MAX_VALUE,
-        col.filter,
-        col.paging,
-        col.schemaSpec)
-  }
-
-  /**
-   * Returns a map from field name to column input spec where the column input spec has been
-   * configured as an output column.
-   *
-   * This is used in tests, when we use KijiScheme to read tuples from a Kiji table, and we want
-   * to read all data in all of the columns, so the test can inspect all data in the table.
-   *
-   * @param columns to transform.
-   * @return transformed map where the column input specs are configured for output.
-   */
-  private def inputColumnSpecifyAllData(
-      columns: Map[String, ColumnInputSpec]): Map[String, ColumnInputSpec] = {
-    columns.mapValues(newGetAllData)
-        // Need this to make the Map serializable (issue with mapValues)
-        .map(identity)
-  }
-
-  /**
-   * A LocalKijiScheme that loads rows in a table into the provided buffer. This class
-   * should only be used during tests.
-   *
-   * @param buffer to fill with post-job table rows for tests.
-   * @param timeRange of timestamps to read from each column.
-   * @param timestampField is the name of a tuple field that will contain cell timestamp when the
-   *     source is used for writing. Specify the empty field name to write all
-   *     cells at the current time.
-   * @param inputColumns is a map of Scalding field name to ColumnInputSpec.
-   * @param outputColumns is a map of ColumnOutputSpec to Scalding field name.
-   */
-  private class TestLocalKijiScheme(
-      val buffer: Buffer[Tuple],
-      timeRange: TimeRange,
-      timestampField: Option[Symbol],
-      inputColumns: Map[String, ColumnInputSpec],
-      outputColumns: Map[String, ColumnOutputSpec])
-      extends LocalKijiScheme(
-          timeRange,
-          timestampField,
-          inputColumnSpecifyAllData(inputColumns),
-          outputColumns) {
-    override def sinkCleanup(
-        process: FlowProcess[Properties],
-        sinkCall: SinkCall[OutputContext, OutputStream]) {
-      // Store the output table.
-      val conf: JobConf = HadoopUtil
-          .createJobConf(process.getConfigCopy, new JobConf(HBaseConfiguration.create()))
-      val uri: KijiURI = KijiURI
-          .newBuilder(conf.get(KijiConfKeys.KIJI_OUTPUT_TABLE_URI))
-          .build()
-
-      // Read table into buffer.
-      withKijiTable(uri, conf) { table: KijiTable =>
-        val layout = table.getLayout
-
-        // Determine what type of record to use (generic, specific).
-        val cellSpecOverrides: Map[KijiColumnName, CellSpec] = outputColumns
-            .values
-            .map { column => (column.columnName, column.schemaSpec) }
-            .collect {
-              case (name, SchemaSpec.DefaultReader) => {
-                val cellSpec = layout.getCellSpec(name).setUseDefaultReaderSchema()
-                (name, cellSpec)
-              }
-              case (name, SchemaSpec.Writer) => {
-                val cellSpec = layout.getCellSpec(name).setUseWriterSchema()
-                (name, cellSpec)
-              }
-              case (name, SchemaSpec.Generic(avroSchema)) => {
-                val cellSpec = layout.getCellSpec(name).setReaderSchema(avroSchema)
-                (name, cellSpec)
-              }
-              case (name, SchemaSpec.Specific(avroClass)) => {
-                val cellSpec = layout.getCellSpec(name).setSpecificRecord(avroClass)
-                (name, cellSpec)
-              }
-            }
-            .toMap
-
-        // Open a table reader that reads data using the generic api.
-        val readerFactory = table.getReaderFactory
-        doAndClose(readerFactory.openTableReader(cellSpecOverrides.asJava)) { reader =>
-          // We also want the entire timerange, so the test can inspect all data in the table.
-          val request: KijiDataRequest = KijiScheme.buildRequest(All, inputColumns.values)
-
-          doAndClose(reader.getScanner(request)) { scanner: KijiRowScanner =>
-            val rows: Iterator[KijiRowData] = scanner.iterator().asScala
-            rows.foreach { row: KijiRowData =>
-              val tuple = KijiScheme
-                  .rowToTuple(
-                      // Use input columns that are based on the output columns
-                      inputColumns,
-                      getSourceFields,
-                      timestampField,
-                      row,
-                      table.getURI,
-                      conf
-                  )
-
-              val newTupleValues = tuple
-                  .iterator()
-                  .asScala
-                  .map {
-                    // This converts stream into a list to force the stream to compute all of the
-                    // transformations that have been applied lazily to it. This is necessary
-                    // because some of the transformations applied in KijiScheme#rowToTuple have
-                    // dependencies on an open connection to a schema table.
-                    case stream: Stream[_] => stream.toList
-                    case x => x
-                  }
-                  .toSeq
-
-              buffer += new Tuple(newTupleValues: _*)
-            }
-          }
-        }
-      }
-
-      super.sinkCleanup(process, sinkCall)
-    }
-  }
-
-  /**
-   * Merges an input column mapping with an output column mapping producing an input column mapping.
-   * This is used to configure input columns for reading back written data on a source that has just
-   * been used as a sink.
-   *
-   * @param inputs describing which columns to request and what fields to associate them with.
-   * @param outputs describing which columns fields should be output to.
-   * @return a merged mapping from field names to input column requests.
-   */
-  private def mergeColumnMapping(
-      inputs: Map[String, ColumnInputSpec],
-      outputs: Map[String, ColumnOutputSpec]
-  ): Map[String, ColumnInputSpec] = {
-    def mergeEntry(
-        inputs: Map[String, ColumnInputSpec],
-        entry: (String, ColumnOutputSpec)
-    ): Map[String, ColumnInputSpec] = {
-      val (fieldName, columnRequest) = entry
-      val input = ColumnInputSpec(
-          column = columnRequest.columnName.getName,
-          maxVersions = Int.MaxValue,
-          schemaSpec = columnRequest.schemaSpec
-      )
-
-      inputs + ((fieldName, input))
-    }
-
-    outputs
-        .foldLeft(inputs)(mergeEntry)
-  }
-
-  /**
-   * A KijiScheme that loads rows in a table into the provided buffer. This class should only be
-   * used during tests.
-   *
-   * @param timestampField is the name of a tuple field that will contain cell timestamp when the
-   *     source is used for writing. Specify the empty field name to write all cells at the current
-   *     time.
-   * @param inputColumns Scalding field name to column input spec mapping.
-   * @param outputColumns Scalding field name to column output spec mapping.
-   */
-  private class TestKijiScheme(
-      timestampField: Option[Symbol],
-      inputColumns: Map[String, ColumnInputSpec],
-      outputColumns: Map[String, ColumnOutputSpec])
-      extends KijiScheme(
-          All,
-          timestampField,
-          mergeColumnMapping(inputColumns, outputColumns),
-          outputColumns) {
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/PagingSpec.scala b/src/main/scala/org/kiji/express/flow/PagingSpec.scala
deleted file mode 100644
index c17c85ed86b2f8bba735352464af8414369f9fe0..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/PagingSpec.scala
+++ /dev/null
@@ -1,87 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-
-/**
- * A specification of the type of paging to use.
- *
- * These can be used in [[org.kiji.express.flow.ColumnInputSpec]].  The default in
- * [[org.kiji.express.flow.ColumnInputSpec]] is [[org.kiji.express.flow.PagingSpec.Off]], which
- * disables all paging.  With paging disabled, all cells from the specified column will be loaded
- * into memory at once.  If the size of all of the loaded cells exceeds the capacity of the
- * receiving machine's main memory, the Scalding job will fail at runtime.  In these cases you can
- * specify how many cells should be paged into memory at a time with
- * [[org.kiji.express.flow.PagingSpec.Cells]]:
- *
- * {{{
- *   paging = PagingSpec.Cells(10)
- * }}}
- *
- * This will load only 10 cells at a time into memory.
- *
- * The appropriate number of cells to be paged in depends on the size of each cell. Users should
- * try to retrieve as many cells as possible (without causing OOME) in order to increase
- * performance.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-sealed trait PagingSpec {
-  private[kiji] def cellsPerPage: Option[Int]
-}
-
-/**
- * Module to provide PagingSpec implementations.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-object PagingSpec {
-  /**
-   * Specifies that paging should not be used. Each row requested from Kiji tables will be fully
-   * materialized into RAM.
-   */
-  @ApiAudience.Public
-  @ApiStability.Experimental
-  case object Off extends PagingSpec {
-    override val cellsPerPage: Option[Int] = None
-  }
-
-  /**
-   * Specifies that paging should be enabled. Each page will contain the specified number of cells.
-   *
-   * Note: Cells may not all be the same size (in bytes).
-   *
-   * Note: There are known issues with paging in Express.  See https://jira.kiji.org/browse/EXP-326,
-   *    [[org.kiji.express.flow.TransientSeq]] and [[org.kiji.express.flow.TransientSeqSuite]] for
-   *    details and workarounds.
-   *
-   * @param count of the cells per page.
-   */
-  @ApiAudience.Public
-  @ApiStability.Experimental
-  @Inheritance.Sealed
-  final case class Cells(count: Int) extends PagingSpec {
-    override val cellsPerPage: Option[Int] = Some(count)
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/SchemaSpec.scala b/src/main/scala/org/kiji/express/flow/SchemaSpec.scala
deleted file mode 100644
index 14800d1123248206716c2d039af054c353244d2a..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/SchemaSpec.scala
+++ /dev/null
@@ -1,123 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.apache.avro.Schema
-import org.apache.avro.specific.SpecificRecord
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-
-/**
- * A specification of how to read or write values to a Kiji column.
- *
- * An instance of one of the subclasses of SchemaSpec, [[org.kiji.express.flow.SchemaSpec.Generic]],
- * [[org.kiji.express.flow.SchemaSpec.Specific]],
- * [[org.kiji.express.flow.SchemaSpec.DefaultReader]], or
- * [[org.kiji.express.flow.SchemaSpec.Writer]], can be used as an optional parameter to
- * [[org.kiji.express.flow.ColumnFamilyInputSpec]],
- * [[org.kiji.express.flow.QualifiedColumnInputSpec]],
- * [[org.kiji.express.flow.ColumnFamilyOutputSpec]], and
- * [[org.kiji.express.flow.QualifiedColumnOutputSpec]].
- *
- * These classes specify the Avro schema to read the data in a column with, or the Avro schema to
- * write the data to a column with.  Here are the possible subclasses you may use in your
- * `ColumnInputSpec` or `ColumnOutputSpec`:
- * <ul>
- *   <li>`SchemaSpec.Specific(classOf[MySpecificRecordClass])`: This option should be used when you
- *   have a specific class that has been compiled by Avro.  `MySpecificRecordClass` must extend
- *   `org.apache.avro.SpecificRecord`</li>
- *   <li>`SchemaSpec.Generic(myGenericSchema)`: If you don’t have the specific class you want to use
- *   to read or write on the classpath, you can construct a generic schema and use it as the reader
- *   schema.</li>
- *   <li>`SchemaSpec.Writer`: used when you want to read with the same schema that the data
- *   was written with, or a schema attached to or inferred from the value to write with.  This is
- *   the default if you don’t specify any `SchemaSpec` for reading or writing.</li>
- *   <li>`SchemaSpec.DefaultReader`: specifies that the default reader for this column, stored in
- *   the table layout, should be used for reading or writing this data.  If you use this option,
- *   first make sure the column in your Kiji table has a default reader specified.</li>
- * </ul>
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-sealed trait SchemaSpec extends java.io.Serializable {
-  /**
-   * Retrieve the Avro [[org.apache.avro.Schema]] object associated with this SchemaSpec,
-   * if possible.
-   */
-  private[kiji] def schema: Option[Schema]
-}
-
-/**
- * Module to provide SchemaSpec implementations.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-object SchemaSpec {
-  /**
-   * Specifies reading or writing with the supplied [[org.apache.avro.Schema]].
-   *
-   * @param genericSchema of data
-   */
-  @ApiAudience.Public
-  @ApiStability.Experimental
-  @Inheritance.Sealed
-  final case class Generic(genericSchema: Schema) extends SchemaSpec {
-    override val schema: Option[Schema] = Some(genericSchema)
-  }
-
-  /**
-   * A specification for reading or writing as an instance of the supplied Avro specific record.
-   *
-   * @param klass of the specific record.
-   */
-  @ApiAudience.Public
-  @ApiStability.Experimental
-  @Inheritance.Sealed
-  final case class Specific(klass: Class[_ <: SpecificRecord]) extends SchemaSpec {
-    override val schema: Option[Schema] = Some(klass.newInstance.getSchema)
-  }
-
-  /**
-   * Use the writer schema associated with a value to read or write.
-   *
-   * In the case of reading a value, the writer schema used to serialize the value will be used.
-   * In the case of writing a value, the schema attached to or inferred from the value will be used.
-   */
-  @ApiAudience.Public
-  @ApiStability.Experimental
-  case object Writer extends SchemaSpec {
-    override val schema: Option[Schema] = None
-  }
-
-  /**
-   * Specifies that the default reader for this column, stored in the table layout, should be used
-   * for reading or writing this data.  If you use this option, first make sure the column in your
-   * Kiji table has a default reader specified.
-   */
-  @ApiAudience.Public
-  @ApiStability.Experimental
-  case object DefaultReader extends SchemaSpec {
-    override val schema: Option[Schema] = None
-  }
-}
-
diff --git a/src/main/scala/org/kiji/express/flow/TimeRange.scala b/src/main/scala/org/kiji/express/flow/TimeRange.scala
deleted file mode 100644
index 6c60a5ffa81a7ef4e8dbfef7058db7c1e9a34bb7..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/TimeRange.scala
+++ /dev/null
@@ -1,139 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.KConstants
-
-/**
- * A trait implemented by classes that specify time ranges when reading data from Kiji tables. This
- * class is used to specify the range of cell versions to request from a column in a Kiji table.
- *
- * To specify that [[org.kiji.express.flow.All]] versions should be requested:
- * {{{
- *   val timeRange: TimeRange = All
- * }}}
- * To specify that a specific version should be requested ([[org.kiji.express.flow.At]]):
- * {{{
- *   // Gets only cells with the version `123456789`.
- *   val timeRange: TimeRange = At(123456789L)
- * }}}
- * To specify that all versions [[org.kiji.express.flow.After]] the specified version should be
- * requested:
- * {{{
- *   // Gets only cells with versions larger than `123456789`.
- *   val timeRange: TimeRange = After(123456789L)
- * }}}
- * To specify that all versions [[org.kiji.express.flow.Before]] the specified version should be
- * requested:
- * {{{
- *   // Gets only cells with versions smaller than `123456789`.
- *   val timeRange: TimeRange = Before(123456789L)
- * }}}
- * To specify that all versions [[org.kiji.express.flow.Between]] the two specified bounds should be
- * requested:
- * {{{
- *   // Gets only cells with versions between `12345678` and `123456789`.
- *   val timeRange: TimeRange = Between(12345678, 123456789)
- * }}}
- *
- * See [[org.kiji.express.flow.KijiInput]] for more information.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-sealed trait TimeRange extends Serializable {
-  /** Earliest version of the TimeRange, inclusive. */
-  def begin: Long
-
-  /** Latest version of the TimeRange, exclusive. */
-  def end: Long
-}
-
-/**
- * Implementation of [[org.kiji.express.flow.TimeRange]] for specifying that all versions should be
- * requested.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-case object All extends TimeRange {
-  override val begin: Long = KConstants.BEGINNING_OF_TIME
-  override val end: Long = KConstants.END_OF_TIME
-}
-
-/**
- * Implementation of [[org.kiji.express.flow.TimeRange]] for specifying that only the provided
- * version should be requested.
- *
- * @param version to request.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class At(version: Long) extends TimeRange {
-  override val begin: Long = version
-  override val end: Long = version
-}
-
-/**
- * Implementation of [[org.kiji.express.flow.TimeRange]] for specifying that all versions after the
- * provided version should be requested (exclusive).
- *
- * @param begin is the earliest version that should be requested (exclusive).
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class After(override val begin: Long) extends TimeRange {
-  override val end: Long = KConstants.END_OF_TIME
-}
-
-/**
- * Implementation of [[org.kiji.express.flow.TimeRange]] for specifying that all versions before the
- * provided version should be requested (inclusive).
- *
- * @param end is the latest version that should be requested (inclusive).
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class Before(override val end: Long) extends TimeRange {
-  override val begin: Long = KConstants.BEGINNING_OF_TIME
-}
-
-/**
- * Implementation of [[org.kiji.express.flow.TimeRange]] for specifying that all versions between
- * the provided begin and end versions should be requested.
- *
- * @param begin is the earliest version that should be requested (inclusive).
- * @param end is the latest version that should be requested (exclusive).
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-final case class Between(
-    override val begin: Long,
-    override val end: Long) extends TimeRange {
-  // Ensure that the timerange bounds are sensible.
-  require(begin <= end, "Invalid time range specified: (%d, %d)".format(begin, end))
-}
diff --git a/src/main/scala/org/kiji/express/flow/TransientSeq.scala b/src/main/scala/org/kiji/express/flow/TransientSeq.scala
deleted file mode 100644
index 32061fd6045ce18c2944001e56e4b03de2b445f5..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/TransientSeq.scala
+++ /dev/null
@@ -1,88 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import scala.collection.SeqView
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-
-/**
- * `TransientSeq` is a special case of [[scala.collection.Seq]] meant to be backed by a source which
- * is potentially larger than memory or mutable. The `TransientSeq` constructor takes an iterator
- * generator- a no arg function which returns an iterator over the backing collection.
- * `TransientSeq` is a [[scala.collection.SeqView]], and thus it applies all transformations lazily.
- * Furthermore, when `force` is called on a `TransientSeq`, an iterator is created, and a lazily
- * evaluated Stream is returned. Operations which truly force a `TransientSeq` into evaluation
- * (and thus creating and using an iterator on the backing collection), include `head`, `tail`,
- * `foreach`, `apply`, `length`, `size`, `contains`, `diff`, `union`, `intersect`, and `sorted`.
- *
- * From a user's perspective, a `TransientSeq` is not mutable, but the underlying collection may be
- * mutable. [[scala.collection.mutable]] mutator methods are not provided, thus users cannot mutate
- * a `TransientSeq`. If the collection underlying the iterator generator is immutable (that is, the
- * iterator generator always returns iterators containing the same elements in the same order), then
- * the `TransientSeq` can be treated as immutable.  If the backing collection is not immutable, then
- * `TransientSeq` will not be immutable, because it will not guarantee referential transparency.
- *
- * It is important when using a `TransientSeq` to limit the calls to operations which create and use
- * an iterator on the backing collection.  For instance:
- * {{
- *    val tseq = new TransientSeq(...)
- *
- *    // Slow! creates 3 separate iterators over the backing collection
- *    val first = tseq(0)
- *    val second = tseq(1)
- *    val third = tseq(2)
- *
- *    // Better, only creates a single iterator over the backing collection
- *    val elements = tseq.take(3).toList // forces first 3 elements of transient representation
- *                                       // to in-memory list
- *    val first = elements(0)
- *    val second = elements(1)
- *    val third = elements(2)
- * }}
- *
- * Be careful when using `toList` to force the `TransientSeq` to an in-memory list.  This could
- * cause an [[java.lang.OutOfMemoryError]] if the items in the `TransientSeq` do not fit in the
- * VM's heap space. In this case, use filters, aggregation, take, or drop to reduce the number of
- * elements.
- *
- * Additionally, `TransientSeq`s should not be passed to recursive methods, instead, `force` the
- * `TransientSeq` into a `Stream` and then pass it to the recursive method, otherwise a new
- * iterator will be created for every recursive call to `tail`.
- *
- * @param genItr function produces a new iterator of the underlying collection of elements.
- * @tparam T type of contained elements.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-class TransientSeq[+T](genItr: () => Iterator[T]) extends SeqView[T, Stream[T]] {
-  require(genItr != null)
-
-  override def length: Int = iterator.size
-
-  override def apply(idx: Int): T = iterator.drop(idx).next()
-
-  override def iterator: Iterator[T] = genItr()
-
-  protected val underlying: Stream[T] = Stream()
-
-  override def toString: String = "TransientSeq(...)"
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/KijiInputFormat.scala b/src/main/scala/org/kiji/express/flow/framework/KijiInputFormat.scala
deleted file mode 100644
index c2f5d5ad246d3af999507e7af1010f134c6331c7..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/KijiInputFormat.scala
+++ /dev/null
@@ -1,131 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import scala.collection.JavaConverters._
-
-import com.google.common.base.Preconditions.checkNotNull
-import org.apache.hadoop.hbase.mapreduce.TableSplit
-import org.apache.hadoop.mapred.InputFormat
-import org.apache.hadoop.mapred.InputSplit
-import org.apache.hadoop.mapred.JobConf
-import org.apache.hadoop.mapred.RecordReader
-import org.apache.hadoop.mapred.Reporter
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.util.Resources.doAndClose
-import org.kiji.express.flow.util.Resources.doAndRelease
-import org.kiji.mapreduce.framework.KijiConfKeys
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiRegion
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiURI
-import org.kiji.schema.impl.HBaseKijiTable
-
-/**
- * Tells the MapReduce framework how to divide a Kiji table into input splits for tasks,
- * and how to read records from those splits.
- *
- * MapReduce views a data set as a collection of key-value pairs divided into input splits,
- * where each input split is processed by a MapReduce task. This input format divides a Kiji
- * table into one input split per HBase region in the table. It also provides access to a record
- * reader (specifically [[org.kiji.express.flow.framework.KijiRecordReader]]) which knows how to
- * read rows from a Kiji table as key-value pairs.
- *
- * A MapReduce job reading from a Kiji table as part of the KijiExpress framework should be
- * configured with this input format. The job using this input format should have a configuration
- * containing a serialized `KijiDataRequest` at the key `kiji.input.data.request` and a Kiji URI
- * addressing the target table at the key `kiji.input.table.uri`.
- *
- * The Kiji framework already has an input format for reading from Kiji tables,
- * but it is written against a newer MapReduce API than the one supported by Cascading. This
- * input format exists to address this compatibility issue.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-@Inheritance.Sealed
-final class KijiInputFormat
-    extends InputFormat[KijiKey, KijiValue] {
-  /**
-   * Creates one input split per HBase region of a Kiji table.
-   *
-   * @param configuration containing a Kiji URI at the key `kiji.input.table.uri` that addresses
-   *     the table to generate splits for.
-   * @param numSplits is a MapReduce framework hint for the number of splits to produce,
-   *     and is ignored here.
-   * @return one input split per HBase region of the Kiji table.
-   */
-  override def getSplits(configuration: JobConf, numSplits: Int): Array[InputSplit] = {
-    val uriString: String = checkNotNull(configuration.get(KijiConfKeys.KIJI_INPUT_TABLE_URI))
-    val inputTableURI: KijiURI = KijiURI.newBuilder(uriString).build()
-
-    doAndRelease(Kiji.Factory.open(inputTableURI, configuration)) { kiji: Kiji =>
-      doAndRelease(kiji.openTable(inputTableURI.getTable())) { table: KijiTable =>
-        doAndClose(HBaseKijiTable.downcast(table).openHTableConnection()) { htable =>
-          table.getRegions().asScala.map { region: KijiRegion =>
-            val startKey: Array[Byte] = region.getStartKey()
-            // TODO(KIJIMR-65): For now pick the first available location (ie. region server),
-            //     if any.
-            val location: String = {
-              if (region.getLocations().isEmpty()) {
-                null
-              } else {
-                region.getLocations().iterator().next()
-              }
-            }
-            val tableSplit: TableSplit = {
-              new TableSplit(
-                  htable.getTableName(),
-                  startKey,
-                  region.getEndKey(),
-                  location)
-            }
-            new KijiTableSplit(tableSplit)
-          }
-          .toArray
-        }
-      }
-    }
-  }
-
-  /**
-   * Creates a record reader that will read rows from a region of a Kiji table as key-value pairs.
-   *
-   * @param split identifies the HBase region of the Kiji table that should be read.
-   * @param configuration containing a serialized data request at the key
-   *     `kiji.input.data.request` which will be used to configure how data will be read from
-   *     HBase.
-   * @param reporter is provided by the MapReduce framework as a means for tasks to report status,
-   *     and is ignored here.
-   * @return a record reader that will read rows from a region of a Kiji table as key-value pairs.
-   */
-  override def getRecordReader(
-      split: InputSplit,
-      configuration: JobConf,
-      reporter: Reporter): RecordReader[KijiKey, KijiValue] = {
-    split match {
-      // TODO: Use reporter to report progress.
-      case kijiSplit: KijiTableSplit => new KijiRecordReader(kijiSplit, configuration)
-      case _ => sys.error("KijiInputFormat requires a KijiTableSplit.")
-    }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/KijiKey.scala b/src/main/scala/org/kiji/express/flow/framework/KijiKey.scala
deleted file mode 100644
index 978b177750571f5cc26b399c97a62451150e8fbf..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/KijiKey.scala
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.{EntityId => JEntityId}
-
-/**
- * A reusable container for [[org.kiji.schema.EntityId]]s.
- *
- * The MapReduce framework views a data set as a collection of key-value pairs,
- * and likes to read those pairs into a reusable instance of the key or value class. When a row
- * is read from a Kiji table, its [[org.kiji.schema.EntityId]] is emitted as the key, and
- * [[org.kiji.schema.KijiRowData]] is emitted as the value. Because instances of
- * [[org.kiji.schema.EntityId]] are not reusable, this class is provided to give the MapReduce
- * framework a reusable container.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-final class KijiKey {
-  /** The entity id contained by this instance. */
-  private var currentKey: JEntityId = null
-
-  /**
-   *  Retrieves the [[org.kiji.schema.EntityId]] wrapped by this instance.
-   *
-   * @return the entity id contained in this instance.
-   */
-  def get(): JEntityId = currentKey
-
-  /**
-   * Sets the [[org.kiji.schema.EntityId]] contained in this instance.
-   *
-   * @param key that will be wrapped by this instance.
-   */
-  def set(key: JEntityId) {
-    currentKey = key
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/KijiRecordReader.scala b/src/main/scala/org/kiji/express/flow/framework/KijiRecordReader.scala
deleted file mode 100644
index 721b8aa2dd1efa34f9f08bd9698d95ab8d101dae..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/KijiRecordReader.scala
+++ /dev/null
@@ -1,203 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import org.apache.commons.codec.binary.Base64
-import org.apache.commons.lang.SerializationUtils
-import org.apache.hadoop.conf.Configuration
-import org.apache.hadoop.hbase.util.Bytes
-import org.apache.hadoop.mapred.RecordReader
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.util.GenericCellSpecs
-import org.kiji.express.flow.util.SpecificCellSpecs
-import org.kiji.express.flow.util.Resources.doAndRelease
-import org.kiji.mapreduce.framework.KijiConfKeys
-import org.kiji.schema.HBaseEntityId
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiDataRequest
-import org.kiji.schema.KijiRowData
-import org.kiji.schema.KijiRowScanner
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiTableReader
-import org.kiji.schema.KijiTableReader.KijiScannerOptions
-import org.kiji.schema.KijiURI
-import org.kiji.schema.hbase.HBaseScanOptions
-
-/**
- * Reads rows from an HBase region of a Kiji table as key-value pairs that can be used with the
- * MapReduce framework.
- *
- * MapReduce views a data set as a collection of key-value pairs divided into input splits,
- * where each input split is processed by a MapReduce task. This record reader can scan a subset
- * of rows from a Kiji table (really an HBase region of the Kiji table identified by an input
- * split) and transform them into key-value pairs that can be processed by the MapReduce framework.
- *
- * Key-value pairs are obtained from Kiji rows as follows. For each row read,
- * the row's entity id is used as key (by populating an instance of
- * [[org.kiji.express.flow.framework.KijiKey]]) and the row data itself is used as value (by
- * populating an instance of [[org.kiji.express.flow.framework.KijiValue]]). The classes
- * [[org.kiji.express.flow.framework.KijiKey]] and [[org.kiji.express.flow.framework.KijiValue]]
- * are simply reusable containers wrapping entity ids and row data.
- *
- * A MapReduce job reading from a Kiji table as part of the KijiExpress framework should be
- * configured to use [[org.kiji.express.flow.framework.KijiInputFormat]],
- * which allow the job to use this record reader. The job using this record reader should have a
- * configuration containing a serialized `KijiDataRequest` at the key `kiji.input.data.request`
- * and a Kiji URI addressing the target table at the key `kiji.input.table.uri`.
- *
- * @param split identifying the HBase region of a Kiji table whose rows will be scanned.
- * @param configuration containing the Kiji URI of the target Kiji table, and a serialized data
- *     request.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-@Inheritance.Sealed
-final class KijiRecordReader(
-    private val split: KijiTableSplit,
-    private val configuration: Configuration)
-    extends RecordReader[KijiKey, KijiValue] {
-  if (!split.isInstanceOf[KijiTableSplit]) {
-    sys.error("KijiRecordReader received an InputSplit that was not a KijiTableSplit.")
-  }
-
-  /** The data request used to read from the Kiji table. */
-  private val dataRequest: KijiDataRequest = {
-    // Get data request from the job configuration.
-    val dataRequestB64: String = {
-      Option(configuration.get(KijiConfKeys.KIJI_INPUT_DATA_REQUEST)) match {
-        case Some(str) => str
-        case None => sys.error("Missing data request in job configuration.")
-      }
-    }
-
-    val dataRequestBytes: Array[Byte] = Base64.decodeBase64(Bytes.toBytes(dataRequestB64))
-    SerializationUtils.deserialize(dataRequestBytes).asInstanceOf[KijiDataRequest]
-  }
-
-  /** A Kiji URI addressing the target table. */
-  private val inputURI: KijiURI = KijiURI
-      .newBuilder(configuration.get(KijiConfKeys.KIJI_INPUT_TABLE_URI))
-      .build()
-
-  /** A reader for the above table. */
-  private val reader: KijiTableReader = {
-    doAndRelease(Kiji.Factory.open(inputURI, configuration)) { kiji: Kiji =>
-      doAndRelease(kiji.openTable(inputURI.getTable())) { table: KijiTable =>
-        val serializedOverrides: String =
-            configuration.get(SpecificCellSpecs.CELLSPEC_OVERRIDE_CONF_KEY)
-        val cellSpecOverrides = SpecificCellSpecs.deserializeOverrides(table, serializedOverrides)
-        val completeCellSpecs =
-            SpecificCellSpecs.mergeCellSpecs(GenericCellSpecs(table), cellSpecOverrides)
-        table.getReaderFactory.openTableReader(completeCellSpecs)
-      }
-    }
-  }
-  /** Used to scan a subset of rows from the table. */
-  private val scanner: KijiRowScanner = {
-    val hbaseScannerOptions: HBaseScanOptions = new HBaseScanOptions()
-    hbaseScannerOptions.setCacheBlocks(false)
-    val scannerOptions: KijiScannerOptions = new KijiScannerOptions()
-        .setStartRow(HBaseEntityId.fromHBaseRowKey(split.getStartRow()))
-        .setStopRow(HBaseEntityId.fromHBaseRowKey(split.getEndRow()))
-        .setHBaseScanOptions(hbaseScannerOptions)
-
-    reader.getScanner(dataRequest, scannerOptions)
-  }
-  /** An iterator over the rows retrieved by the scanner. */
-  private val iterator: java.util.Iterator[KijiRowData] = scanner.iterator()
-
-  /** This prevents errors when KijiRecordReader is closed multiple times. See CHOP-56. */
-  private var isClosed: Boolean = false
-
-  /**
-   * Gets a key instance that can be populated with entity ids scanned from the table.
-   *
-   * @return a new, empty, reusable key instance that will hold entity ids scanned by this record
-   *   reader. Note that until populated, the key instance will return `null` if you attempt to
-   *   retrieve the entity id from the key.
-   */
-  override def createKey(): KijiKey = new KijiKey()
-
-  /**
-   * Gets a value instance that can be populated with row data scanned from the table.
-   *
-   * @return a new, empty, reusable value instance that will hold row data scanned by this record
-   *     reader. Note that until populated, the value instance will return `null` if you attempt
-   *     to retrieve the row data from the value.
-   */
-  override def createValue(): KijiValue = new KijiValue()
-
-  /**
-   * @return `OL` always, because it's impossible to tell how much we've read through
-   *     a particular key range, because we have no knowledge of how many rows are actually in
-   *     the range.
-   */
-  override def getPos(): Long = 0L
-
-  /**
-   * @return `0.0` always, because it's impossible to tell how much we've read through
-   *     a particular key range, because we have no knowledge of how many rows are actually in
-   *     the range.
-   */
-  override def getProgress(): Float = 0.0f
-
-  /**
-   * Scans the next row from the region of the Kiji table, and populates a key and value with the
-   * entity id and row data obtained.
-   *
-   * @param key instance to populate with the next entity id read.
-   * @param value instance to populate with the next row data read.
-   * @return `true` if a new row was scanned and the key and value populated, `false` if the end
-   *     of the region has been reached.
-   */
-  override def next(key: KijiKey, value: KijiValue): Boolean = {
-    if (iterator.hasNext()) {
-      // Read the next row and store it in the provided key/value pair.
-      val row: KijiRowData = iterator.next()
-      if (null != key) {
-        key.set(row.getEntityId())
-      }
-      if (null != value) {
-        value.set(row)
-      }
-      true
-    } else {
-      false
-    }
-  }
-
-  /**
-   * Closes the table scanner and table reader used by this instance to scan a region of a Kiji
-   * table.
-   *
-   * It is safe (but unnecessary) to call this method multiple times.
-   */
-  override def close() {
-    if (!isClosed) {
-      isClosed = true
-
-      scanner.close()
-      reader.close()
-    }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/KijiScheme.scala b/src/main/scala/org/kiji/express/flow/framework/KijiScheme.scala
deleted file mode 100644
index 9ef49e022d716577c0fc20e6e4504ff3c7974207..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/KijiScheme.scala
+++ /dev/null
@@ -1,619 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import scala.collection.JavaConverters.asScalaIteratorConverter
-
-import cascading.flow.FlowProcess
-import cascading.scheme.Scheme
-import cascading.scheme.SinkCall
-import cascading.scheme.SourceCall
-import cascading.tap.Tap
-import cascading.tuple.Fields
-import cascading.tuple.Tuple
-import cascading.tuple.TupleEntry
-import com.google.common.base.Objects
-import org.apache.avro.Schema
-import org.apache.commons.codec.binary.Base64
-import org.apache.commons.lang.SerializationUtils
-import org.apache.hadoop.conf.Configuration
-import org.apache.hadoop.hbase.HConstants
-import org.apache.hadoop.mapred.JobConf
-import org.apache.hadoop.mapred.OutputCollector
-import org.apache.hadoop.mapred.RecordReader
-import org.slf4j.Logger
-import org.slf4j.LoggerFactory
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.express.flow.ColumnFamilyInputSpec
-import org.kiji.express.flow.ColumnFamilyOutputSpec
-import org.kiji.express.flow.ColumnInputSpec
-import org.kiji.express.flow.ColumnOutputSpec
-import org.kiji.express.flow.EntityId
-import org.kiji.express.flow.FlowCell
-import org.kiji.express.flow.PagingSpec
-import org.kiji.express.flow.QualifiedColumnInputSpec
-import org.kiji.express.flow.QualifiedColumnOutputSpec
-import org.kiji.express.flow.TimeRange
-import org.kiji.express.flow.TransientSeq
-import org.kiji.express.flow.framework.serialization.KijiLocker
-import org.kiji.express.flow.util.AvroUtil
-import org.kiji.express.flow.util.Resources.doAndRelease
-import org.kiji.express.flow.util.SpecificCellSpecs
-import org.kiji.mapreduce.framework.KijiConfKeys
-import org.kiji.schema.ColumnVersionIterator
-import org.kiji.schema.EntityIdFactory
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiCell
-import org.kiji.schema.KijiDataRequest
-import org.kiji.schema.KijiDataRequestBuilder
-import org.kiji.schema.KijiRowData
-import org.kiji.schema.KijiSchemaTable
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiTableWriter
-import org.kiji.schema.KijiURI
-import org.kiji.schema.MapFamilyVersionIterator
-import org.kiji.schema.avro.AvroSchema
-import org.kiji.schema.filter.KijiColumnFilter
-import org.kiji.schema.layout.KijiTableLayout
-
-/**
- * A Kiji-specific implementation of a Cascading `Scheme`, which defines how to read and write the
- * data stored in a Kiji table.
- *
- * KijiScheme is responsible for converting rows from a Kiji table that are input to a Cascading
- * flow into Cascading tuples
- * (see `source(cascading.flow.FlowProcess, cascading.scheme.SourceCall)`) and writing output
- * data from a Cascading flow to a Kiji table
- * (see `sink(cascading.flow.FlowProcess, cascading.scheme.SinkCall)`).
- *
- * KijiScheme must be used with [[org.kiji.express.flow.framework.KijiTap]],
- * since it expects the Tap to have access to a Kiji table.  [[org.kiji.express.flow.KijiSource]]
- * handles the creation of both KijiScheme and KijiTap in KijiExpress.
- *
- * @param timeRange to include from the Kiji table.
- * @param timestampField is the optional name of a field containing the timestamp that all values
- *     in a tuple should be written to.
- *     Use None if all values should be written at the current time.
- * @param inputColumns mapping tuple field names to requests for Kiji columns.
- * @param outputColumns mapping tuple field names to specifications for Kiji columns to write out
- *     to.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-class KijiScheme(
-    private[express] val timeRange: TimeRange,
-    private[express] val timestampField: Option[Symbol],
-    @transient private[express] val inputColumns: Map[String, ColumnInputSpec] = Map(),
-    @transient private[express] val outputColumns: Map[String, ColumnOutputSpec] = Map())
-    extends Scheme[JobConf, RecordReader[KijiKey, KijiValue], OutputCollector[_, _],
-        KijiSourceContext, KijiSinkContext] {
-  import KijiScheme._
-
-  // ColumnInputSpec and ColumnOutputSpec objects cannot be correctly serialized via
-  // java.io.Serializable.  Chiefly, Avro objects including Schema and all of the Generic types
-  // are not Serializable.  By making the inputColumns and outputColumns transient and wrapping
-  // them in KijiLocker objects (which handle serialization correctly),
-  // we can work around this limitation.  Thus, the following two lines should be the only to
-  // reference `inputColumns` and `outputColumns`, because they will be null after serialization.
-  // Everything else should instead use _inputColumns.get and _outputColumns.get.
-  private val _inputColumns = KijiLocker(inputColumns)
-  private val _outputColumns = KijiLocker(outputColumns)
-
-  // Including output column keys here because we might need to read back outputs during test
-  // TODO (EXP-250): Ideally we should include outputColumns.keys here only during tests.
-  setSourceFields(buildSourceFields(_inputColumns.get.keys ++ _outputColumns.get.keys))
-  setSinkFields(buildSinkFields(_outputColumns.get, timestampField))
-
-  /**
-   * Sets any configuration options that are required for running a MapReduce job
-   * that reads from a Kiji table. This method gets called on the client machine
-   * during job setup.
-   *
-   * @param flow being built.
-   * @param tap that is being used with this scheme.
-   * @param conf to which we will add our KijiDataRequest.
-   */
-  override def sourceConfInit(
-      flow: FlowProcess[JobConf],
-      tap: Tap[JobConf, RecordReader[KijiKey, KijiValue], OutputCollector[_, _]],
-      conf: JobConf) {
-    // Build a data request.
-    val request: KijiDataRequest = buildRequest(timeRange, _inputColumns.get.values)
-
-    // Write all the required values to the job's configuration object.
-    conf.setInputFormat(classOf[KijiInputFormat])
-    conf.set(
-        KijiConfKeys.KIJI_INPUT_DATA_REQUEST,
-        Base64.encodeBase64String(SerializationUtils.serialize(request)))
-    conf.set(SpecificCellSpecs.CELLSPEC_OVERRIDE_CONF_KEY,
-        SpecificCellSpecs.serializeOverrides(_inputColumns.get))
-  }
-
-  /**
-   * Sets up any resources required for the MapReduce job. This method is called
-   * on the cluster.
-   *
-   * @param flow is the current Cascading flow being run.
-   * @param sourceCall containing the context for this source.
-   */
-  override def sourcePrepare(
-      flow: FlowProcess[JobConf],
-      sourceCall: SourceCall[KijiSourceContext, RecordReader[KijiKey, KijiValue]]) {
-    val tableUriProperty = flow.getStringProperty(KijiConfKeys.KIJI_INPUT_TABLE_URI)
-    val uri: KijiURI = KijiURI.newBuilder(tableUriProperty).build()
-
-    // Set the context used when reading data from the source.
-    sourceCall.setContext(KijiSourceContext(
-        sourceCall.getInput.createValue(),
-        uri))
-  }
-
-  /**
-   * Reads and converts a row from a Kiji table to a Cascading Tuple. This method
-   * is called once for each row on the cluster.
-   *
-   * @param flow is the current Cascading flow being run.
-   * @param sourceCall containing the context for this source.
-   * @return `true` if another row was read and it was converted to a tuple,
-   *     `false` if there were no more rows to read.
-   */
-  override def source(
-      flow: FlowProcess[JobConf],
-      sourceCall: SourceCall[KijiSourceContext, RecordReader[KijiKey, KijiValue]]): Boolean = {
-    // Get the current key/value pair.
-    val KijiSourceContext(value, tableUri) = sourceCall.getContext
-
-    // Get the next row.
-    if (sourceCall.getInput.next(null, value)) {
-      val row: KijiRowData = value.get()
-
-      // Build a tuple from this row.
-      val result: Tuple = rowToTuple(
-          _inputColumns.get,
-          getSourceFields,
-          timestampField,
-          row,
-          tableUri,
-          flow.getConfigCopy
-      )
-
-      // If no fields were missing, set the result tuple and return from this method.
-      sourceCall.getIncomingEntry.setTuple(result)
-      flow.increment(counterGroupName, counterSuccess, 1)
-
-      // We set a result tuple, return true for success.
-      return true
-    } else {
-      return false // We reached the end of the RecordReader.
-    }
-  }
-
-  /**
-   * Cleans up any resources used during the MapReduce job. This method is called
-   * on the cluster.
-   *
-   * @param flow currently being run.
-   * @param sourceCall containing the context for this source.
-   */
-  override def sourceCleanup(
-      flow: FlowProcess[JobConf],
-      sourceCall: SourceCall[KijiSourceContext, RecordReader[KijiKey, KijiValue]]) {
-    sourceCall.setContext(null)
-  }
-
-  /**
-   * Sets any configuration options that are required for running a MapReduce job
-   * that writes to a Kiji table. This method gets called on the client machine
-   * during job setup.
-   *
-   * @param flow being built.
-   * @param tap that is being used with this scheme.
-   * @param conf to which we will add our KijiDataRequest.
-   */
-  override def sinkConfInit(
-      flow: FlowProcess[JobConf],
-      tap: Tap[JobConf, RecordReader[KijiKey, KijiValue], OutputCollector[_, _]],
-      conf: JobConf) {
-    // No-op since no configuration parameters need to be set to encode data for Kiji.
-  }
-
-  /**
-   * Sets up any resources required for the MapReduce job. This method is called
-   * on the cluster.
-   *
-   * @param flow is the current Cascading flow being run.
-   * @param sinkCall containing the context for this source.
-   */
-  override def sinkPrepare(
-      flow: FlowProcess[JobConf],
-      sinkCall: SinkCall[KijiSinkContext, OutputCollector[_, _]]) {
-    // Open a table writer.
-    val uriString: String = flow.getConfigCopy.get(KijiConfKeys.KIJI_OUTPUT_TABLE_URI)
-    val uri: KijiURI = KijiURI.newBuilder(uriString).build()
-
-    val kiji: Kiji = Kiji.Factory.open(uri, flow.getConfigCopy)
-    doAndRelease(kiji.openTable(uri.getTable)) { table: KijiTable =>
-      // Set the sink context to an opened KijiTableWriter.
-      sinkCall.setContext(
-          KijiSinkContext(table.openTableWriter(), uri, kiji, table.getLayout))
-    }
-  }
-
-  /**
-   * Converts and writes a Cascading Tuple to a Kiji table. This method is called once
-   * for each row on the cluster.
-   *
-   * @param flow is the current Cascading flow being run.
-   * @param sinkCall containing the context for this source.
-   */
-  override def sink(
-      flow: FlowProcess[JobConf],
-      sinkCall: SinkCall[KijiSinkContext, OutputCollector[_, _]]) {
-    // Retrieve writer from the scheme's context.
-    val KijiSinkContext(writer, tableUri, kiji, layout) = sinkCall.getContext
-
-    // Write the tuple out.
-    val output: TupleEntry = sinkCall.getOutgoingEntry
-    putTuple(
-        _outputColumns.get,
-        tableUri,
-        kiji,
-        timestampField,
-        output,
-        writer,
-        layout,
-        flow.getConfigCopy)
-  }
-
-  /**
-   * Cleans up any resources used during the MapReduce job. This method is called
-   * on the cluster.
-   *
-   * @param flow is the current Cascading flow being run.
-   * @param sinkCall containing the context for this source.
-   */
-  override def sinkCleanup(
-      flow: FlowProcess[JobConf],
-      sinkCall: SinkCall[KijiSinkContext, OutputCollector[_, _]]) {
-    sinkCall.getContext.kiji.release()
-    sinkCall.getContext.kijiTableWriter.close()
-    sinkCall.setContext(null)
-  }
-
-  override def equals(other: Any): Boolean = {
-    other match {
-      case scheme: KijiScheme => {
-        _inputColumns.get == scheme._inputColumns.get &&
-            _outputColumns.get == scheme._outputColumns.get &&
-            timestampField == scheme.timestampField &&
-            timeRange == scheme.timeRange
-      }
-      case _ => false
-    }
-  }
-
-  override def hashCode(): Int = Objects.hashCode(
-      _inputColumns.get,
-      _outputColumns.get,
-      timeRange,
-      timestampField)
-}
-
-/**
- * Companion object for KijiScheme.
- *
- * Contains constants and helper methods for converting between Kiji rows and Cascading tuples,
- * building Kiji data requests, and some utility methods for handling Cascading fields.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-object KijiScheme {
-  type HadoopScheme = Scheme[JobConf, RecordReader[_, _], OutputCollector[_, _], _, _]
-
-  private val logger: Logger = LoggerFactory.getLogger(classOf[KijiScheme])
-
-  /** Hadoop mapred counter group for KijiExpress. */
-  private[express] val counterGroupName = "kiji-express"
-  /** Counter name for the number of rows successfully read. */
-  private[express] val counterSuccess = "ROWS_SUCCESSFULLY_READ"
-  /** Field name containing a row's [[org.kiji.schema.EntityId]]. */
-  val entityIdField: String = "entityId"
-  /** Default number of qualifiers to retrieve when paging in a map type family.*/
-  private val qualifierPageSize: Int = 1000
-
-  /**
-   * Converts a KijiRowData to a Cascading tuple.
-   *
-   * If there is no data in a column in this row, the value of that field in the tuple will be an
-   * empty iterable of cells.
-   *
-   * This is used in the `source` method of KijiScheme, for reading rows from Kiji into
-   * KijiExpress tuples.
-   *
-   * @param columns Mapping from field name to column definition.
-   * @param fields Field names of desired tuple elements.
-   * @param timestampField is the optional name of a field containing the timestamp that all values
-   *     in a tuple should be written to.
-   *     Use None if all values should be written at the current time.
-   * @param row to convert to a tuple.
-   * @param tableUri is the URI of the Kiji table.
-   * @param configuration identifying the cluster to use when building EntityIds.
-   * @return a tuple containing the values contained in the specified row.
-   */
-  private[express] def rowToTuple(
-      columns: Map[String, ColumnInputSpec],
-      fields: Fields,
-      timestampField: Option[Symbol],
-      row: KijiRowData,
-      tableUri: KijiURI,
-      configuration: Configuration
-  ): Tuple = {
-    val result: Tuple = new Tuple()
-
-    // Add the row's EntityId to the tuple.
-    val entityId: EntityId = EntityId.fromJavaEntityId(row.getEntityId)
-    result.add(entityId)
-
-    def rowToTupleColumnFamily(cf: ColumnFamilyInputSpec): Unit = {
-      if (row.containsColumn(cf.family)) {
-        cf.paging match {
-          case PagingSpec.Off => {
-            val stream: Seq[FlowCell[_]] = row
-                .iterator(cf.family)
-                .asScala
-                .toList
-                .map { kijiCell: KijiCell[_] => FlowCell(kijiCell) }
-            result.add(stream)
-          }
-          case PagingSpec.Cells(pageSize) => {
-            def genItr(): Iterator[FlowCell[_]] = {
-              new MapFamilyVersionIterator(row, cf.family, qualifierPageSize, pageSize)
-                  .asScala
-                  .map { entry: MapFamilyVersionIterator.Entry[_] =>
-                    FlowCell(
-                        cf.family,
-                        entry.getQualifier,
-                        entry.getTimestamp,
-                        AvroUtil.avroToScala(entry.getValue))
-              }
-            }
-            result.add(new TransientSeq(genItr))
-          }
-        }
-      } else {
-        result.add(Seq())
-      }
-    }
-
-    def rowToTupleQualifiedColumn(qc: QualifiedColumnInputSpec): Unit = {
-      if (row.containsColumn(qc.family, qc.qualifier)) {
-        qc.paging match {
-          case PagingSpec.Off => {
-            val stream: Seq[FlowCell[_]] = row
-                .iterator(qc.family, qc.qualifier)
-                .asScala
-                .toList
-                .map { kijiCell: KijiCell[_] => FlowCell(kijiCell) }
-            result.add(stream)
-          }
-          case PagingSpec.Cells(pageSize) => {
-            def genItr(): Iterator[FlowCell[_]] = {
-              new ColumnVersionIterator(row, qc.family, qc.qualifier, pageSize)
-                  .asScala
-                  .map { entry: java.util.Map.Entry[java.lang.Long,_] =>
-                    FlowCell(
-                      qc.family,
-                      qc.qualifier,
-                      entry.getKey,
-                      AvroUtil.avroToScala(entry.getValue)
-                    )
-                  }
-            }
-            result.add(new TransientSeq(genItr))
-          }
-        }
-      } else {
-        result.add(Seq())
-      }
-    }
-
-    // Get rid of the entity id and timestamp fields, then map over each field to add a column
-    // to the tuple.
-    fields
-        .iterator()
-        .asScala
-        .filter { field => field.toString != entityIdField }
-        .filter { field => field.toString != timestampField.getOrElse("") }
-        .map { field => columns(field.toString) }
-        // Build the tuple, by adding each requested value into result.
-        .foreach {
-          case cf: ColumnFamilyInputSpec => rowToTupleColumnFamily(cf)
-          case qc: QualifiedColumnInputSpec => rowToTupleQualifiedColumn(qc)
-        }
-
-    return result
-  }
-
-  /**
-   * Writes a Cascading tuple to a Kiji table.
-   *
-   * This is used in KijiScheme's `sink` method.
-   *
-   * @param columns mapping field names to column definitions.
-   * @param tableUri of the Kiji table.
-   * @param kiji is the Kiji instance the table belongs to.
-   * @param timestampField is the optional name of a field containing the timestamp that all values
-   *     in a tuple should be written to.
-   *     Use None if all values should be written at the current time.
-   * @param output to write out to Kiji.
-   * @param writer to use for writing to Kiji.
-   * @param layout Kiji table layout.
-   * @param configuration identifying the cluster to use when building EntityIds.
-   */
-  private[express] def putTuple(
-      columns: Map[String, ColumnOutputSpec],
-      tableUri: KijiURI,
-      kiji: Kiji,
-      timestampField: Option[Symbol],
-      output: TupleEntry,
-      writer: KijiTableWriter,
-      layout: KijiTableLayout,
-      configuration: Configuration
-  ) {
-    // Get the entityId.
-    val entityId = output
-        .getObject(entityIdField)
-        .asInstanceOf[EntityId]
-        .toJavaEntityId(EntityIdFactory.getFactory(layout))
-
-    // Get a timestamp to write the values to, if it was specified by the user.
-    val timestamp: Long = timestampField match {
-      case Some(field) => output.getObject(field.name).asInstanceOf[Long]
-      case None => HConstants.LATEST_TIMESTAMP
-    }
-
-    columns.keys.iterator
-        .foreach { field =>
-          val value = output.getObject(field)
-          val col: ColumnOutputSpec = columns(field)
-
-          val qualifier = col match {
-            case qc: QualifiedColumnOutputSpec => qc.qualifier
-            case cf: ColumnFamilyOutputSpec => output.getString(cf.qualifierSelector.name)
-          }
-
-          writer.put(entityId, col.family, qualifier, timestamp, col.encode(value))
-      }
-  }
-
-  /**
-   * Gets a schema from the reader schema.
-   *
-   * @param readerSchema to find the schema for.
-   * @param schemaTable to look up IDs in.
-   * @return the resolved Schema.
-   */
-  private[express] def resolveSchemaFromJSONOrUid(
-      readerSchema: AvroSchema,
-      schemaTable: KijiSchemaTable
-  ): Schema = {
-    Option(readerSchema.getJson) match {
-      case None => schemaTable.getSchema(readerSchema.getUid)
-      case Some(json) => new Schema.Parser().parse(json)
-    }
-  }
-
-  /**
-   * Builds a data request out of the timerange and list of column requests.
-   *
-   * @param timeRange of cells to retrieve.
-   * @param columns to retrieve.
-   * @return data request configured with timeRange and columns.
-   */
-  private[express] def buildRequest(
-      timeRange: TimeRange,
-      columns: Iterable[ColumnInputSpec]
-  ): KijiDataRequest = {
-    def addColumn(
-        builder: KijiDataRequestBuilder,
-        column: ColumnInputSpec
-    ): KijiDataRequestBuilder.ColumnsDef = {
-      val kijiFilter: KijiColumnFilter = column
-          .filter
-          .map { _.toKijiColumnFilter }
-          .getOrElse(null)
-      builder.newColumnsDef()
-          .withMaxVersions(column.maxVersions)
-          .withFilter(kijiFilter)
-          .withPageSize(column.paging.cellsPerPage.getOrElse(0))
-          .add(column.columnName)
-    }
-
-    val requestBuilder: KijiDataRequestBuilder = KijiDataRequest.builder()
-        .withTimeRange(timeRange.begin, timeRange.end)
-
-    columns
-        .foldLeft(requestBuilder) { (builder, column) =>
-          addColumn(builder, column)
-          builder
-        }
-        .build()
-  }
-
-  /**
-   * Transforms a list of field names into a Cascading [[cascading.tuple.Fields]] object.
-   *
-   * @param fieldNames is a list of field names.
-   * @return a Fields object containing the names.
-   */
-  private def toField(fieldNames: Iterable[Comparable[_]]): Fields = {
-    new Fields(fieldNames.toArray:_*)
-  }
-
-  /**
-   * Builds the list of tuple fields being read by a scheme. The special field name
-   * "entityId" will be included to hold entity ids from the rows of Kiji tables.
-   *
-   * @param fieldNames is a list of field names that a scheme should read.
-   * @return is a collection of fields created from the names.
-   */
-  private[express] def buildSourceFields(fieldNames: Iterable[String]): Fields = {
-    toField(Set(entityIdField) ++ fieldNames)
-  }
-
-  /**
-   * Builds the list of tuple fields being written by a scheme. The special field name "entityId"
-   * will be included to hold entity ids that values should be written to. Any fields that are
-   * specified as qualifiers for a map-type column family will also be included. A timestamp field
-   * can also be included, identifying a timestamp that all values will be written to.
-   *
-   * @param columns is the column requests for this Scheme, with the names of each of the
-   *     fields that contain data to write to Kiji.
-   * @param timestampField is the optional name of a field containing the timestamp that all values
-   *     in a tuple should be written to.
-   *     Use None if all values should be written at the current time.
-   * @return a collection of fields created from the parameters.
-   */
-  private[express] def buildSinkFields(
-      columns: Map[String, ColumnOutputSpec],
-      timestampField: Option[Symbol]
-  ): Fields = {
-    toField(Set(entityIdField)
-        ++ columns.keys
-        ++ extractQualifierSelectors(columns)
-        ++ timestampField.map { _.name } )
-  }
-
-  /**
-   * Extracts the names of qualifier selectors from the column requests for a Scheme.
-   *
-   * @param columns is the column requests for a Scheme.
-   * @return the names of fields that are qualifier selectors.
-   */
-  private[express] def extractQualifierSelectors(
-      columns: Map[String, ColumnOutputSpec]
-  ): Iterator[String] = {
-    columns.valuesIterator.collect {
-      case x: ColumnFamilyOutputSpec => x.qualifierSelector.name
-    }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/KijiSinkContext.scala b/src/main/scala/org/kiji/express/flow/framework/KijiSinkContext.scala
deleted file mode 100644
index 9086769512b6087d2d804dcd5514420d2c422a4f..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/KijiSinkContext.scala
+++ /dev/null
@@ -1,48 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.KijiTableWriter
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiURI
-import org.kiji.schema.layout.KijiTableLayout
-
-/**
- * Container for the table writer and Kiji table layout required during a sink
- * operation to write the output of a map reduce task back to a Kiji table.
- * This is configured during the sink prepare operation.
- *
- * @param kijiTableWriter is a writer object for this Kiji table.
- * @param tableUri of the Kiji table.
- * @param kiji is the Kiji instance for this table, used to access the schema table.
- * @param kijiLayout is the layout for this Kiji table.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] case class KijiSinkContext (
-    kijiTableWriter: KijiTableWriter,
-    tableUri: KijiURI,
-    kiji: Kiji,
-    kijiLayout: KijiTableLayout) {
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/KijiSourceContext.scala b/src/main/scala/org/kiji/express/flow/framework/KijiSourceContext.scala
deleted file mode 100644
index adc122346f0d9a469bf0fa6897927eae72000cb9..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/KijiSourceContext.scala
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.KijiURI
-
-/**
- * Container for a Kiji row data and Kiji table layout object that is required by a map reduce
- * task while reading from a Kiji table.
- *
- * @param rowContainer is the representation of a Kiji row.
- * @param tableUri is the URI of the Kiji table.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] case class KijiSourceContext (
-    rowContainer: KijiValue,
-    tableUri: KijiURI) {
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/KijiTableSplit.scala b/src/main/scala/org/kiji/express/flow/framework/KijiTableSplit.scala
deleted file mode 100644
index 61c9cc1c3516a87dc2bc38012d25f1daea8e6ec0..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/KijiTableSplit.scala
+++ /dev/null
@@ -1,96 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import java.io.DataInput
-import java.io.DataOutput
-
-import org.apache.hadoop.hbase.mapreduce.TableSplit
-import org.apache.hadoop.mapred.InputSplit
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-
-/**
- * An input split that specifies a region of rows from a Kiji table to be processed by a MapReduce
- * task. This input split stores exactly the same data as `TableSplit`, but does a better job
- * handling default split sizes.
- *
- * @param split to which functionality is delegated.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-final private[express] class KijiTableSplit(
-    split: TableSplit)
-    extends InputSplit {
-  /**
-   * No argument constructor for KijiTableSplit so that it can be constructed via reflection. This
-   * is required to be a separate constructor so that java has access to it.
-   */
-  // scalastyle:off public.methods.have.type
-  def this() = this(new TableSplit())
-  // scalastyle:on public.methods.have.type
-
-  /**
-   * Returns the length of the split.
-   * TODO: EXP-180: KijiTableSplit should return a better estimate in getLength
-   *
-   * @return the length of the split.
-   * @see org.apache.hadoop.mapreduce.InputSplit#getLength()
-   */
-  override def getLength(): Long = split.getLength()
-
-  /**
-   * Get the list of hostnames where the input split is located.
-   *
-   * @return a list of hostnames defining this input split.
-   */
-  override def getLocations(): Array[String] = split.getLocations()
-
-  /**
-   * Implementation of Hadoop's writable deserialization.
-   *
-   * @param input data stream containing data to populate this split with.
-   */
-  override def readFields(input: DataInput) {
-    split.readFields(input)
-  }
-
-  /**
-   * Implementation of Hadoop's writable serialization.
-   *
-   * @param output data stream to populate with the data this split contains.
-   */
-  override def write(output: DataOutput) {
-    split.write(output)
-  }
-
-  /**
-   * @return the HBase row key of the first row processed by this input split.
-   */
-  def getStartRow(): Array[Byte] = split.getStartRow()
-
-  /**
-   * @return the HBase row key of the last row processed by this input split.
-   */
-  def getEndRow(): Array[Byte] = split.getEndRow()
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/KijiTap.scala b/src/main/scala/org/kiji/express/flow/framework/KijiTap.scala
deleted file mode 100644
index 3c49b54c0699d17898a33ec15036bec82bb886d0..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/KijiTap.scala
+++ /dev/null
@@ -1,303 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import java.util.UUID
-
-import cascading.flow.FlowProcess
-import cascading.flow.hadoop.HadoopFlowProcess
-import cascading.scheme.Scheme
-import cascading.tap.Tap
-import cascading.tap.hadoop.io.HadoopTupleEntrySchemeCollector
-import cascading.tap.hadoop.io.HadoopTupleEntrySchemeIterator
-import cascading.tuple.TupleEntryCollector
-import cascading.tuple.TupleEntryIterator
-import com.google.common.base.Objects
-
-import org.apache.hadoop.conf.Configuration
-import org.apache.hadoop.mapred.JobConf
-import org.apache.hadoop.mapred.OutputCollector
-import org.apache.hadoop.mapred.RecordReader
-import org.apache.hadoop.mapred.lib.NullOutputFormat
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.express.flow._
-import org.kiji.express.flow.util.Resources.doAndRelease
-import org.kiji.mapreduce.framework.KijiConfKeys
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiURI
-import org.kiji.schema.layout.KijiTableLayout
-
-/**
- * A Kiji-specific implementation of a Cascading `Tap`, which defines the location of a Kiji table.
- *
- * KijiTap is responsible for configuring a MapReduce job with the correct input format for reading
- * from a Kiji table.
- *
- * KijiTap must be used with [[org.kiji.express.flow.framework.KijiScheme]] to perform decoding of
- * cells in a Kiji table. [[org.kiji.express.flow.KijiSource]] handles the creation of both
- * KijiScheme and KijiTap in KijiExpress.
- *
- * @param uri of the Kiji table to read or write from.
- * @param scheme that will convert data read from Kiji into Cascading's tuple model.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-class KijiTap(
-    // This is not a val because KijiTap needs to be serializable and KijiURI is not.
-    uri: KijiURI,
-    private val scheme: KijiScheme)
-    extends Tap[JobConf, RecordReader[KijiKey, KijiValue], OutputCollector[_, _]](
-        scheme.asInstanceOf[Scheme[JobConf, RecordReader[KijiKey, KijiValue],
-            OutputCollector[_, _], _, _]]) {
-
-  /** Address of the table to read from or write to. */
-  private val tableUri: String = uri.toString()
-
-  /** Unique identifier for this KijiTap instance. */
-  private val id: String = UUID.randomUUID().toString()
-
-  /**
-   * Sets any configuration options that are required for running a MapReduce job
-   * that reads from a Kiji table. This method gets called on the client machine
-   * during job setup.
-   *
-   * @param flow being built.
-   * @param conf to which we will add the table uri.
-   */
-  override def sourceConfInit(flow: FlowProcess[JobConf], conf: JobConf) {
-    // Configure the job's input format.
-    conf.setInputFormat(classOf[KijiInputFormat])
-
-    // Store the input table.
-    conf.set(KijiConfKeys.KIJI_INPUT_TABLE_URI, tableUri)
-
-    super.sourceConfInit(flow, conf)
-  }
-
-  /**
-   * Sets any configuration options that are required for running a MapReduce job
-   * that writes to a Kiji table. This method gets called on the client machine
-   * during job setup.
-   *
-   * @param flow being built.
-   * @param conf to which we will add the table uri.
-   */
-  override def sinkConfInit(flow: FlowProcess[JobConf], conf: JobConf) {
-    // TODO(CHOP-35): Use an output format that writes to HFiles.
-    // Configure the job's output format.
-    conf.setOutputFormat(classOf[NullOutputFormat[_, _]])
-
-    // Store the output table.
-    conf.set(KijiConfKeys.KIJI_OUTPUT_TABLE_URI, tableUri)
-
-    super.sinkConfInit(flow, conf)
-  }
-
-  /**
-   * Provides a string representing the resource this `Tap` instance represents.
-   *
-   * @return a java UUID representing this KijiTap instance. Note: This does not return the uri of
-   *     the Kiji table being used by this tap to allow jobs that read from or write to the same
-   *     table to have different data request options.
-   */
-  override def getIdentifier(): String = id
-
-  /**
-   * Opens any resources required to read from a Kiji table.
-   *
-   * @param flow being run.
-   * @param recordReader that will read from the desired Kiji table.
-   * @return an iterator that reads rows from the desired Kiji table.
-   */
-  override def openForRead(
-      flow: FlowProcess[JobConf],
-      recordReader: RecordReader[KijiKey, KijiValue]): TupleEntryIterator = {
-    val modifiedFlow = if (flow.getStringProperty(KijiConfKeys.KIJI_INPUT_TABLE_URI) == null) {
-      // TODO CHOP-71 Remove this hack which is introduced by a scalding bug:
-      // https://github.com/twitter/scalding/issues/369
-      // This hack is only required for testing (HadoopTest Mode)
-      val jconf = flow.getConfigCopy
-      val fp = new HadoopFlowProcess(jconf)
-      sourceConfInit(fp, jconf)
-      fp
-    } else {
-      flow
-    }
-    new HadoopTupleEntrySchemeIterator(
-        modifiedFlow,
-        this.asInstanceOf[Tap[JobConf, RecordReader[_, _], OutputCollector[_, _]]],
-        recordReader)
-  }
-
-  /**
-   * Opens any resources required to write from a Kiji table.
-   *
-   * @param flow being run.
-   * @param outputCollector that will write to the desired Kiji table. Note: This is ignored
-   *     currently since writing to a Kiji table is currently implemented without using an output
-   *     format by writing to the table directly from
-   *     [[org.kiji.express.flow.framework.KijiScheme]].
-   * @return a collector that writes tuples to the desired Kiji table.
-   */
-  override def openForWrite(
-      flow: FlowProcess[JobConf],
-      outputCollector: OutputCollector[_, _]): TupleEntryCollector = {
-    new HadoopTupleEntrySchemeCollector(
-        flow,
-        this.asInstanceOf[Tap[JobConf, RecordReader[_, _], OutputCollector[_, _]]],
-        outputCollector)
-  }
-
-  /**
-   * Builds any resources required to read from or write to a Kiji table.
-   *
-   * Note: KijiExpress currently does not support automatic creation of Kiji tables.
-   *
-   * @param conf containing settings for this flow.
-   * @return true if required resources were created successfully.
-   * @throws UnsupportedOperationException always.
-   */
-  override def createResource(conf: JobConf): Boolean = {
-    throw new UnsupportedOperationException("KijiTap does not support creating tables for you.")
-  }
-
-  /**
-   * Deletes any unnecessary resources used to read from or write to a Kiji table.
-   *
-   * Note: KijiExpress currently does not support automatic deletion of Kiji tables.
-   *
-   * @param conf containing settings for this flow.
-   * @return true if superfluous resources were deleted successfully.
-   * @throws UnsupportedOperationException always.
-   */
-  override def deleteResource(conf: JobConf): Boolean = {
-    throw new UnsupportedOperationException("KijiTap does not support deleting tables for you.")
-  }
-
-  /**
-   * Determines if the Kiji table this `Tap` instance points to exists.
-   *
-   * @param conf containing settings for this flow.
-   * @return true if the target Kiji table exists.
-   */
-  override def resourceExists(conf: JobConf): Boolean = {
-    val uri: KijiURI = KijiURI.newBuilder(tableUri).build()
-
-    doAndRelease(Kiji.Factory.open(uri, conf)) { kiji: Kiji =>
-      kiji.getTableNames().contains(uri.getTable())
-    }
-  }
-
-  /**
-   * Gets the time that the target Kiji table was last modified.
-   *
-   * Note: This will always return the current timestamp.
-   *
-   * @param conf containing settings for this flow.
-   * @return the current time.
-   */
-  override def getModifiedTime(conf: JobConf): Long = System.currentTimeMillis()
-
-  override def equals(other: Any): Boolean = {
-    other match {
-      case tap: KijiTap => (tableUri == tap.tableUri) && (scheme == tap.scheme) && (id == tap.id)
-      case _ => false
-    }
-  }
-
-  override def hashCode(): Int = Objects.hashCode(tableUri, scheme, id)
-
-  /**
-   * Checks whether the instance, tables, and columns this tap uses can be accessed.
-   *
-   * @throws KijiExpressValidationException if the tables and columns are not accessible when this
-   *    is called.
-   */
-  private[express] def validate(conf: JobConf): Unit = {
-    val kijiUri: KijiURI = KijiURI.newBuilder(tableUri).build()
-    KijiTap.validate(kijiUri, scheme.inputColumns, scheme.outputColumns, conf)
-  }
-}
-
-@ApiAudience.Framework
-@ApiStability.Experimental
-object KijiTap {
-  /**
-   * Checks whether the instance, tables, and columns specified can be accessed.
-   *
-   * @throws KijiExpressValidationException if the tables and columns are not accessible when this
-   *    is called.
-   */
-  private[express] def validate(
-      kijiUri: KijiURI,
-      inputColumns: Map[String, ColumnInputSpec],
-      outputColumns: Map[String, ColumnOutputSpec],
-      conf: Configuration) {
-    // Try to open the Kiji instance.
-    val kiji: Kiji =
-        try {
-          Kiji.Factory.open(kijiUri, conf)
-        } catch {
-          case e: Exception =>
-            throw new InvalidKijiTapException(
-                "Error opening Kiji instance: %s\n".format(kijiUri.getInstance()) + e.getMessage)
-        }
-
-    // Try to open the table.
-    val table: KijiTable =
-        try {
-          kiji.openTable(kijiUri.getTable())
-        } catch {
-          case e: Exception =>
-            throw new InvalidKijiTapException(
-                "Error opening Kiji table: %s\n".format(kijiUri.getTable()) + e.getMessage)
-        } finally {
-          kiji.release() // Release the Kiji instance.
-        }
-
-    // Check the columns are valid
-    val tableLayout: KijiTableLayout = table.getLayout
-    table.release() // Release the KijiTable.
-
-    // Get a list of columns that don't exist
-    val inputColumnNames: Seq[KijiColumnName] = inputColumns.values.map(_.columnName).toList
-    val outputColumnNames: Seq[KijiColumnName] = outputColumns.values.map(_.columnName).toList
-
-    val nonExistentColumnErrors = (inputColumnNames ++ outputColumnNames)
-        // Filter for columns that don't exist
-        .filter( { case colname => !tableLayout.exists(colname) } )
-        .map { column =>
-            "One or more columns does not exist in the table %s: %s\n".format(table.getName, column)
-        }
-
-    val allErrors = nonExistentColumnErrors
-
-    // Combine all error strings.
-    if (!allErrors.isEmpty) {
-      throw new InvalidKijiTapException(
-          "Errors found in validating Tap: %s".format(allErrors.mkString(", \n"))
-      )
-    }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/KijiValue.scala b/src/main/scala/org/kiji/express/flow/framework/KijiValue.scala
deleted file mode 100644
index caa90506134daeea28a85ec900373f917fd75eba..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/KijiValue.scala
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.KijiRowData
-
-/**
- * A reusable container for [[org.kiji.schema.KijiRowData]].
- *
- * The MapReduce framework views a data set as a collection of key-value pairs,
- * and likes to read those pairs into a reusable instance of the key or value class. When a row
- * is read from a Kiji table, its [[org.kiji.schema.EntityId]] is emitted as the key, and
- * [[org.kiji.schema.KijiRowData]] is emitted as the value. Because instances of
- * [[org.kiji.schema.KijiRowData]] are not reusable, this class is provided to give the MapReduce
- * framework a reusable container.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-final class KijiValue {
-  /** The row data being wrapped by this instance. */
-  private var currentValue: KijiRowData = null
-
-  /**
-   * Retrieves the [[org.kiji.schema.KijiRowData]] wrapped by this instance.
-   *
-   * @return the row data wrapped by this instance.
-   */
-  def get(): KijiRowData = currentValue
-
-  /**
-   * Sets the [[org.kiji.schema.KijiRowData]] wrapped by this instance.
-   *
-   * @param value that will be wrapped by this instance.
-   */
-  def set(value: KijiRowData) {
-    currentValue = value
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/LocalKijiScheme.scala b/src/main/scala/org/kiji/express/flow/framework/LocalKijiScheme.scala
deleted file mode 100644
index f3401ab5ee54a939fe8496fe985f3ea39a1f7c4c..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/LocalKijiScheme.scala
+++ /dev/null
@@ -1,344 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import java.io.InputStream
-import java.io.OutputStream
-import java.util.HashMap
-import java.util.{Map => JMap}
-import java.util.Properties
-
-import scala.collection.JavaConverters.mapAsJavaMapConverter
-import scala.collection.JavaConverters.asScalaIteratorConverter
-
-import cascading.flow.FlowProcess
-import cascading.flow.hadoop.util.HadoopUtil
-import cascading.scheme.Scheme
-import cascading.scheme.SinkCall
-import cascading.scheme.SourceCall
-import cascading.tap.Tap
-import cascading.tuple.Tuple
-import cascading.tuple.TupleEntry
-import com.google.common.base.Objects
-import org.apache.hadoop.conf.Configuration
-import org.apache.hadoop.hbase.HBaseConfiguration
-import org.apache.hadoop.mapred.JobConf
-import org.slf4j.Logger
-import org.slf4j.LoggerFactory
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.ColumnInputSpec
-import org.kiji.express.flow.ColumnOutputSpec
-import org.kiji.express.flow.TimeRange
-import org.kiji.express.flow.util.GenericCellSpecs
-import org.kiji.express.flow.util.Resources._
-import org.kiji.express.flow.util.SpecificCellSpecs
-import org.kiji.mapreduce.framework.KijiConfKeys
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiRowData
-import org.kiji.schema.KijiRowScanner
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiTableReader
-import org.kiji.schema.KijiTableWriter
-import org.kiji.schema.KijiURI
-import org.kiji.schema.layout.CellSpec
-import org.kiji.schema.layout.KijiTableLayout
-
-/**
- * Encapsulates the state required to read from a Kiji table locally, for use in
- * [[org.kiji.express.flow.framework.LocalKijiScheme]].
- *
- * @param reader that has an open connection to the desired Kiji table.
- * @param scanner that has an open connection to the desired Kiji table.
- * @param iterator that maintains the current row pointer.
- * @param tableUri of the kiji table.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] case class InputContext(
-    reader: KijiTableReader,
-    scanner: KijiRowScanner,
-    iterator: Iterator[KijiRowData],
-    tableUri: KijiURI,
-    configuration: Configuration)
-
-/**
- * Encapsulates the state required to write to a Kiji table.
- *
- * @param writer that has an open connection to the desired Kiji table.
- * @param tableUri of the Kiji table.
- * @param layout of the kiji table.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] case class OutputContext(
-    writer: KijiTableWriter,
-    tableUri: KijiURI,
-    kiji: Kiji,
-    layout: KijiTableLayout,
-    configuration: Configuration)
-
-/**
- * A local version of [[org.kiji.express.flow.framework.KijiScheme]] that is meant to be used with
- * Cascading's local job runner. [[org.kiji.express.flow.framework.KijiScheme]] and
- * LocalKijiScheme both define how to read and write the data stored in a Kiji table.
- *
- * This scheme is meant to be used with [[org.kiji.express.flow.framework.LocalKijiTap]] and
- * Cascading's local job runner. Jobs run with Cascading's local job runner execute on
- * your local machine instead of a cluster. This can be helpful for testing or quick jobs.
- *
- * In KijiExpress, LocalKijiScheme is used in tests.  See [[org.kiji.express.flow.KijiSource]]'s
- * `TestLocalKijiScheme` class.
- *
- * This scheme is responsible for converting rows from a Kiji table that are input to a
- * Cascading flow into Cascading tuples (see
- * `source(cascading.flow.FlowProcess, cascading.scheme.SourceCall)`) and writing output
- * data from a Cascading flow to a Kiji table
- * (see `sink(cascading.flow.FlowProcess, cascading.scheme.SinkCall)`).
- *
- * Note: LocalKijiScheme logs every row that was skipped because of missing data in a column. It
- * lacks the parameter `loggingInterval` in [[org.kiji.express.flow.framework.KijiScheme]] that
- * configures how many skipped rows will be logged.
- *
- * Note: Warnings about a missing serialVersionUID are ignored here. When KijiScheme is
- * serialized, the result is not persisted anywhere making serialVersionUID unnecessary.
- *
- * @see [[org.kiji.express.flow.framework.KijiScheme]]
- *
- * @param timeRange to include from the Kiji table.
- * @param timestampField is the optional name of a field containing the timestamp that all values
- *     in a tuple should be written to.
- *     Use None if all values should be written at the current time.
- * @param icolumns is a one-to-one mapping from field names to Kiji columns. The columns in the
- *     map will be read into their associated tuple fields.
- * @param ocolumns is a one-to-one mapping from field names to Kiji columns. Values from the
- *     tuple fields will be written to their associated column.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-private[express] class LocalKijiScheme(
-    private[express] val timeRange: TimeRange,
-    private[express] val timestampField: Option[Symbol],
-    private[express] val icolumns: Map[String, ColumnInputSpec] = Map(),
-    private[express] val ocolumns: Map[String, ColumnOutputSpec] = Map())
-    extends Scheme[Properties, InputStream, OutputStream, InputContext, OutputContext] {
-  private val logger: Logger = LoggerFactory.getLogger(classOf[LocalKijiScheme])
-
-  /** Set the fields that should be in a tuple when this source is used for reading and writing. */
-  setSourceFields(KijiScheme.buildSourceFields(icolumns.keys ++ ocolumns.keys))
-  setSinkFields(KijiScheme.buildSinkFields(ocolumns, timestampField))
-
-  /**
-   * Sets any configuration options that are required for running a local job
-   * that reads from a Kiji table.
-   *
-   * @param process flow being built.
-   * @param tap that is being used with this scheme.
-   * @param conf is an unused Properties object that is a stand-in for a job configuration object.
-   */
-  override def sourceConfInit(
-      process: FlowProcess[Properties],
-      tap: Tap[Properties, InputStream, OutputStream],
-      conf: Properties) {
-    // No-op. Setting options in a java Properties object is not going to help us read from
-    // a Kiji table.
-  }
-
-  /**
-   * Sets up any resources required to read from a Kiji table.
-   *
-   * @param process currently being run.
-   * @param sourceCall containing the context for this source.
-   */
-  override def sourcePrepare(
-      process: FlowProcess[Properties],
-      sourceCall: SourceCall[InputContext, InputStream]) {
-    val conf: JobConf = HadoopUtil.createJobConf(
-        process.getConfigCopy,
-        new JobConf(HBaseConfiguration.create()))
-    val uriString: String = conf.get(KijiConfKeys.KIJI_INPUT_TABLE_URI)
-    val kijiUri: KijiURI = KijiURI.newBuilder(uriString).build()
-
-    // Build the input context.
-    withKijiTable(kijiUri, conf) { table: KijiTable =>
-      val request = KijiScheme.buildRequest(timeRange, icolumns.values)
-      val reader = {
-        val allCellSpecs: JMap[KijiColumnName, CellSpec] = new HashMap[KijiColumnName, CellSpec]()
-        allCellSpecs.putAll(GenericCellSpecs(table))
-        allCellSpecs.putAll(SpecificCellSpecs.buildCellSpecs(table.getLayout, icolumns).asJava)
-        table.getReaderFactory.openTableReader(allCellSpecs)
-      }
-      val scanner = reader.getScanner(request)
-      val tableUri = table.getURI
-      val context = InputContext(reader, scanner, scanner.iterator.asScala, tableUri, conf)
-
-      sourceCall.setContext(context)
-    }
-  }
-
-  /**
-   * Reads and converts a row from a Kiji table to a Cascading Tuple. This method
-   * is called once for each row in the table.
-   *
-   * @param process is the current Cascading flow being run.
-   * @param sourceCall containing the context for this source.
-   * @return <code>true</code> if another row was read and it was converted to a tuple,
-   *     <code>false</code> if there were no more rows to read.
-   */
-  override def source(
-      process: FlowProcess[Properties],
-      sourceCall: SourceCall[InputContext, InputStream]): Boolean = {
-    val context: InputContext = sourceCall.getContext
-    if (context.iterator.hasNext) {
-      // Get the current row.
-      val row: KijiRowData = context.iterator.next()
-      val result: Tuple =
-          KijiScheme.rowToTuple(
-              icolumns,
-              getSourceFields,
-              timestampField,
-              row,
-              context.tableUri,
-              context.configuration)
-
-      // Set the result tuple and return from this method.
-      sourceCall.getIncomingEntry.setTuple(result)
-      process.increment(KijiScheme.counterGroupName, KijiScheme.counterSuccess, 1)
-      return true // We set a result tuple, return true for success.
-    } else {
-      return false // We reached the end of the input.
-    }
-  }
-
-  /**
-   * Cleans up any resources used to read from a Kiji table.
-   *
-   * @param process Current Cascading flow being run.
-   * @param sourceCall Object containing the context for this source.
-   */
-  override def sourceCleanup(
-      process: FlowProcess[Properties],
-      sourceCall: SourceCall[InputContext, InputStream]) {
-    val context: InputContext = sourceCall.getContext
-    context.reader.close()
-    context.scanner.close()
-
-    // Set the context to null so that we no longer hold any references to it.
-    sourceCall.setContext(null)
-  }
-
-  /**
-   * Sets any configuration options that are required for running a local job
-   * that writes to a Kiji table.
-   *
-   * @param process Current Cascading flow being built.
-   * @param tap The tap that is being used with this scheme.
-   * @param conf The job configuration object.
-   */
-  override def sinkConfInit(
-      process: FlowProcess[Properties],
-      tap: Tap[Properties, InputStream, OutputStream],
-      conf: Properties) {
-    // No-op. Setting options in a java Properties object is not going to help us write to
-    // a Kiji table.
-  }
-
-  /**
-   * Sets up any resources required to write to a Kiji table.
-   *
-   * @param process Current Cascading flow being run.
-   * @param sinkCall Object containing the context for this source.
-   */
-  override def sinkPrepare(
-      process: FlowProcess[Properties],
-      sinkCall: SinkCall[OutputContext, OutputStream]) {
-    // Open a table writer.
-    val conf: JobConf = HadoopUtil.createJobConf(process.getConfigCopy,
-        new JobConf(HBaseConfiguration.create()))
-    val uriString: String = conf.get(KijiConfKeys.KIJI_OUTPUT_TABLE_URI)
-    val uri: KijiURI = KijiURI.newBuilder(uriString).build()
-
-    val kiji = Kiji.Factory.open(uri)
-    doAndRelease(kiji.openTable(uri.getTable)) { table: KijiTable =>
-      // Set the sink context to an opened KijiTableWriter.
-      sinkCall
-        .setContext(OutputContext(table.openTableWriter(), uri, kiji, table.getLayout, conf))
-    }
-  }
-
-  /**
-   * Converts and writes a Cascading Tuple to a Kiji table.
-   *
-   * @param process Current Cascading flow being run.
-   * @param sinkCall Object containing the context for this source.
-   */
-  override def sink(
-      process: FlowProcess[Properties],
-      sinkCall: SinkCall[OutputContext, OutputStream]) {
-    // Retrieve writer from the scheme's context.
-    val OutputContext(writer, tableUri, kiji, layout, configuration) = sinkCall.getContext
-
-    // Write the tuple out.
-    val output: TupleEntry = sinkCall.getOutgoingEntry
-    KijiScheme.putTuple(
-        ocolumns,
-        tableUri,
-        kiji,
-        timestampField,
-        output,
-        writer,
-        layout,
-        configuration)
-  }
-
-  /**
-   * Cleans up any resources used to write to a Kiji table.
-   *
-   * @param process Current Cascading flow being run.
-   * @param sinkCall Object containing the context for this source.
-   */
-  override def sinkCleanup(
-      process: FlowProcess[Properties],
-      sinkCall: SinkCall[OutputContext, OutputStream]) {
-    sinkCall.getContext.writer.close()
-    sinkCall.getContext.kiji.release()
-    // Set the context to null so that we no longer hold any references to it.
-    sinkCall.setContext(null)
-  }
-
-  override def equals(other: Any): Boolean = {
-    other match {
-      case scheme: LocalKijiScheme => {
-        icolumns == scheme.icolumns &&
-        ocolumns == scheme.ocolumns &&
-            timestampField == scheme.timestampField &&
-            timeRange == scheme.timeRange
-      }
-      case _ => false
-    }
-  }
-
-  override def hashCode(): Int = Objects.hashCode(icolumns, ocolumns, timestampField, timeRange)
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/LocalKijiTap.scala b/src/main/scala/org/kiji/express/flow/framework/LocalKijiTap.scala
deleted file mode 100644
index 9019027f20a98b45bf9a912f1f29ab8c4b6906f3..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/LocalKijiTap.scala
+++ /dev/null
@@ -1,228 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import java.io.ByteArrayInputStream
-import java.io.ByteArrayOutputStream
-import java.io.InputStream
-import java.io.OutputStream
-import java.util.Properties
-import java.util.UUID
-
-import cascading.flow.FlowProcess
-import cascading.flow.hadoop.util.HadoopUtil
-import cascading.scheme.Scheme
-import cascading.tap.Tap
-import cascading.tuple.TupleEntryCollector
-import cascading.tuple.TupleEntryIterator
-import cascading.tuple.TupleEntrySchemeCollector
-import cascading.tuple.TupleEntrySchemeIterator
-import com.google.common.base.Objects
-import org.apache.hadoop.hbase.HBaseConfiguration
-import org.apache.hadoop.mapred.JobConf
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.express.flow.util.Resources.doAndRelease
-import org.kiji.mapreduce.framework.KijiConfKeys
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiURI
-
-/**
- * A Kiji-specific implementation of a Cascading `Tap`, which defines the location of a Kiji table.
- *
- * LocalKijiTap is responsible for configuring a local Cascading job with the settings necessary to
- * read from a Kiji table.
- *
- * LocalKijiTap must be used with [[org.kiji.express.flow.framework.LocalKijiScheme]] to perform
- * decoding of cells in a Kiji table. [[org.kiji.express.flow.KijiSource]] handles the creation
- * of both LocalKijiScheme and LocalKijiTap in KijiExpress.
- *
- * @param uri of the Kiji table to read or write from.
- * @param scheme that will convert data read from Kiji into Cascading's tuple model.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-private[express] class LocalKijiTap(
-    uri: KijiURI,
-    private val scheme: LocalKijiScheme)
-    extends Tap[Properties, InputStream, OutputStream](
-        scheme.asInstanceOf[Scheme[Properties, InputStream, OutputStream, _, _]]) {
-
-  /** The URI of the table to be read through this tap. */
-  private val tableUri: String = uri.toString()
-
-  /** A unique identifier for this tap instance. */
-  private val id: String = UUID.randomUUID().toString()
-
-  /**
-   * Sets any configuration options that are required for running a local job
-   * that reads from a Kiji table. This method gets called on the client machine
-   * during job setup.
-   *
-   * @param flow being built.
-   * @param conf to which we will add the table uri.
-   */
-  override def sourceConfInit(
-      flow: FlowProcess[Properties],
-      conf: Properties) {
-    // Store the input table.
-    conf.setProperty(KijiConfKeys.KIJI_INPUT_TABLE_URI, tableUri)
-
-    super.sourceConfInit(flow, conf)
-  }
-
-  /**
-   * Sets any configuration options that are required for running a local job
-   * that writes to a Kiji table. This method gets called on the client machine
-   * during job setup.
-   *
-   * @param flow being built.
-   * @param conf to which we will add the table uri.
-   */
-  override def sinkConfInit(
-      flow: FlowProcess[Properties],
-      conf: Properties) {
-    // Store the output table.
-    conf.setProperty(KijiConfKeys.KIJI_OUTPUT_TABLE_URI, tableUri)
-
-    super.sinkConfInit(flow, conf)
-  }
-
-  /**
-   * Provides a string representing the resource this `Tap` instance represents.
-   *
-   * @return a java UUID representing this KijiTap instance. Note: This does not return the uri of
-   *     the Kiji table being used by this tap to allow jobs that read from or write to the same
-   *     table to have different data request options.
-   */
-  override def getIdentifier(): String = id
-
-  /**
-   * Opens any resources required to read from a Kiji table.
-   *
-   * @param flow being run.
-   * @param input stream that will read from the desired Kiji table.
-   * @return an iterator that reads rows from the desired Kiji table.
-   */
-  override def openForRead(
-      flow: FlowProcess[Properties],
-      input: InputStream): TupleEntryIterator = {
-    return new TupleEntrySchemeIterator[Properties, InputStream](
-        flow,
-        getScheme(),
-        if (null == input) new ByteArrayInputStream(Array()) else input,
-        getIdentifier())
-  }
-
-  /**
-   * Opens any resources required to write from a Kiji table.
-   *
-   * @param flow being run.
-   * @param output stream that will write to the desired Kiji table. Note: This is ignored
-   *     currently since writing to a Kiji table is currently implemented without using an output
-   *     format by writing to the table directly from
-   *     [[org.kiji.express.flow.framework.KijiScheme]].
-   * @return a collector that writes tuples to the desired Kiji table.
-   */
-  override def openForWrite(
-      flow: FlowProcess[Properties],
-      output: OutputStream): TupleEntryCollector = {
-    return new TupleEntrySchemeCollector[Properties, OutputStream](
-        flow,
-        getScheme(),
-        if (null == output) new ByteArrayOutputStream() else output,
-        getIdentifier())
-  }
-
-  /**
-   * Builds any resources required to read from or write to a Kiji table.
-   *
-   * Note: KijiExpress currently does not support automatic creation of Kiji tables.
-   *
-   * @param conf containing settings for this flow.
-   * @return true if required resources were created successfully.
-   * @throws UnsupportedOperationException always.
-   */
-  override def createResource(conf: Properties): Boolean = {
-    throw new UnsupportedOperationException("KijiTap does not support creating tables for you.")
-  }
-
-  /**
-   * Deletes any unnecessary resources used to read from or write to a Kiji table.
-   *
-   * Note: KijiExpress currently does not support automatic deletion of Kiji tables.
-   *
-   * @param conf containing settings for this flow.
-   * @return true if superfluous resources were deleted successfully.
-   * @throws UnsupportedOperationException always.
-   */
-  override def deleteResource(conf: Properties): Boolean = {
-    throw new UnsupportedOperationException("KijiTap does not support deleting tables for you.")
-  }
-
-  /**
-   * Determines if the Kiji table this `Tap` instance points to exists.
-   *
-   * @param conf containing settings for this flow.
-   * @return true if the target Kiji table exists.
-   */
-  override def resourceExists(conf: Properties): Boolean = {
-    val uri: KijiURI = KijiURI.newBuilder(tableUri).build()
-    val jobConf: JobConf = HadoopUtil.createJobConf(conf,
-        new JobConf(HBaseConfiguration.create()))
-    doAndRelease(Kiji.Factory.open(uri, jobConf)) { kiji: Kiji =>
-      kiji.getTableNames().contains(uri.getTable())
-    }
-  }
-
-  /**
-   * Gets the time that the target Kiji table was last modified.
-   *
-   * Note: This will always return the current timestamp.
-   *
-   * @param conf containing settings for this flow.
-   * @return the current time.
-   */
-  override def getModifiedTime(conf: Properties): Long = System.currentTimeMillis()
-
-  override def equals(other: Any): Boolean = {
-    other match {
-      case tap: LocalKijiTap => ((tableUri == tap.tableUri)
-          && (scheme == tap.scheme)
-          && (id == tap.id))
-      case _ => false
-    }
-  }
-
-  override def hashCode(): Int = Objects.hashCode(tableUri, scheme, id)
-
-  /**
-   * Checks whether the instance, tables, and columns this tap uses can be accessed.
-   *
-   * @throws KijiExpressValidationException if the tables and columns are not accessible when this
-   *    is called.
-   */
-  private[express] def validate(conf: Properties): Unit = {
-    val kijiUri: KijiURI = KijiURI.newBuilder(tableUri).build()
-    KijiTap.validate(kijiUri, scheme.icolumns, scheme.ocolumns, HadoopUtil.createJobConf(conf,
-        new JobConf(HBaseConfiguration.create())))
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileFlowStepStrategy.scala b/src/main/scala/org/kiji/express/flow/framework/hfile/HFileFlowStepStrategy.scala
deleted file mode 100644
index 07ac7fd83196e97b88317cb879a1e802124deae7..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileFlowStepStrategy.scala
+++ /dev/null
@@ -1,122 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.hfile
-
-import java.net.URI
-import java.util.List
-
-import cascading.flow.Flow
-import cascading.flow.FlowStep
-import cascading.flow.FlowStepStrategy
-import cascading.tap.hadoop.Hfs
-import cascading.tap.hadoop.util.TempHfs
-import cascading.tuple.Tuple
-import cascading.util.Util
-import org.apache.hadoop.filecache.DistributedCache
-import org.apache.hadoop.fs.Path
-import org.apache.hadoop.io.NullWritable
-import org.apache.hadoop.mapred.FileOutputFormat
-import org.apache.hadoop.mapred.JobConf
-import org.apache.hadoop.mapred.SequenceFileOutputFormat
-import org.apache.hadoop.mapred.lib.IdentityReducer
-import org.apache.hadoop.mapred.lib.KeyFieldBasedComparator
-import org.apache.hadoop.mapred.lib.TotalOrderPartitioner
-import org.apache.hadoop.mapreduce.JobContext
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.mapreduce.framework.HFileKeyValue
-import org.kiji.mapreduce.framework.HFileKeyValue.FastComparator
-import org.kiji.mapreduce.output.HFileMapReduceJobOutput
-import org.kiji.mapreduce.output.framework.KijiHFileOutputFormat
-import org.kiji.schema.KijiURI
-import org.kiji.schema.mapreduce.KijiConfKeys
-
-/**
- * An implementation of a Cascading FlowStepStrategy used to alter the properties
- * of the flow step corresponding to the sink to support writing directly to HFiles. This
- * will only operate on FlowSteps where the sink's outputFormat is the KijiHFileOutputFormat.
- *
- * There are two situations that can happen when writing to HFiles:
- * <ol>
- *  <li> The Cascading sink step is a map-only flow (with no reducer). In this case, the Identity
- *  reducer will be forced to be used and the correct partitioner configured so that the
- *  tuples will be sinked to HFiles. </li>
- *  <li> The sink step is part of an flow with a reducer in which case the output will be routed
- *  to a temporary sequence file that a secondary M/R job will use to correct sort and store
- *  the data into HFiles for bulk loading </li>
- * </ol>
- *
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-@Inheritance.Sealed
-class HFileFlowStepStrategy extends FlowStepStrategy[JobConf] {
-
-  override def apply(flow: Flow[JobConf],
-                     predecessorSteps: List[FlowStep[JobConf]],
-                     flowStep: FlowStep[JobConf]) {
-    val conf = flowStep.getConfig()
-    val outputFormat = conf.get("elephantbird.class.for.DeprecatedOutputFormatWrapper")
-    val numReducers = conf.getNumReduceTasks()
-    val hfOutputFormat = classOf[KijiHFileOutputFormat].getName()
-
-    if (outputFormat != null && outputFormat == hfOutputFormat) {
-      // If this is part of a map-only job, then we configure a reducer with the correct
-      // partitioner.
-      if (numReducers == 0) {
-        conf.setPartitionerClass(classOf[TotalOrderPartitioner[HFileKeyValue, NullWritable]])
-        conf.setReducerClass(classOf[IdentityReducer[HFileKeyValue, NullWritable]])
-
-        conf.setMapOutputKeyClass(classOf[HFileKeyValue])
-        conf.setMapOutputValueClass(classOf[NullWritable])
-        conf.setOutputKeyComparatorClass(classOf[FastComparator])
-
-        val outputURI = conf.get(KijiConfKeys.OUTPUT_KIJI_TABLE_URI)
-        val kijiURI = KijiURI.newBuilder(outputURI).build()
-        val splits = HFileMapReduceJobOutput.makeTableKeySplit(kijiURI, 0, conf)
-        conf.setNumReduceTasks(splits.size())
-
-        // Write the file that the TotalOrderPartitioner reads to determine where to partition
-        // records.
-        var partitionFilePath =
-          new Path(conf.getWorkingDirectory(), TotalOrderPartitioner.DEFAULT_PATH)
-
-        val fs = partitionFilePath.getFileSystem(conf)
-        partitionFilePath = fs.makeQualified(partitionFilePath)
-        HFileMapReduceJobOutput.writePartitionFile(conf, partitionFilePath, splits)
-        val cacheUri =
-          new URI(partitionFilePath.toString() + "#" + TotalOrderPartitioner.DEFAULT_PATH)
-        DistributedCache.addCacheFile(cacheUri, conf)
-        DistributedCache.createSymlink(conf)
-      } else {
-        // We use the temporary path that was configured by the job to dump the HFileKVs
-        // for the second M/R job to process and then properly produce HFiles.
-        val newOutputPath = new Path(conf.get(HFileKijiOutput.TEMP_HFILE_OUTPUT_KEY))
-        conf.setOutputKeyClass(classOf[HFileKeyValue])
-        conf.setOutputValueClass(classOf[NullWritable])
-
-        conf.setOutputFormat(classOf[SequenceFileOutputFormat[HFileKeyValue, NullWritable]])
-        FileOutputFormat.setOutputPath(conf, newOutputPath)
-      }
-    }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiJob.scala b/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiJob.scala
deleted file mode 100644
index 2b39d243320c919fef02df5729fe4de8d03556f4..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiJob.scala
+++ /dev/null
@@ -1,168 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.hfile
-
-import scala.transient
-
-import cascading.flow.Flow
-import cascading.tap.hadoop.Hfs
-import cascading.util.Util
-import com.twitter.scalding.Args
-import com.twitter.scalding.HadoopTest
-import com.twitter.scalding.Hdfs
-import com.twitter.scalding.Job
-import com.twitter.scalding.Mode
-import com.twitter.scalding.WritableSequenceFile
-import org.apache.hadoop.conf.Configuration
-import org.apache.hadoop.fs.FileSystem
-import org.apache.hadoop.fs.Path
-import org.apache.hadoop.io.NullWritable
-import org.apache.hadoop.mapred.JobConf
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.KijiJob
-import org.kiji.mapreduce.framework.HFileKeyValue
-
-/**
- * HFileKijiJob is an extension of KijiJob and users should extend it instead of KijiJob when
- * writing jobs in KijiExpress that need to output HFiles that can eventually be bulk-loaded into
- * HBase.
- *
- * In your HFileKijiJob, you need to write to a source constructed with
- * [[org.kiji.express.flow.framework.hfile.HFileKijiOutput]].
- *
- * You can extend HFileKijiJob like this:
- *
- * {{{
- * class MyKijiExpressClass(args) extends HFileKijiJob(args) {
- *   // Your code here.
- *   .write(HFileKijiOutput(tableUri = "kiji://localhost:2181/default/mytable",
- *       hFileOutput = "my_hfiles",
- *       timestampField = 'timestamps,
- *       'column1 -> "info:column1",
- *       'column2 -> "info:column2"))
- * }
- * }}}
- *
- *     NOTE: To properly work with dumping to HFiles, the argument --hfile-output must be provided.
- *     This argument specifies the location where the HFiles will be written upon job completion.
- *     Also required is the --output flag. This argument specifies the Kiji table to use to obtain
- *     layout information to properly format the HFiles for bulk loading.
- *
- * @param args to the job. These get parsed in from the command line by Scalding.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Extensible
-class HFileKijiJob(args: Args) extends KijiJob(args) {
-  /** Name of the command-line argument that specifies the temporary HFile root directory. */
-  final val HFileOutputArgName: String = "hfile-output"
-
-  /** Name of the command-line argument that specifies the target output table. */
-  final val TableOutputArgName: String = "output"
-
-  // Force the check to ensure that a value has been provided for the hFileOutput
-  args(HFileOutputArgName)
-  args(TableOutputArgName)
-
-  @transient
-  lazy private val jobConf: Configuration = implicitly[Mode] match {
-    case Hdfs(_, configuration) => {
-      configuration
-    }
-    case HadoopTest(configuration, _) => {
-      configuration
-    }
-    case _ => new JobConf()
-  }
-
-  @transient
-  lazy val uniqTempFolder = makeTemporaryPathDirString("HFileDumper")
-
-  val tempPath = new Path(Hfs.getTempPath(jobConf.asInstanceOf[JobConf]), uniqTempFolder).toString
-
-  override def config(implicit mode: Mode): Map[AnyRef, AnyRef] = {
-    val baseConfig = super.config(mode)
-    baseConfig ++ Map(HFileKijiOutput.TEMP_HFILE_OUTPUT_KEY -> tempPath.toString())
-  }
-
-  override def buildFlow(implicit mode: Mode): Flow[_] = {
-    val flow = super.buildFlow
-    // Here we set the strategy to change the sink steps since we are dumping to HFiles.
-    flow.setFlowStepStrategy(new HFileFlowStepStrategy)
-    flow
-  }
-
-  override def next: Option[Job] = {
-    val fs = FileSystem.get(jobConf)
-    if(fs.exists(new Path(tempPath))) {
-      val newArgs = args + ("input" -> Some(tempPath))
-      val job = new HFileMapJob(newArgs)
-      Some(job)
-    } else {
-      None
-    }
-  }
-
-  // Borrowed from Hfs#makeTemporaryPathDirString
-  private def makeTemporaryPathDirString(name: String) = {
-    // _ is treated as a hidden file, so wipe them out
-    val name2 = name.replaceAll("^[_\\W\\s]+", "")
-
-    val name3 = if (name2.isEmpty()) {
-      "temp-path"
-    } else {
-      name2
-    }
-
-    name3.replaceAll("[\\W\\s]+", "_") + Util.createUniqueID()
-  }
-}
-
-/**
- * Private job implementation that executes the conversion of the intermediary HFile key-value
- * sequence files to the final HFiles. This is done only if the first job had a Cascading
- * configured reducer.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-private final class HFileMapJob(args: Args) extends HFileKijiJob(args) {
-
-  override def next: Option[Job] = {
-    val conf: Configuration = implicitly[Mode] match {
-      case Hdfs(_, configuration) => {
-        configuration
-      }
-      case HadoopTest(configuration, _) => {
-        configuration
-      }
-      case _ => new JobConf()
-    }
-    val fs = FileSystem.get(conf)
-    val input = args("input")
-    fs.delete(new Path(input), true)
-    None
-  }
-
-  WritableSequenceFile[HFileKeyValue, NullWritable](args("input"), ('keyValue, 'bogus))
-      .write(new HFileSource(args(TableOutputArgName),args(HFileOutputArgName)))
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiOutput.scala b/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiOutput.scala
deleted file mode 100644
index 14fe2cf2b2817e4be0e408ae7414b33a1c6c5135..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiOutput.scala
+++ /dev/null
@@ -1,174 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.hfile
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.All
-import org.kiji.express.flow.ColumnOutputSpec
-import org.kiji.express.flow.QualifiedColumnOutputSpec
-
-/**
- * Factory methods for constructing [[org.kiji.express.flow.framework.hfile.HFileKijiSource]]s that
- * will be used as outputs of a KijiExpress flow. Two basic APIs are provided with differing
- * complexity. These are similar to the factory methods in [[org.kiji.express.flow.KijiOutput]]
- * APIs except that an extra parameter for the HFile output location is required.
- *
- * Simple:
- * {{{
- *   // Create an HFileKijiOutput that writes to the table named `mytable` putting timestamps in the
- *   // `'timestamps` field and writing the fields `'column1` and `'column2` to the columns
- *   // `info:column1` and `info:column2`. The resulting HFiles will be written to the "my_hfiles"
- *   // folder.
- *   HFileKijiOutput(
- *       tableUri = "kiji://localhost:2181/default/mytable",
- *       hFileOutput = "my_hfiles",
- *       timestampField = 'timestamps,
- *       'column1 -> "info:column1",
- *       'column2 -> "info:column2")
- * }}}
- *
- * Verbose:
- * {{{
- *   // Create a KijiOutput that writes to the table named `mytable` putting timestamps in the
- *   // `'timestamps` field and writing the fields `'column1` and `'column2` to the columns
- *   // `info:column1` and `info:column2`. The resulting HFiles will be written to the "my_hfiles"
- *   // folder.
- *   HFileKijiOutput(
- *       tableUri = "kiji://localhost:2181/default/mytable",
- *       hFileOutput = "my_hfiles",
- *       timestampField = 'timestamps,
- *       columns = Map(
- *           // Enable paging for `info:column1`.
- *           'column1 -> QualifiedColumn("info", "column1").withPaging(cellsPerPage = 100),
- *           'column2 -> QualifiedColumn("info", "column2")))
- * }}}
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-object HFileKijiOutput {
-
-  val TEMP_HFILE_OUTPUT_KEY = "kiji.tempHFileOutput"
-
-  /**
-   * A factory method for instantiating [[org.kiji.express.flow.framework.hfile.HFileKijiSource]]s
-   * used as sinks. This method permits specifying the full range of read options for each column.
-   * Values written will be tagged with the current time at write.
-   *
-   * @param tableUri that addresses a table in a Kiji instance.
-   * @param hFileOutput is the location where the resulting HFiles will be placed.
-   * @param columns is a mapping specifying what column to write each field value to.
-   * @return a source that can write tuple field values to columns of a Kiji table.
-   */
-  def apply(
-      tableUri: String,
-      hFileOutput: String,
-      columns: Map[Symbol, _ <: ColumnOutputSpec]
-  ): HFileKijiSource = {
-    new HFileKijiSource(
-        tableAddress = tableUri,
-        hFileOutput,
-        timeRange = All,
-        timestampField = None,
-        loggingInterval = 1000,
-        columns = columns)
-  }
-
-  /**
-   * A factory method for instantiating [[org.kiji.express.flow.framework.hfile.HFileKijiSource]]s
-   * used as sinks. This method permits specifying the full range of read options for each column.
-   * Values written will be tagged with the current time at write.
-   *
-   * @param tableUri that addresses a table in a Kiji instance.
-   * @param hFileOutput is the location where the resulting HFiles will be placed.
-   * @param columns is a mapping specifying what column to write each field value to.
-   * @return a source that can write tuple field values to columns of a Kiji table.
-   */
-  def apply(
-      tableUri: String,
-      hFileOutput: String,
-      timestampField: Symbol,
-      columns: (Symbol, String)*
-  ): HFileKijiSource = {
-
-    val columnMap = columns
-        .toMap
-        .mapValues(QualifiedColumnOutputSpec(_))
-    new HFileKijiSource(
-        tableAddress = tableUri,
-        hFileOutput = hFileOutput,
-        timeRange = All,
-        Some(timestampField),
-        loggingInterval = 1000,
-        columns = columnMap)
-  }
-
-  /**
-   * A factory method for instantiating [[org.kiji.express.flow.framework.hfile.HFileKijiSource]]s
-   * used as sinks. This method permits specifying the full range of read options for each column.
-   *
-   * @param tableUri that addresses a table in a Kiji instance.
-   * @param hFileOutput is the location where the resulting HFiles will be placed.
-   * @param columns is a mapping specifying what column to write each field value to.
-   * @param timestampField is the name of a tuple field that will contain cell timestamps when the
-   *     source is used for writing.
-   * @return a source that can write tuple field values to columns of a Kiji table.
-   */
-  def apply(
-      tableUri: String,
-      hFileOutput: String,
-      timestampField: Symbol,
-      columns: Map[Symbol, _ <: ColumnOutputSpec]
-  ): HFileKijiSource = {
-    require(timestampField != null)
-
-    new HFileKijiSource(
-        tableAddress = tableUri,
-        hFileOutput,
-        timeRange = All,
-        timestampField = Some(timestampField),
-        loggingInterval = 1000,
-        columns = columns)
-  }
-
-  /**
-   * A factory method for instantiating [[org.kiji.express.flow.framework.hfile.HFileKijiSource]]s
-   * used as sinks. Values written will be tagged with the current time at write.
-   *
-   * @param tableUri that addresses a table in a Kiji instance.
-   * @param hFileOutput is the location where the resulting HFiles will be placed.
-   * @param columns are a series of pairs mapping tuple field names to Kiji column names. When
-   *     naming columns, use the format `"family:qualifier"`.
-   * @return a source that can write tuple field values to columns of a Kiji table.
-   */
-  def apply(
-      tableUri: String,
-      hFileOutput: String,
-      columns: (Symbol, String)*
-  ): HFileKijiSource = {
-    val columnMap = columns
-        .toMap
-        .mapValues(QualifiedColumnOutputSpec(_))
-
-    HFileKijiOutput(tableUri, hFileOutput, columnMap)
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiScheme.scala b/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiScheme.scala
deleted file mode 100644
index f1b8f42cb2f4a77efd741edc93957cbb018916a4..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiScheme.scala
+++ /dev/null
@@ -1,296 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.hfile
-
-import cascading.flow.FlowProcess
-import cascading.scheme.NullScheme
-import cascading.scheme.SinkCall
-import cascading.tap.Tap
-import cascading.tuple.TupleEntry
-import com.google.common.base.Objects
-import org.apache.hadoop.hbase.HConstants
-import org.apache.hadoop.io.NullWritable
-import org.apache.hadoop.mapred.JobConf
-import org.apache.hadoop.mapred.OutputCollector
-import org.apache.hadoop.mapred.RecordReader
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.ColumnFamilyOutputSpec
-import org.kiji.express.flow.ColumnOutputSpec
-import org.kiji.express.flow.EntityId
-import org.kiji.express.flow.QualifiedColumnOutputSpec
-import org.kiji.express.flow.TimeRange
-import org.kiji.express.flow.framework.KijiScheme
-import org.kiji.express.flow.framework.KijiSourceContext
-import org.kiji.express.flow.util.Resources.doAndRelease
-import org.kiji.mapreduce.framework.HFileKeyValue
-import org.kiji.mapreduce.framework.KijiConfKeys
-import org.kiji.schema.EntityIdFactory
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiURI
-import org.kiji.schema.impl.DefaultKijiCellEncoderFactory
-import org.kiji.schema.layout.KijiTableLayout
-import org.kiji.schema.layout.impl.CellEncoderProvider
-import org.kiji.schema.layout.impl.ColumnNameTranslator
-
- /**
- * A Kiji-specific implementation of a Cascading `Scheme` which defines how to write data
- * to HFiles.
- *
- * HFileKijiScheme is responsible for converting rows from a Kiji table that are input to a
- * Cascading flow into Cascading tuples
- * (see `source(cascading.flow.FlowProcess, cascading.scheme.SourceCall)`) and writing output
- * data from a Cascading flow to an HFile capable of being bulk loaded into HBase
- * (see `sink(cascading.flow.FlowProcess, cascading.scheme.SinkCall)`).
- *
- * HFileKijiScheme must be used with [[org.kiji.express.flow.framework.hfile.HFileKijiTap]],
- * since it expects the Tap to have access to a Kiji table.
- * [[org.kiji.express.flow.framework.hfile.HFileKijiSource]] handles the creation of both
- * HFileKijiScheme and KijiTap in KijiExpress.
- *
- * @param timeRange to include from the Kiji table.
- * @param timestampField is the optional name of a field containing the timestamp that all values
- *     in a tuple should be written to.
- *     Use None if all values should be written at the current time.
- * @param loggingInterval to log skipped rows on. For example, if loggingInterval is 1000,
- *     then every 1000th skipped row will be logged.
- * @param columns mapping tuple field names to requests for Kiji columns.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-private[express] class HFileKijiScheme(
-  private[express] val timeRange: TimeRange,
-  private[express] val timestampField: Option[Symbol],
-  private[express] val loggingInterval: Long,
-  private[express] val columns: Map[String, ColumnOutputSpec])
-    extends HFileKijiScheme.HFileScheme {
-
-  import KijiScheme._
-  import HFileKijiScheme._
-
-  setSinkFields(buildSinkFields(columns, timestampField))
-
-  /**
-   * Sets up any resources required for the MapReduce job. This method is called
-   * on the cluster.
-   *
-   * @param flow is the current Cascading flow being run.
-   * @param sinkCall containing the context for this source.
-   */
-  override def sinkPrepare(
-    flow: FlowProcess[JobConf],
-    sinkCall: SinkCall[HFileKijiSinkContext, OutputCollector[HFileKeyValue, NullWritable]]) {
-    val conf = flow.getConfigCopy()
-    val uri = conf.get(KijiConfKeys.KIJI_OUTPUT_TABLE_URI)
-    val kijiURI = KijiURI.newBuilder(uri).build()
-    val kiji = Kiji.Factory.open(kijiURI)
-
-    doAndRelease(kiji.openTable(kijiURI.getTable)) { table: KijiTable =>
-      // Set the sink context to an opened KijiTableWriter.
-      val ctx = HFileKijiSinkContext(kiji, kijiURI,
-        table.getLayout, new ColumnNameTranslator(table.getLayout))
-      sinkCall.setContext(ctx)
-    }
-  }
-
-  /**
-   * Converts and writes a Cascading Tuple to a Kiji table. This method is called once
-   * for each row on the cluster.
-   *
-   * @param flow is the current Cascading flow being run.
-   * @param sinkCall containing the context for this source.
-   */
-  override def sink(
-    flow: FlowProcess[JobConf],
-    sinkCall: SinkCall[HFileKijiSinkContext, OutputCollector[HFileKeyValue, NullWritable]]) {
-
-    // Write the tuple out.
-    val output: TupleEntry = sinkCall.getOutgoingEntry
-
-    val HFileKijiSinkContext(kiji, uri, layout, colTranslator) = sinkCall.getContext()
-    val eidFactory = EntityIdFactory.getFactory(layout)
-
-    val encoderProvider = new CellEncoderProvider(uri, layout, kiji.getSchemaTable(),
-        DefaultKijiCellEncoderFactory.get())
-
-    outputCells(output, timestampField, columns) { key: HFileCell =>
-      // Convert cell to an HFileKeyValue
-      val kijiColumn = new KijiColumnName(key.colRequest.family, key.colRequest.qualifier)
-      val hbaseColumn = colTranslator.toHBaseColumnName(kijiColumn)
-      val cellSpec = layout.getCellSpec(kijiColumn)
-        .setSchemaTable(kiji.getSchemaTable())
-
-      val encoder = encoderProvider.getEncoder(kijiColumn.getFamily(), kijiColumn.getQualifier())
-      val hFileKeyValue = new HFileKeyValue(
-        key.entityId.toJavaEntityId(eidFactory).getHBaseRowKey(),
-        hbaseColumn.getFamily(), hbaseColumn.getQualifier(), key.timestamp,
-        encoder.encode(key.datum))
-
-      sinkCall.getOutput().collect(hFileKeyValue, NullWritable.get())
-    }
-  }
-
-  /**
-   * Cleans up any resources used during the MapReduce job. This method is called
-   * on the cluster.
-   *
-   * @param flow is the current Cascading flow being run.
-   * @param sinkCall containing the context for this source.
-   */
-  override def sinkCleanup(
-    flow: FlowProcess[JobConf],
-    sinkCall: SinkCall[HFileKijiSinkContext, OutputCollector[HFileKeyValue, NullWritable]]) {
-
-    val HFileKijiSinkContext(kiji, _, _, _) = sinkCall.getContext()
-
-    kiji.release()
-    sinkCall.setContext(null)
-
-  }
-
-  /**
-   * Sets any configuration options that are required for running a MapReduce job
-   * that writes to a Kiji table. This method gets called on the client machine
-   * during job setup.
-   *
-   * @param flow being built.
-   * @param tap that is being used with this scheme.
-   * @param conf to which we will add our KijiDataRequest.
-   */
-  override def sinkConfInit(
-    flow: FlowProcess[JobConf],
-    tap: Tap[JobConf, RecordReader[_, _], OutputCollector[HFileKeyValue, NullWritable]],
-    conf: JobConf) {
-  }
-
-
-  override def equals(other: Any): Boolean = {
-    other match {
-      case scheme: HFileKijiScheme => {
-        columns == scheme.columns &&
-          timestampField == scheme.timestampField &&
-          timeRange == scheme.timeRange
-      }
-      case _ => false
-    }
-  }
-
-
-  override def hashCode(): Int =
-    Objects.hashCode(columns, timeRange, timestampField, loggingInterval: java.lang.Long)
-}
-
-/**
- * Private scheme that is a subclass of Cascading's NullScheme that doesn't do anything but
- * sinks data. This is used in the secondary M/R job that takes intermediate HFile Key/Values
- * from a sequence files and outputs them to the KijiHFileOutputFormat ultimately going to HFiles.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-private[express] final class SemiNullScheme extends HFileKijiScheme.HFileScheme {
-  /**
-   * Converts and writes a Cascading Tuple to a Kiji table. This method is called once
-   * for each row on the cluster.
-   *
-   * @param flow is the current Cascading flow being run.
-   * @param sinkCall containing the context for this source.
-   */
-  override def sink(
-    flow: FlowProcess[JobConf],
-    sinkCall: SinkCall[HFileKijiSinkContext, OutputCollector[HFileKeyValue, NullWritable]]) {
-
-    // Write the tuple out.
-    val output: TupleEntry = sinkCall.getOutgoingEntry
-
-    val hFileKeyValue = output.getObject(0).asInstanceOf[HFileKeyValue]
-    sinkCall.getOutput().collect(hFileKeyValue, NullWritable.get())
-  }
-}
-
-/**
- * Context housing information necessary for the scheme to interact
- * with the Kiji table.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] case class HFileKijiSinkContext (
-  kiji: Kiji,
-  kijiUri: KijiURI,
-  layout: KijiTableLayout,
-  columnTranslator: ColumnNameTranslator
-)
-
-/**
- * A cell from a Kiji table containing some datum, addressed by a family, qualifier,
- * and version timestamp.
- *
- * @param entityId of the Kiji table cell.
- * @param colRequest identifying the location of the Kiji table cell.
- * @param timestamp  of the Kiji table cell.
- * @param datum in the Kiji table cell.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] case class HFileCell private[express] (
-  entityId: EntityId,
-  colRequest: QualifiedColumnOutputSpec,
-  timestamp: Long,
-  datum: AnyRef)
-
-object HFileKijiScheme {
-  type HFileScheme = NullScheme[JobConf, RecordReader[_, _],
-    OutputCollector[HFileKeyValue, NullWritable], KijiSourceContext, HFileKijiSinkContext]
-
-  private[express] def outputCells(output: TupleEntry,
-                                   timestampField: Option[Symbol],
-                                   columns: Map[String, ColumnOutputSpec])(
-                                     cellHandler: HFileCell => Unit) {
-
-    // Get a timestamp to write the values to, if it was specified by the user.
-    val timestamp: Long = timestampField match {
-      case Some(field) => output.getObject(field.name).asInstanceOf[Long]
-      case None        => HConstants.LATEST_TIMESTAMP
-    }
-
-    // Get the entityId.
-    val entityId: EntityId =
-      output.getObject(KijiScheme.entityIdField).asInstanceOf[EntityId]
-
-    columns.foreach(kv => {
-      val (fieldName, colRequest) = kv
-      val colValue = output.getObject(fieldName)
-      val newColRequest = colRequest match {
-        case cf @ ColumnFamilyOutputSpec(family, qualField, schemaId) => {
-          val qualifier = output.getString(qualField.name)
-          QualifiedColumnOutputSpec(family, qualifier)
-        }
-        case qc @ QualifiedColumnOutputSpec(_, _, _) => qc
-      }
-      val cell = HFileCell(entityId, newColRequest, timestamp, colValue)
-      cellHandler(cell)
-    })
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiSource.scala b/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiSource.scala
deleted file mode 100644
index 26932a5d445360bd44cb52558c696fc79a89d362..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiSource.scala
+++ /dev/null
@@ -1,163 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.hfile
-
-import java.lang.UnsupportedOperationException
-
-import cascading.tap.Tap
-import com.google.common.base.Objects
-import com.twitter.scalding.AccessMode
-import com.twitter.scalding.HadoopTest
-import com.twitter.scalding.Hdfs
-import com.twitter.scalding.Mode
-import com.twitter.scalding.Source
-import com.twitter.scalding.Write
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.express.flow.All
-import org.kiji.express.flow.ColumnOutputSpec
-import org.kiji.express.flow.TimeRange
-import org.kiji.express.flow.framework.KijiScheme
-
-/**
- * A read or write view of a Kiji table.
- *
- * A Scalding `Source` provides a view of a data source that can be read as Scalding tuples. It
- * is comprised of a Cascading tap [[cascading.tap.Tap]], which describes where the data is and how
- * to access it, and a Cascading Scheme [[cascading.scheme.Scheme]], which describes how to read
- * and interpret the data.
- *
- * An `HFileKijiSource` should never be used for reading.  It is intended to be used only in
- * [[org.kiji.express.flow.framework.hfile.HFileKijiJob]], for writing out to HFiles formatted for
- * bulk-loading into Kiji.
- *
- * When writing to a Kiji table, a `HFileKijiSource` views a Kiji table as a collection of tuples
- * that correspond to cells from the Kiji table. Each tuple to be written must provide a cell
- * address by specifying a Kiji `EntityID` in the tuple field `entityId`, a value to be written in a
- * configurable field, and (optionally) a timestamp in a configurable field.
- *
- * End-users cannot directly obtain instances of `KijiSource`. Instead,
- * they should use the factory methods provided as part of the
- * [[org.kiji.express.flow.framework.hfile]] module.
- *
- * @param tableAddress is a Kiji URI addressing the Kiji table to read or write to.
- * @param timeRange that cells read must belong to. Ignored when the source is used to write.
- * @param timestampField is the name of a tuple field that will contain cell timestamp when the
- *     source is used for writing. Specify `None` to write all
- *     cells at the current time.
- * @param loggingInterval The interval at which to log skipped rows.
- * @param columns is a one-to-one mapping from field names to Kiji columns. When reading,
- *     the columns in the map will be read into their associated tuple fields. When
- *     writing, values from the tuple fields will be written to their associated column.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-class HFileKijiSource private[express] (
-    val tableAddress: String,
-    val hFileOutput: String,
-    val timeRange: TimeRange,
-    val timestampField: Option[Symbol],
-    val loggingInterval: Long,
-    val columns: Map[Symbol, ColumnOutputSpec]
-) extends Source {
-  import org.kiji.express.flow.KijiSource._
-
-  /**
-   * Creates a Scheme that writes to/reads from a Kiji table for usage with
-   * the hadoop runner.
-   */
-  override val hdfsScheme: KijiScheme.HadoopScheme =
-    new HFileKijiScheme(timeRange, timestampField, loggingInterval, convertKeysToStrings(columns))
-      // This cast is required due to Scheme being defined with invariant type parameters.
-      .asInstanceOf[KijiScheme.HadoopScheme]
-
-  /**
-   * Create a connection to the physical data source (also known as a Tap in Cascading)
-   * which, in this case, is a [[org.kiji.schema.KijiTable]].
-   *
-   * @param readOrWrite Specifies if this source is to be used for reading or writing.
-   * @param mode Specifies which job runner/flow planner is being used.
-   * @return A tap to use for this data source.
-   */
-  override def createTap(readOrWrite: AccessMode)(implicit mode: Mode): Tap[_, _, _] = {
-    val tap: Tap[_, _, _] = mode match {
-      // Production taps.
-      case Hdfs(_, _) => new HFileKijiTap(tableAddress, hdfsScheme, hFileOutput)
-
-      // Test taps.
-      case HadoopTest(conf, buffers) => {
-        readOrWrite match {
-          case Write => {
-            new HFileKijiTap(tableAddress, hdfsScheme, hFileOutput)
-          }
-          case _ => throw new UnsupportedOperationException("Read unsupported")
-        }
-      }
-      // Delegate any other tap types to Source's default behaviour.
-      case _ => super.createTap(readOrWrite)(mode)
-    }
-
-    return tap
-  }
-
-  override def toString: String = {
-    Objects
-        .toStringHelper(this)
-        .add("tableAddress", tableAddress)
-        .add("timeRange", timeRange)
-        .add("timestampField", timestampField)
-        .add("loggingInterval", loggingInterval)
-        .add("columns", columns)
-        .toString
-  }
-
-  override def equals(other: Any): Boolean = {
-    other match {
-      case source: HFileKijiSource => {
-        Objects.equal(tableAddress, source.tableAddress) &&
-        Objects.equal(hFileOutput, source.hFileOutput) &&
-        Objects.equal(columns, source.columns) &&
-        Objects.equal(timestampField, source.timestampField) &&
-        Objects.equal(timeRange, source.timeRange)
-      }
-      case _ => false
-    }
-  }
-
-  override def hashCode(): Int =
-      Objects.hashCode(tableAddress, hFileOutput, columns, timestampField, timeRange)
-}
-
-/**
- * Private Scalding source implementation whose scheme is the SemiNullScheme that
- * simply sinks tuples to an output for later writing to HFiles.
- */
-private final class HFileSource(
-    override val tableAddress: String,
-    override val hFileOutput: String
-) extends HFileKijiSource(tableAddress, hFileOutput, All, None, 0, Map()) {
-  /**
-   * Creates a Scheme that writes to/reads from a Kiji table for usage with
-   * the hadoop runner.
-   */
-  override val hdfsScheme: KijiScheme.HadoopScheme =
-      new SemiNullScheme().asInstanceOf[KijiScheme.HadoopScheme]
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiTap.scala b/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiTap.scala
deleted file mode 100644
index 541a0ff9d699931432e8ffbb351eb15b93a302fd..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/hfile/HFileKijiTap.scala
+++ /dev/null
@@ -1,179 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.hfile
-
-import java.util.UUID
-
-import cascading.flow.FlowProcess
-import cascading.scheme.Scheme
-import cascading.tap.Tap
-import cascading.tap.hadoop.io.HadoopTupleEntrySchemeCollector
-import cascading.tuple.TupleEntryCollector
-import cascading.tuple.TupleEntryIterator
-import com.twitter.elephantbird.mapred.output.DeprecatedOutputFormatWrapper
-import org.apache.hadoop.fs.Path
-import org.apache.hadoop.mapred.FileOutputFormat
-import org.apache.hadoop.mapred.JobConf
-import org.apache.hadoop.mapred.OutputCollector
-import org.apache.hadoop.mapred.RecordReader
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.express.flow.framework.KijiKey
-import org.kiji.express.flow.framework.KijiScheme
-import org.kiji.express.flow.framework.KijiValue
-import org.kiji.mapreduce.framework.KijiConfKeys
-import org.kiji.mapreduce.impl.HFileWriterContext
-import org.kiji.mapreduce.output.framework.KijiHFileOutputFormat
-
-/**
- * A Kiji-specific implementation of a Cascading `Tap`, which defines how data is to be read from
- * and written to a particular endpoint. This implementation only handles writing to Kiji
- * formatted HFiles to be bulk loaded into HBase.
- *
- * HFileKijiTap must be used with [[org.kiji.express.flow.framework.hfile.HFileKijiScheme]]
- * to perform decoding of cells in a Kiji table. [[org.kiji.express.flow.KijiSource]] handles
- * the creation of both HFileKijiScheme and HFileKijiTap in KijiExpress.
- *
- * @param tableUri of the Kiji table to read or write from.
- * @param scheme that will convert data read from Kiji into Cascading's tuple model.
- * @param hFileOutput is the location where the HFiles will be written to.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-private[express] class HFileKijiTap(
-  private val tableUri: String,
-  private val scheme: KijiScheme.HadoopScheme,
-  private val hFileOutput: String)
-    extends Tap[JobConf, RecordReader[KijiKey, KijiValue], OutputCollector[_, _]](
-        scheme.asInstanceOf[Scheme[JobConf, RecordReader[KijiKey, KijiValue],
-            OutputCollector[_, _], _, _]]) {
-
-  /** Unique identifier for this KijiTap instance. */
-  private val id: String = UUID.randomUUID().toString()
-
-  /**
-   * Provides a string representing the resource this `Tap` instance represents.
-   *
-   * @return a java UUID representing this KijiTap instance. Note: This does not return the uri of
-   *     the Kiji table being used by this tap to allow jobs that read from or write to the same
-   *     table to have different data request options.
-   */
-  override def getIdentifier(): String = id
-
-
-  /**
-   * Opens any resources required to read from a Kiji table.
-   *
-   * @param flow being run.
-   * @param recordReader that will read from the desired Kiji table.
-   * @return an iterator that reads rows from the desired Kiji table.
-   */
-  override def openForRead(
-      flow: FlowProcess[JobConf],
-      recordReader: RecordReader[KijiKey, KijiValue]): TupleEntryIterator = {
-    null
-  }
-
-  /**
-   * Opens any resources required to write from a Kiji table.
-   *
-   * @param flow being run.
-   * @param outputCollector that will write to the desired Kiji table.
-   *
-   * @return a collector that writes tuples to the desired Kiji table.
-   */
-  override def openForWrite(
-      flow: FlowProcess[JobConf],
-      outputCollector: OutputCollector[_, _]): TupleEntryCollector = {
-
-    new HadoopTupleEntrySchemeCollector(
-        flow,
-        this.asInstanceOf[Tap[JobConf, RecordReader[_, _], OutputCollector[_, _]]],
-        outputCollector)
-  }
-  /**
-   * Builds any resources required to read from or write to a Kiji table.
-   *
-   * Note: KijiExpress currently does not support automatic creation of Kiji tables.
-   *
-   * @param conf containing settings for this flow.
-   * @return true if required resources were created successfully.
-   * @throws UnsupportedOperationException always.
-   */
-  override def createResource(conf: JobConf): Boolean = {
-    throw new UnsupportedOperationException("KijiTap does not support creating tables for you.")
-  }
-
-  /**
-   * Deletes any unnecessary resources used to read from or write to a Kiji table.
-   *
-   * Note: KijiExpress currently does not support automatic deletion of Kiji tables.
-   *
-   * @param conf containing settings for this flow.
-   * @return true if superfluous resources were deleted successfully.
-   * @throws UnsupportedOperationException always.
-   */
-  override def deleteResource(conf: JobConf): Boolean = {
-    throw new UnsupportedOperationException("KijiTap does not support deleting tables for you.")
-  }
-
-  /**
-   * Gets the time that the target Kiji table was last modified.
-   *
-   * Note: This will always return the current timestamp.
-   *
-   * @param conf containing settings for this flow.
-   * @return the current time.
-   */
-  override def getModifiedTime(conf: JobConf): Long = System.currentTimeMillis()
-
-  /**
-   * Determines if the Kiji table this `Tap` instance points to exists.
-   *
-   * @param conf containing settings for this flow.
-   * @return true if the target Kiji table exists.
-   */
-  override def resourceExists(conf: JobConf): Boolean = {
-    true
-  }
-
-  /**
-   * Sets any configuration options that are required for running a MapReduce job
-   * that writes to a Kiji table. This method gets called on the client machine
-   * during job setup.
-   *
-   * @param flow being built.
-   * @param conf to which we will add the table uri.
-   */
-  override def sinkConfInit(flow: FlowProcess[JobConf], conf: JobConf) {
-
-    FileOutputFormat.setOutputPath(conf, new Path(hFileOutput))
-    DeprecatedOutputFormatWrapper.setOutputFormat(classOf[KijiHFileOutputFormat], conf)
-    val hfContext = classOf[HFileWriterContext].getName()
-    conf.set(KijiConfKeys.KIJI_TABLE_CONTEXT_CLASS, hfContext)
-    // Store the output table.
-    conf.set(KijiConfKeys.KIJI_OUTPUT_TABLE_URI, tableUri)
-
-    super.sinkConfInit(flow, conf)
-  }
-}
-
-
diff --git a/src/main/scala/org/kiji/express/flow/framework/hfile/package.scala b/src/main/scala/org/kiji/express/flow/framework/hfile/package.scala
deleted file mode 100644
index 00f6b37459919c687d3604ecc5095755267a4fd3..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/hfile/package.scala
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-/**
- * Provides the necessary implementation to allow Scalding jobs to write directly to HFiles. To
- * support this, simply subclass the HFileKijiJob. The only requirement is that you provide
- * a command line argument hFileOutput which references the location on HDFS where the final
- * HFiles are to be stored.
- *
- * Under the covers, Scalding will be configured to write to HFiles either by manipulating the
- * current Cascading flow if the defined flow has no reducers OR by redirecting output to a
- * temporary location where a secondary job will re-order the results into something that
- * HBase can load. The reason for this is because KeyValues must be sorted according to a
- * TotalOrderPartitioner that relies on the region splits of HBase so that KeyValues are placed
- * in the right region.
- */
-package object hfile
diff --git a/src/main/scala/org/kiji/express/flow/framework/package.scala b/src/main/scala/org/kiji/express/flow/framework/package.scala
deleted file mode 100644
index 9c490cb3582d360d835f4f5efdb38eb8642c78ff..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/package.scala
+++ /dev/null
@@ -1,27 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-/**
- * Package containing supporting classes and objects for the KijiExpress flow API. Includes
- * components required to integrate with Hadoop, Cascading, and Scalding. The classes in this
- * package should only be used for developing other Kiji platform projects.
- */
-package object framework
diff --git a/src/main/scala/org/kiji/express/flow/framework/serialization/AvroSerializer.scala b/src/main/scala/org/kiji/express/flow/framework/serialization/AvroSerializer.scala
deleted file mode 100644
index 6495eaa5a6197ee3eee63f23e24b967521488c26..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/serialization/AvroSerializer.scala
+++ /dev/null
@@ -1,153 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.serialization
-
-import com.esotericsoftware.kryo.Kryo
-import com.esotericsoftware.kryo.Serializer
-import com.esotericsoftware.kryo.io.Input
-import com.esotericsoftware.kryo.io.Output
-import org.apache.avro.Schema
-import org.apache.avro.generic.GenericContainer
-import org.apache.avro.generic.GenericDatumReader
-import org.apache.avro.generic.GenericDatumWriter
-import org.apache.avro.io.DecoderFactory
-import org.apache.avro.io.EncoderFactory
-import org.apache.avro.specific.SpecificDatumReader
-import org.apache.avro.specific.SpecificDatumWriter
-import org.apache.avro.specific.SpecificRecord
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-
-/**
- * Provides serialization for Avro schemas while using Kryo serialization.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-// TODO (EXP-295): Should these maybe be Framework?
-class AvroSchemaSerializer
-    extends Serializer[Schema] {
-  setAcceptsNull(false)
-
-  override def write(
-      kryo: Kryo,
-      output: Output,
-      schema: Schema
-  ) {
-    val encodedSchema = schema.toString(false)
-    output.writeString(encodedSchema)
-  }
-
-  override def read(
-      kryo: Kryo,
-      input: Input,
-      klazz: Class[Schema]
-  ): Schema = {
-    val encodedSchema = input.readString()
-    new Schema.Parser().parse(encodedSchema)
-  }
-}
-
-/**
- * Provides serialization for Avro generic records while using Kryo serialization. Record schemas
- * are prepended to the encoded generic record data.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-// TODO (EXP-295): Should these maybe be Framework?
-class AvroGenericSerializer
-    extends Serializer[GenericContainer] {
-  // TODO(EXP-269): Cache encoders per schema.
-
-  // We at least need an avro schema to perform serialization.
-  setAcceptsNull(false)
-
-  override def write(
-      kryo: Kryo,
-      output: Output,
-      avroObject: GenericContainer
-  ) {
-    // Serialize the schema.
-    new AvroSchemaSerializer().write(kryo, output, avroObject.getSchema)
-
-    // Serialize the data.
-    val writer = new GenericDatumWriter[GenericContainer](avroObject.getSchema)
-    val encoder = EncoderFactory
-        .get()
-        .directBinaryEncoder(output, null)
-    writer.write(avroObject, encoder)
-  }
-
-  override def read(
-      kryo: Kryo,
-      input: Input,
-      klazz: Class[GenericContainer]
-  ): GenericContainer = {
-    // Deserialize the schema.
-    val schema = new AvroSchemaSerializer().read(kryo, input, null)
-
-    // Deserialize the data.
-    val reader = new GenericDatumReader[GenericContainer](schema)
-    val decoder = DecoderFactory
-        .get()
-        .directBinaryDecoder(input, null)
-    reader.read(null.asInstanceOf[GenericContainer], decoder)
-  }
-}
-
-/**
- * Provides serialization for Avro specific records while using Kryo serialization. Record schemas
- * are not serialized as all clients interacting with this data are assumed to have the correct
- * specific record class on their classpath.
- */
-final class AvroSpecificSerializer
-    extends Serializer[SpecificRecord] {
-  // TODO(EXP-269) Cache encoders per class/schema.
-
-  setAcceptsNull(false)
-
-  override def write(
-      kryo: Kryo,
-      output: Output,
-      record: SpecificRecord
-  ) {
-    val writer =
-        new SpecificDatumWriter[SpecificRecord](record.getClass.asInstanceOf[Class[SpecificRecord]])
-    val encoder = EncoderFactory
-        .get()
-        .directBinaryEncoder(output, null)
-    writer.write(record, encoder)
-  }
-
-  override def read(
-      kryo: Kryo,
-      input: Input,
-      klazz: Class[SpecificRecord]
-  ): SpecificRecord = {
-    val reader = new SpecificDatumReader[SpecificRecord](klazz)
-    val decoder = DecoderFactory
-        .get()
-        .directBinaryDecoder(input, null)
-    reader.read(null.asInstanceOf[SpecificRecord], decoder)
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/serialization/KijiLocker.scala b/src/main/scala/org/kiji/express/flow/framework/serialization/KijiLocker.scala
deleted file mode 100644
index 48e3078d0f9e641852a8b274e3fe30141a0c60cd..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/serialization/KijiLocker.scala
+++ /dev/null
@@ -1,93 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.serialization
-
-import com.esotericsoftware.kryo.io.Output
-import com.twitter.bijection.Injection
-import com.twitter.chill.KryoBase
-import com.twitter.chill.KryoInjectionInstance
-import org.objenesis.strategy.StdInstantiatorStrategy
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-
-/**
- * Provides a constructor function for a
- * [[org.kiji.express.flow.framework.serialization.KijiLocker]].
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-// TODO (EXP-295): Should these maybe be Framework?
-object KijiLocker {
-  def apply[T <: AnyRef](t: T): KijiLocker[T] = new KijiLocker(t)
-}
-
-/**
- * A clone of Chill's [[com.twitter.chill.MeatLocker]] with serialization provided by our custom
- * [[org.kiji.express.flow.framework.serialization.KryoKiji]].
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-// TODO (EXP-295): Should these maybe be Framework?
-class KijiLocker[T <: AnyRef](@transient private var t: T) extends java.io.Serializable {
-
-  /**
-   * Creates an [[com.twitter.bijection.Injection]] for converting objects between their
-   * serialized byte for and back.
-   *
-   * @return an Object <-> Array[Byte] Injection.
-   */
-  private def injection: Injection[AnyRef, Array[Byte]] = {
-    val kryo = {
-      val kryo = new KryoBase
-      kryo.setRegistrationRequired(false)
-      kryo.setInstantiatorStrategy(new StdInstantiatorStrategy)
-      new KryoKiji().decorateKryo(kryo)
-      kryo
-    }
-    new KryoInjectionInstance(kryo, new Output( 1 << 10, 1 << 24))
-  }
-
-  /**
-   * Serialized value of t.
-   */
-  private val tBytes: Array[Byte] = injection(t)
-
-  /**
-   * Retrieve the value wrapped by this
-   * [[org.kiji.express.flow.framework.serialization.KijiLocker]].
-   *
-   * @return the value
-   */
-  def get: T = {
-    if(t == null) {
-      // we were serialized
-      t = injection
-          .invert(tBytes)
-          .getOrElse(throw new RuntimeException("Deserialization failed in KijiLocker."))
-          .asInstanceOf[T]
-    }
-    t
-  }
-}
-
diff --git a/src/main/scala/org/kiji/express/flow/framework/serialization/KryoKiji.scala b/src/main/scala/org/kiji/express/flow/framework/serialization/KryoKiji.scala
deleted file mode 100644
index becada6c16fd8156e5b1eb3355edbd1ed7030613..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/serialization/KryoKiji.scala
+++ /dev/null
@@ -1,47 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.serialization
-
-import com.esotericsoftware.kryo.Kryo
-import com.twitter.scalding.serialization.KryoHadoop
-import org.apache.avro.Schema
-import org.apache.avro.generic.GenericContainer
-import org.apache.avro.specific.SpecificRecord
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-
-/**
- * Kryo specification that adds avro schema, generic record, and specific record serialization
- * support. Used with [[org.kiji.express.flow.KijiJob]].
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-class KryoKiji extends KryoHadoop {
-  override def decorateKryo(kryo: Kryo) {
-    super.decorateKryo(kryo)
-
-    kryo.addDefaultSerializer(classOf[Schema], classOf[AvroSchemaSerializer])
-    kryo.addDefaultSerializer(classOf[GenericContainer], classOf[AvroGenericSerializer])
-    kryo.addDefaultSerializer(classOf[SpecificRecord], classOf[AvroSpecificSerializer])
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/framework/serialization/package.scala b/src/main/scala/org/kiji/express/flow/framework/serialization/package.scala
deleted file mode 100644
index 4624b13d29eca1976172231617ded1d99a66c14d..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/framework/serialization/package.scala
+++ /dev/null
@@ -1,26 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-/**
- * Package containing utility classes related to serialization of data during a KijiExpress flow.
- * Includes components for integrating avro and kryo serialization.
- */
-package object serialization
diff --git a/src/main/scala/org/kiji/express/flow/package.scala b/src/main/scala/org/kiji/express/flow/package.scala
deleted file mode 100644
index 36892235e38b869841ec6c7c9be7b2ffab8d6c1c..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/package.scala
+++ /dev/null
@@ -1,186 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express
-
-import org.apache.hadoop.hbase.HConstants
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.schema.KConstants
-import org.kiji.schema.KijiInvalidNameException
-import org.kiji.schema.filter.KijiColumnFilter
-import org.kiji.schema.filter.RegexQualifierColumnFilter
-
-/**
- * Module providing the ability to write Scalding using data stored in Kiji tables.
- *
- * KijiExpress users should import the members of this module to gain access to factory
- * methods that produce [[org.kiji.express.flow.KijiSource]]s that can perform data processing
- * operations that read from or write to Kiji tables.
- * {{{
- *   import org.kiji.express.flow._
- * }}}
- *
- * === Reading from columns and map-type column families. ===
- * Specify columns to read from a Kiji table using instances of the
- * [[org.kiji.express.flow.QualifiedColumnInputSpec]] and
- * [[org.kiji.express.flow.ColumnFamilyInputSpec]] classes, which contain fields for specifying
- * the names of the columns to read, as well as what data to read back (e.g., only the latest
- * version of a cell, or a certain number of recent versions) and how it is read back (e.g., using
- * paging to limit the amount of data in memory).
- *
- * Specify a fully-qualified column with an instance of `QualifiedColumnInputSpec`.  Below are
- * several examples for specifying the column `info:name`:
- * {{{
- *   // Request the latest cell.
- *   val myInputColumn = QualifiedColumnInputSpec("info", "name")
- *   val myInputColumn = QualifiedColumnInputSpec("info", "name", maxVersions = latest)
- *   val myInputColumn = QualifiedColumnInputSpec("info", "name", maxVersions = 1)
- *
- *   // Request every cell.
- *   val myInputColumn = QualifiedColumnInputSpec("info:name", maxVersions = all)
- *
- *   // Request the 10 most recent cells.
- *   val myInputColumn = QualifiedColumnInputSpec("info:name", maxVersions = 10)
- * }}}
- *
- * To request cells from all of the columns in a family, use the `ColumnFamilyInputSpec`
- * class, which, like `QualifiedColumnInputSpec`, provides options on the input spec such as
- * the maximum number of cell versions to return, filters to use, etc.  A user can
- * specify a filter, for example, to specify a regular expression such that a column in the family
- * will only be retrieved if its qualifier matches the regular expression:
- * {{{
- *   // Gets the most recent cell for all columns in the column family "searches".
- *   var myFamilyInput = ColumnFamilyInputSpec("searches")
- *
- *   // Gets all cells for all columns in the column family "searches" whose
- *   // qualifiers contain the word "penguin".
- *   myFamilyInput = ColumnFamilyInputSpec(
- *      "searches",
- *      filter = Some(new RegexQualifierColumnFilter(""".*penguin.*""")),
- *      maxVersions = all)
- *
- *   // Gets all cells for all columns in the column family "searches".
- *   myFamilyInput = ColumnFamilyInputSpec("searches", maxVersions = all)
- * }}}
- *
- * See [[org.kiji.express.flow.QualifiedColumnInputSpec]] and
- * [[org.kiji.express.flow.ColumnFamilyInputSpec]] for a full list of options for column input
- * specs.
- *
- * When specifying a column for writing, the user can likewise use the
- * `QualifiedColumnOutputSpec` and `ColumnFamilyOutputSpec` classes to indicate the name of
- * the column and any options.  The following, for example, specifies a column to use for writes
- * with the default reader schema:
- * {{{
- *   // Create a column output spec for writing to "info:name" using the default reader schema
- *   var myWriteReq = QualifiedColumnOutputSpec("info", "name", useDefaultReaderSchema = true)
- * }}}
- *
- *
- * When writing to a family, you specify a Scalding field that contains the name of the qualifier to
- * use for your write.  For example, to use the value in the Scalding field ``'terms`` as the name
- * of the column qualifier, use the following:
- * {{{
- *   var myOutputFamily = ColumnFamilyOutputSpec("searches", 'terms)
- * }}}
- *
- * See [[org.kiji.express.flow.QualifiedColumnOutputSpec]] and
- * [[org.kiji.express.flow.ColumnFamilyOutputSpec]] for a full list of options for column output
- * specs.
- *
- * === Getting input from a Kiji table. ===
- * The factory `KijiInput` can be used to obtain a
- * [[org.kiji.express.flow.KijiSource]] to process rows from the table (represented as tuples)
- * using various operations. When using `KijiInput`, users specify a table (using a Kiji URI) and
- * use column specs and other options to control how data is read from Kiji into tuple fields.
- * ``KijiInput`` contains different factories that allow for abbreviated column specifications,
- * as illustrated in the examples below:
- * {{{
- *   // Read the most recent cells from columns "info:id" and "info:name" into tuple fields "id"
- *   // and "name" (don't explicitly instantiate a QualifiedColumnInputSpec).
- *   var myKijiSource =
- *       KijiInput("kiji://.env/default/newsgroup_users", "info:id" -> 'id, "info:name" -> 'name)
- *
- *   // Read only cells from "info:id" that occurred before Unix time 100000.
- *   // (Don't explicitly instantiate a QualifiedColumnInputSpec)
- *   myKijiSource =
- *       KijiInput("kiji://.env/default/newsgroup_users", Before(100000), "info:id" -> 'id)
- *
- *   // Read all versions from "info:posts"
- *   myKijiSource = KijiInput(
- *       "kiji://.env/default/newsgroup_users",
- *       Map(QualifiedColumnOutputSpec("info", "id", maxVersions = all) -> 'id))
- * }}}
- *
- * See [[org.kiji.express.flow.KijiInput]] and [[org.kiji.express.flow.ColumnInputSpec]] for more
- * information on how to create and use time ranges for requesting data.
- *
- * === Writing to a Kiji table. ===
- * Data from any Cascading `Source` can be written to a Kiji table. Tuples to be written to a
- * Kiji table must have a field named `entityId` which contains an entity id for a row in a Kiji
- * table. The contents of a tuple field can be written as a cell at the most current timestamp to
- * a column in a Kiji table. To do so, you specify a mapping from tuple field names to qualified
- * Kiji table column names.
- * {{{
- *   // Write from the tuple field "average" to the column "stats:average" of the Kiji table
- *   // "newsgroup_users".
- *   mySource.write("kiji://.env/default/newsgroup_users", 'average -> "stats:average")
- *
- *   // Create a KijiSource to write the data in tuple field "results" to column family
- *   // "searches" with the string in tuple field "terms" as the column qualifier.
- *   myOutput = KijiOutput(
- *       "kiji://.env/default/searchstuff",
- *       'results -> ColumnFamilyOutputSpec("searches", "terms"))
- * }}}
- *
- * === Specifying ranges of time. ===
- * Instances of [[org.kiji.express.flow.TimeRange]] are used to specify a range of timestamps
- * that should be retrieved when reading data from Kiji. There are five implementations of
- * `TimeRange` that can be used when requesting data.
- *
- * <ul>
- *   <li>All</li>
- *   <li>At(timestamp: Long)</li>
- *   <li>After(begin: Long)</li>
- *   <li>Before(end: Long)</li>
- *   <li>Between(begin: Long, end: Long)</li>
- * </ul>
- *
- * These implementations can be used with [[org.kiji.express.flow.KijiInput]] to specify a range
- * that a Kiji cell's timestamp must be in to be retrieved. For example,
- * to read cells from the column `info:word` that have timestamps between `0L` and `10L`,
- * you can do the following.
- *
- * @example {{{
- *     KijiInput("kiji://.env/default/words", timeRange=Between(0L, 10L), "info:word" -> 'word)
- * }}}
- */
-package object flow {
-
-  /** Used with a column input spec to indicate that all cells of a column should be retrieved. */
-  val all = HConstants.ALL_VERSIONS
-
-  /**
-   * Used with a column input spec to indicate that only the latest cell of a column should be
-   * retrieved.
-   */
-  val latest = 1
-}
diff --git a/src/main/scala/org/kiji/express/flow/tool/TmpJarsTool.scala b/src/main/scala/org/kiji/express/flow/tool/TmpJarsTool.scala
deleted file mode 100644
index 21943c07136a6864b6f1d5a3900f150009d34a5f..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/tool/TmpJarsTool.scala
+++ /dev/null
@@ -1,149 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.tool
-
-import java.io.File
-
-import org.apache.hadoop.hbase.HBaseConfiguration
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.mapreduce.util.Jars
-
-/**
- * Reads a colon-separated list of classpath entries and outputs comma-separated list of
- * URIs to jar files accessible from that classpath, as well as the HBase library jars.
- *
- * This tool is meant to be used to specify a collection of jars that should be included on the
- * distributed cache of Hadoop jobs run through KijiExpress. It should be used from the `express`
- * script and should be passed a classpath entry for the KijiExpress `lib` directory as well
- * as any classpath entries specified by the user through `EXPRESS_CLASSPATH`.
- *
- * Classpath entries can come in one of three forms:
- *
- * 1. A path to a specific file. In this case we send the file through the distributed cache if
- * it is a jar.
- * 2. A path to a directory. Since directory entries do not include jars on the classpath,
- * we send no jars through the distributed cache.
- * 3. A path to a directory with the wildcard * appended. In this case all jars under the
- * specified directory are sent through the distributed cache.
- *
- * This tool accepts one command line argument: a string containing a colon-separated list of
- * classpath entries.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-@Inheritance.Sealed
-object TmpJarsTool {
-
-  /**
-   * Constructs a file from a classpath entry.
-   *
-   * @param entry the file will be constructed from.
-   * @return a file for the classpath entry.
-   */
-  private[tool] def entryToFile(entry: String): File = new File(entry)
-
-  /**
-   * If a file is a glob entry, transform it into all files present in the globed directory.
-   *
-   * @param maybeGlob is a file that might be a glob entry from the classpath.
-   * @return the original file if it was not a glob, the files present in the globed directory
-   *     otherwise.
-   */
-  private[tool] def globToFiles(maybeGlob: File): Array[File] = maybeGlob.getName match {
-    case "*" => maybeGlob.getParentFile
-                .listFiles()
-                .filterNot { _.isDirectory }
-    case _ => Array(maybeGlob)
-  }
-
-  /**
-   * Determines if a file has the `.jar` or `.JAR` extensions.
-   *
-   * @param maybeJar is a file that may, in fact, be a jar.
-   * @return `true` if the file is jar, `false` otherwise.
-   */
-  private[tool] def isJar(maybeJar: File): Boolean = {
-    maybeJar.getName.endsWith(".jar") || maybeJar.getName.endsWith(".JAR")
-  }
-
-  /**
-   * Transforms a colon-separated list of classpath entries into the jar files accessible from
-   * the classpath entries.
-   *
-   * @param classpath is a colon-separated list of classpath entries.
-   * @return the jar files accessible from the classpath entries.
-   */
-  private[tool] def getJarsFromClasspath(classpath: String): Array[File] = {
-    classpath
-        .split(':')
-        .filterNot { _.isEmpty }
-        .map { entryToFile }
-        .filterNot { _.isDirectory }
-        .flatMap { globToFiles }
-        .filter { isJar }
-  }
-
-  /**
-   * Retrieves all jars located in the directory containing the HBase library jar.
-   *
-   * @return the jar files contained in the HBase lib directory.
-   */
-  private def getJarsForHBase: Array[File] = {
-    val hbaseJar: File = new File(Jars.getJarPathForClass(classOf[HBaseConfiguration]))
-    hbaseJar.getParentFile.listFiles()
-        .filterNot { _.isDirectory }
-        .filter { isJar }
-  }
-
-  /**
-   * Formats the paths to jar files as a comma-separated list of URIs to files on the local file
-   * system.
-   *
-   * @param jars that should be added to the generated comma-separated list.
-   * @return a comma-separated list of URIs to jar files.
-   */
-  private[tool] def toJarURIList(jars: Array[File]): String = {
-    val pathToPathWithScheme = (path: String) => "file://" + path
-    val joinPaths = (path1: String, path2: String) => path1 + "," + path2
-    jars
-        .map { _.getCanonicalPath }
-        .map { pathToPathWithScheme }
-        .reduce { joinPaths }
-  }
-
-  /**
-   * Transforms a classpath into a comma-separated list of URIs to jar files accessible from the
-   * classpath.
-   *
-   * @param args from the command line, which should only include a colon-separated classpath.
-   */
-  def main(args: Array[String]) {
-    if (args.length != 1) {
-      println("Usage: TmpJarsTool <classpath>")
-    }
-    val jarsFromClasspath = getJarsFromClasspath(args(0))
-    val jarsFromHBase = getJarsForHBase
-    val jarsForDCache = jarsFromClasspath ++ jarsFromHBase
-    println(toJarURIList(jarsForDCache))
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/tool/package.scala b/src/main/scala/org/kiji/express/flow/tool/package.scala
deleted file mode 100644
index 3a1d57eb2ee2f5f464d92ccba75687b65d0e7517..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/tool/package.scala
+++ /dev/null
@@ -1,25 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-/**
- * Package containing supporting classes for KijiExpress' command-line interface.
- */
-package object tool
diff --git a/src/main/scala/org/kiji/express/flow/util/AvroTupleConversions.scala b/src/main/scala/org/kiji/express/flow/util/AvroTupleConversions.scala
deleted file mode 100644
index d1fd505d7449785577122d847fd1e8928accc983..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/AvroTupleConversions.scala
+++ /dev/null
@@ -1,201 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import java.lang.reflect.Method
-
-import scala.collection.JavaConversions.asScalaIterator
-import scala.reflect.Manifest
-
-import cascading.tuple.Fields
-import cascading.tuple.Tuple
-import cascading.tuple.TupleEntry
-import com.twitter.scalding.TupleConversions
-import com.twitter.scalding.TupleConverter
-import com.twitter.scalding.TuplePacker
-import com.twitter.scalding.TupleSetter
-import com.twitter.scalding.TupleUnpacker
-import org.apache.avro.Schema
-import org.apache.avro.generic.GenericRecord
-import org.apache.avro.generic.GenericRecordBuilder
-import org.apache.avro.specific.SpecificRecord
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.framework.serialization.KijiLocker
-
-/**
- * Provides implementations of Scalding abstract classes to enable packing and unpacking Avro
- * specific and generic records.  Also provides implicit definitions to support implicitly using
- * these classes, where necessary.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-trait AvroTupleConversions {
-  /**
-   * [[com.twitter.scalding.TuplePacker]] implementation provides instances of the
-   * [[org.kiji.express.flow.util.AvroSpecificTupleConverter]] for converting fields
-   * in a Scalding flow into specific Avro records of the parameterized type.  An instance of this
-   * class must be in implicit scope, or passed in explicitly to
-   * [[com.twitter.scalding.RichPipe.pack]].
-   *
-   * @tparam T type of compiled Avro class to be packed (created).
-   * @param m [[scala.reflect.Manifest]] of T.  Provided implicitly by a built-in conversion.
-   */
-  private[express] class AvroSpecificTuplePacker[T <: SpecificRecord](implicit m: Manifest[T])
-      extends TuplePacker[T] {
-    override def newConverter(fields: Fields): TupleConverter[T] = {
-      new AvroSpecificTupleConverter(fields, m)
-    }
-  }
-
-  /**
-   * Provides a [[com.twitter.scalding.TupleSetter]] for unpacking an Avro
-   * [[org.apache.avro.generic.GenericRecord]] into a [[cascading.tuple.Tuple]].
-   */
-  private[express] class AvroGenericTupleUnpacker extends TupleUnpacker[GenericRecord] {
-    override def newSetter(fields: Fields): TupleSetter[GenericRecord] = {
-      new AvroGenericTupleSetter(fields)
-    }
-  }
-
-  /**
-   * Takes an Avro [[org.apache.avro.generic.GenericRecord]] and unpacks the specified fields
-   * into a new [[cascading.tuple.Tuple]].
-   *
-   * @param fs the fields to be unpacked from the [[org.apache.avro.generic.GenericRecord]].
-   */
-  private[express] class AvroGenericTupleSetter(fs: Fields) extends TupleSetter[GenericRecord] {
-    override def arity: Int = fs.size
-
-    private val fields: List[String] = fs.iterator.map(_.toString).toList
-
-    override def apply(arg: GenericRecord): Tuple = {
-      new Tuple(fields.map(arg.get): _*)
-    }
-  }
-
-  /**
-   * Provides an [[org.kiji.express.flow.util.AvroTupleConversions.AvroSpecificTuplePacker]] to the
-   * implicit scope.
-   *
-   * @tparam T Avro compiled [[org.apache.avro.specific.SpecificRecord]] class.
-   * @param mf implicitly provided [[scala.reflect.Manifest]] of provided Avro type.
-   * @return [[org.kiji.express.flow.util.AvroTupleConversions.AvroSpecificTuplePacker]] for given
-   *         Avro specific record type
-   */
-  private[express] implicit def avroSpecificTuplePacker[T <: SpecificRecord]
-      (implicit mf: Manifest[T]): AvroSpecificTuplePacker[T] = {
-    new AvroSpecificTuplePacker[T]
-  }
-
-  /**
-   * Provides an [[org.kiji.express.flow.util.AvroTupleConversions.AvroGenericTupleUnpacker]] to
-   * implicit scope.
-   *
-   * @return an [[org.kiji.express.flow.util.AvroTupleConversions.AvroGenericTupleUnpacker]].
-   */
-  private[express] implicit def avroGenericTupleUnpacker: AvroGenericTupleUnpacker = {
-    new AvroGenericTupleUnpacker
-  }
-}
-
-/**
- * Converts [[cascading.tuple.TupleEntry]]s with the given fields into an Avro
- * [[org.apache.avro.specific.SpecificRecord]] instance of the parameterized type.  This
- * converter will fill in default values of the record if not specified by tuple fields.
- *
- * @tparam T Type of the target specific Avro record.
- * @param fs The fields to convert into an Avro record.  The field names must match the field
- *           names of the Avro record type.  There must be only one result field.
- * @param m [[scala.reflect.Manifest]] of the target type.  Implicitly provided.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] case class AvroSpecificTupleConverter[T](fs: Fields, m: Manifest[T])
-    extends TupleConverter[T] {
-
-  override def arity: Int = -1
-
-  /**
-   * Attempts to translate an avro field name (e.g. count, my_count, should_be_snake_case) to an
-   * Avro record setter name (e.g., setCount, setMyCount, setShouldBeSnakeCase).
-   * @param field to translate to setter format.
-   * @return setter of given field.
-   */
-  private def fieldToSetter(field: String): String = {
-    field.split('_').map(_.capitalize).mkString("set", "", "")
-  }
-
-  // Precompute as much of the reflection business as possible.  Method objects do not serialize,
-  // so any val containing a method in it must be lazy.
-  private val avroClass: Class[_] = m.erasure
-  private val builderClass: Class[_] =
-      avroClass.getDeclaredClasses.find(_.getSimpleName == "Builder").get
-  lazy private val newBuilderMethod: Method = avroClass.getMethod("newBuilder")
-  lazy private val buildMethod: Method = builderClass.getMethod("build")
-
-  /**
-   * Map of field name to setter method.
-   */
-  lazy private val fieldSetters: Map[String, Method] = {
-    val fields: List[String] = fs.iterator.map(_.toString).toList
-    val setters: Map[String, Method] = builderClass
-        .getDeclaredMethods
-        .map { m => (m.getName, m) }
-        .toMap
-
-    fields
-        .zip(fields.map(fieldToSetter))
-        .toMap
-        .mapValues(setters)
-  }
-
-  override def apply(entry: TupleEntry): T = {
-    val builder = newBuilderMethod.invoke(avroClass)
-    fieldSetters.foreach { case (field, setter) => setter.invoke(builder, entry.getObject(field)) }
-    buildMethod.invoke(builder).asInstanceOf[T]
-  }
-}
-
-/**
- * Converts [[cascading.tuple.TupleEntry]]s into an Avro [[org.apache.avro.generic.GenericRecord]]
- * object with the provided schema.  This converter will fill in default values of the schema if
- * they are not specified through fields.
- *
- * @param schemaLocker wrapping the schema of the target record
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] class AvroGenericTupleConverter(schemaLocker: KijiLocker[Schema])
-    extends TupleConverter[GenericRecord] with TupleConversions {
-
-  override def arity: Int = -1
-
-  override def apply(entry: TupleEntry): GenericRecord = {
-    val builder = new GenericRecordBuilder(schemaLocker.get)
-    toMap(entry).foreach { kv => builder.set(kv._1, kv._2) }
-    builder.build()
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/AvroUtil.scala b/src/main/scala/org/kiji/express/flow/util/AvroUtil.scala
deleted file mode 100644
index 8718c923fcb7e593a99aecd3b7d07cfadc4776c1..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/AvroUtil.scala
+++ /dev/null
@@ -1,194 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import java.nio.ByteBuffer
-
-import scala.collection.JavaConverters.asScalaBufferConverter
-import scala.collection.JavaConverters.mapAsScalaMapConverter
-import scala.collection.JavaConverters.seqAsJavaListConverter
-
-import org.apache.avro.Schema
-import org.apache.avro.generic.GenericData
-import org.apache.avro.generic.GenericData.Fixed
-import org.apache.avro.generic.GenericEnumSymbol
-import org.apache.avro.generic.GenericFixed
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-
-/**
- * A module with functions that can convert Java values (including Java API Avro values)
- * read from a Kiji table to Scala values (including our Express-specific generic API Avro values)
- * for use in KijiExpress, and vice versa for writing back to a Kiji table.
- *
- * When reading from Kiji, using the specific API, use the method `decodeSpecificFromJava`.
- * When reading from Kiji, using the generic API, use the method `decodeGenericFromJava`.
- * When writing to Kiji, use the method `encodeToJava`.
- *
- * When decoding using the generic API, primitives are converted to Scala primitives, and records
- * are converted to AvroValues. The contents of a record are also AvroValues, even if they
- * represent primitives.  These primitives can be accessed with `asInt`, `asLong`, etc methods.
- * For example: `myRecord("fieldname").asInt` gets an integer field with name `fieldname` from
- * `myRecord`.
- *
- * When decoding using the specific API, primitives are converted to Scala primitives, and records
- * are converted to their corresponding Java classes.
- *
- * Certain AvroValues are used in both the specific and the generic API:
- * <ul>
- *   <li>AvroEnum, which always wraps an enum from Avro.  This is because there is not an easy
- *       Scala equivalent of a Java enum.</li>
- *   <li>AvroFixed, which always wraps a fixed-length byte array from Avro.  This is to distinguish
- *       between a regular byte array, which gets converted to an Array[Byte], and a fixed-length
- *       byte array.</li>
- * <ul>
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-object AvroUtil {
-
-  /**
-   * Attempts to convert complex Avro types to their Scala equivalents.  If no such conversion
-   * exists, passes the value through.  In particular, four conversion are attempted:
-   *
-   * <ul>
-   *   <li>Java [[java.util.Map]]s will be converted to Scala [[scala.collection.Map]].</li>
-   *   <li>Java [[java.util.List]]s (including [[org.apache.avro.generic.GenericArray]]s) will be
-   *       converted to a [[scala.collection.mutable.Buffer]]s.</li>
-   *   <li>Avro [[org.apache.avro.generic.GenericFixed]] instances will be converted to Scala
-   *       [[scala.Array[Byte]]]s.</li>
-   *   <li>Avro [[org.apache.avro.generic.GenericEnumSymbol]]s will be converted to their
-   *       [[java.lang.String]] equivalent.</li>
-   * </ul>
-   *
-   * @param value to be converted to Scala equivalent
-   * @return Scala equivalent, or original object if no such equivalent exists
-   */
-  private[express] def avroToScala(value: Any): Any = value match {
-    case m: java.util.Map[_, _] => m.asScala.mapValues(avroToScala)
-    case l: java.util.List[_] => l.asScala.map(avroToScala)
-    case f: GenericFixed => f.bytes
-    case e: GenericEnumSymbol => e.toString
-    case other => other
-  }
-
-  /**
-   * Provides an encoder function that will coerce a [[scala.collection.TraversableOnce]] to an
-   * [[org.apache.avro.generic.GenericData.Array]].
-   *
-   * @param schema of the array
-   * @return an encoding function for Avro array types.
-   */
-  private[express] def arrayEncoder(schema: Schema): Any => Any = {
-    require(schema.getType == Schema.Type.ARRAY)
-    return {
-      case tr: TraversableOnce[_] =>
-        new GenericData.Array(schema, tr.map(avroEncoder(schema.getElementType)).toList.asJava)
-      case other => other
-    }
-  }
-
-  /**
-   * Provides an encoder function that will coerce values to an
-   * [[org.apache.avro.generic.GenericData.EnumSymbol]] if possible.
-   *
-   * @param schema of enum type
-   * @return an encoder function for enum values.
-   */
-  private[express] def enumEncoder(schema: Schema): Any => Any = {
-    require(schema.getType == Schema.Type.ENUM)
-    val genericData = new GenericData()
-
-    return {
-      case e: Enum[_] =>
-        // Perhaps useful for the case where a user defines their own enum with the same members
-        // as defined in the schema.
-        genericData.createEnum(e.name, schema)
-      case s: String => genericData.createEnum(s, schema)
-      case other => other
-    }
-  }
-
-  /**
-   * Provides an encoder function that will coerce values into an Avro compatible bytes format.
-   *
-   * @param schema of bytes type
-   * @return an encoder function for bytes values
-   */
-  private[express] def bytesEncoder(schema: Schema): Any => Any = {
-    require(schema.getType == Schema.Type.BYTES)
-
-    return {
-      case bs: Array[Byte] => ByteBuffer.wrap(bs)
-      case other => other
-    }
-  }
-
-  /**
-   * Provides an encoder function that will coerce values into an Avro generic fixed.
-   *
-   * @param schema of fixed type.
-   * @return an encoder function for fixed values.
-   */
-  private[express] def fixedEncoder(schema: Schema): Any => Any = {
-    require(schema.getType == Schema.Type.FIXED)
-
-    def getBytes(bb: ByteBuffer): Array[Byte] = {
-      val backing = bb.array()
-      val remaining = bb.remaining()
-      if (remaining == backing.length) { // no need to copy
-        backing
-      } else {
-        val copy = Array.ofDim[Byte](remaining)
-        bb.get(copy)
-        copy
-      }
-    }
-
-    return {
-      case bs: Array[Byte] => new Fixed(schema, bs)
-      case bb: ByteBuffer if bb.hasArray => new Fixed(schema, getBytes(bb))
-      case other => other
-    }
-  }
-
-  /**
-   * Creates an encoder function for a given schema.  For most cases no encoding needs to be done,
-   * so the encoder is the identity function.  For the collection types that have generic
-   * implementations we can convert a scala version of the type to the generic avro version.  We
-   * don't try to catch type mismatches here; instead Schema will check if the value is compatible
-   * with the given schema.
-   *
-   * @param schema of the values that the encoder function will take.
-   * @return an encoder function that will make a best effort to encode values to a type
-   *    compatible with the given schema.
-   */
-  private[express] def avroEncoder(schema: Schema): Any => Any = {
-    schema.getType match {
-      case Schema.Type.ARRAY => arrayEncoder(schema)
-      case Schema.Type.ENUM => enumEncoder(schema)
-      case Schema.Type.BYTES => bytesEncoder(schema)
-      case Schema.Type.FIXED => fixedEncoder(schema)
-      case _ => identity
-    }
-  }
-
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/CellMathUtil.scala b/src/main/scala/org/kiji/express/flow/util/CellMathUtil.scala
deleted file mode 100644
index 4d449bba47a1e768dfe88ce7c34fd45f8e0e5802..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/CellMathUtil.scala
+++ /dev/null
@@ -1,181 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import scala.annotation.implicitNotFound
-
-import org.slf4j.Logger
-import org.slf4j.LoggerFactory
-
-import org.kiji.express.flow.FlowCell
-
-/**
- * Provides aggregator functions for sequences of [[org.kiji.express.flow.FlowCell]]s.
- */
-object CellMathUtil {
-  private val logger: Logger = LoggerFactory.getLogger(CellMathUtil.getClass)
-
-  /**
-   * Computes the sum of the values stored within the [[org.kiji.express.flow.FlowCell]]s in the
-   * provided `Seq`.
-   *
-   * <b>Note:</b> This method will not compile unless the cells contained within this collection
-   * contain values of a type that has an implicit implementation of the `scala.Numeric` trait.
-   *
-   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
-   * @param slice is the collection of [[org.kiji.express.flow.FlowCell]]s to sum.
-   * @return the sum of the values within the [[org.kiji.express.flow.FlowCell]]s in the provided
-   *     slice.
-   */
-  @implicitNotFound("The type of data contained within the provided cells does not support" +
-      " numeric operations through the scala.Numeric trait.")
-  def sum[T](slice: Seq[FlowCell[T]])(implicit num: Numeric[T]): T = {
-    slice.foldLeft(num.zero) { (sum: T, cell: FlowCell[T]) =>
-      num.plus(sum, cell.datum)
-    }
-  }
-
-  /**
-   * Computes the mean of the values stored within the [[org.kiji.express.flow.FlowCell]]s in the
-   * provided slice.
-   *
-   * <b>Note:</b> This method will not compile unless the cells contained within this collection
-   * contain values of a type that has an implicit implementation of the `scala.Numeric` trait.
-   *
-   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
-   * @param slice is the collection of [[org.kiji.express.flow.FlowCell]]s to compute the mean of.
-   * @return the mean of the values within the provided [[org.kiji.express.flow.FlowCell]]s.
-   */
-  @implicitNotFound("The type of data contained within the provided cells does not support" +
-      " numeric operations through the scala.Numeric trait.")
-  def mean[T](slice: Seq[FlowCell[T]])(implicit num: Numeric[T]): Double = {
-    val n = slice.size
-    slice.foldLeft(0.0) { (mean: Double, cell: FlowCell[T]) =>
-      mean + (num.toDouble(cell.datum) / n)
-    }
-  }
-
-  /**
-   * Finds the minimum of the values stored within the [[org.kiji.express.flow.FlowCell]]s in the
-   * provided slice.
-   *
-   * <b>Note:</b> This method will not compile unless the cells contained within this collection
-   * contain values of a type that has an implicit implementation of the `scala.Ordering` trait.
-   *
-   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
-   * @param slice is the collection of [[org.kiji.express.flow.FlowCell]]s to find the minimum of.
-   * @return the minimum value contained in the `Seq` of [[org.kiji.express.flow.FlowCell]]s.
-   */
-  @implicitNotFound("The type of data contained within the provided cells does not support" +
-      " ordering operations through the scala.Ordering trait.")
-  def min[T](slice: Seq[FlowCell[T]])(implicit cmp: Ordering[T]): T = {
-    slice.min(Ordering.by { cell: FlowCell[T] => cell.datum }).datum
-  }
-
-  /**
-   * Finds the maximum of the values stored within the [[org.kiji.express.flow.FlowCell]]s in the
-   * provided slice.
-   *
-   * <b>Note:</b> This method will not compile unless the cells contained within this collection
-   * contain values of a type that has an implicit implementation of the `scala.Ordering` trait.
-   *
-   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
-   * @param slice is the collection of [[org.kiji.express.flow.FlowCell]]s to find the maximum of.
-   * @return the maximum of the value within the provided [[org.kiji.express.flow.FlowCell]]s.
-   */
-  @implicitNotFound("The type of data contained within the provided cells does not support" +
-      " ordering operations through the scala.Ordering trait.")
-  def max[T](slice: Seq[FlowCell[T]])(implicit cmp: Ordering[T]): T = {
-    slice.max(Ordering.by { cell: FlowCell[T] => cell.datum }).datum
-  }
-
-  /**
-   * Computes the standard deviation of the values stored within the
-   * [[org.kiji.express.flow.FlowCell]]s in the provided slice.
-   *
-   * <b>Note:</b> This method will not compile unless the cells contained within this collection
-   * contain values of a type that has an implicit implementation of the `scala.Numeric` trait.
-   *
-   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
-   * @param slice the `Seq` of [[org.kiji.express.flow.FlowCell]]s to compute the standard
-   *     deviation of.
-   * @return the standard deviation of the values within the [[org.kiji.express.flow.FlowCell]]s in
-   *     the provided slice.
-   */
-  @implicitNotFound("The type of data contained within the provided cells does not support" +
-      " numeric operations through the scala.Numeric trait.")
-  def stddev[T](slice:Seq[FlowCell[T]])(implicit num: Numeric[T]): Double = {
-    scala.math.sqrt(variance(slice))
-  }
-
-  /**
-   * Computes the squared sum of the values stored within the [[org.kiji.express.flow.FlowCell]]s in
-   * the provided slice.
-   *
-   * <b>Note:</b> This method will not compile unless the cells contained within this collection
-   * contain values of a type that has an implicit implementation of the `scala.Numeric` trait.
-   *
-   * @tparam T the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
-   * @param slice the collection of [[org.kiji.express.flow.FlowCell]]s to compute the squared
-   *     sum of.
-   * @return the squared sum of the values in the provided [[org.kiji.express.flow.FlowCell]]s.
-   */
-  @implicitNotFound("The type of data contained within the provided cells does not support" +
-      " numeric operations through the scala.Numeric trait.")
-  def sumSquares[T](slice: Seq[FlowCell[T]])(implicit num: Numeric[T]): T = {
-    slice.foldLeft(num.zero) { (sumSquares: T, cell: FlowCell[T]) =>
-      num.plus(sumSquares, num.times(cell.datum, cell.datum))
-    }
-  }
-
-  /**
-   * Computes the variance of the values stored within the [[org.kiji.express.flow.FlowCell]]s in
-   * the provided slice.
-   *
-   * <b>Note:</b> This method will not compile unless the cells contained within this collection
-   * contain values of a type that has an implicit implementation of the `scala.Numeric` trait.
-   *
-   * @tparam T is the type of the [[org.kiji.express.flow.FlowCell]]s contained in the slice.
-   * @param slice is the collection of [[org.kiji.express.flow.FlowCell]]s to compute the
-   *     variance of.
-   * @return the variance of the values within the [[org.kiji.express.flow.FlowCell]]s in the
-   *     provided slice.
-   */
-  @implicitNotFound("The type of data contained within the provided cells does not support" +
-      " numeric operations through the scala.Numeric trait.")
-  def variance[T](slice: Seq[FlowCell[T]])(implicit num: Numeric[T]): Double = {
-    val (n, _, m2) = slice
-        .foldLeft((0L, 0.0, 0.0)) { (acc: (Long, Double, Double), cell: FlowCell[T]) =>
-          val (n, mean, m2) = acc
-          logger.debug("cell: %s".format(cell.datum))
-          logger.debug("acc: %s".format(acc))
-          val x = num.toDouble(cell.datum)
-
-          val nPrime = n + 1
-          val delta = x - mean
-          val meanPrime = mean + delta / nPrime
-          val m2Prime = m2 + delta * (x - meanPrime)
-
-          (nPrime, meanPrime, m2Prime)
-        }
-
-    m2 / n
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/EntityIdFactoryCache.scala b/src/main/scala/org/kiji/express/flow/util/EntityIdFactoryCache.scala
deleted file mode 100644
index ce93653c25720ba1766c8af4f29e6cda7ad7c718..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/EntityIdFactoryCache.scala
+++ /dev/null
@@ -1,126 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import java.io.ByteArrayInputStream
-import java.io.ByteArrayOutputStream
-import java.io.DataInputStream
-import java.io.DataOutputStream
-import java.nio.ByteBuffer
-
-import org.apache.hadoop.conf.Configuration
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.express.flow.util.Resources._
-import org.kiji.schema.EntityIdFactory
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiURI
-
-/**
- * EntityIdFactoryCache performs the operation of getting the EntityIdFactory for a Kiji
- * table in a memoized way. If the required EntityIdFactory is not present in its cache, this
- * will open a connection to a Kiji table, get the factory from it and cache it for
- * subsequent calls. There will be one such cache per JVM.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-private[express] object EntityIdFactoryCache {
-  /**
-   * Memoizes construction of EntityId factories. The conf is represented as a ByteBuffer for proper
-   * comparison.
-   */
-  private val factoryCache: Memoize[(KijiURI, ByteBuffer), EntityIdFactory] =
-      Memoize { entry: (KijiURI, ByteBuffer) =>
-        val (tableUri, serializedConf) = entry
-        val conf = deserializeConf(serializedConf)
-
-        val tableLayout = doAndRelease(Kiji.Factory.open(tableUri, conf)) { kiji: Kiji =>
-          doAndRelease(kiji.openTable(tableUri.getTable)) { table: KijiTable =>
-            table.getLayout
-          }
-        }
-        EntityIdFactory.getFactory(tableLayout)
-      }
-
-  /** Memoizes construction of KijiURIs. */
-  private val uriCache: Memoize[String, KijiURI] =
-      Memoize { tableUri: String =>
-        KijiURI.newBuilder(tableUri).build()
-      }
-
-  /**
-   * Get an EntityIdFactory for the table specified. This method memoizes EntityId factory
-   * construction and will not fetch the most up-to-date factory from the addressed table.
-   *
-   * @param tableUri of the Kiji table to fetch an EntityId factory from.
-   * @param conf identifying the cluster to use when building EntityIds.
-   * @return an EntityIdFactory associated with the addressed table.
-   */
-  private[express] def getFactory(
-      tableUri: String,
-      conf: Configuration): EntityIdFactory = {
-    val uri: KijiURI = uriCache(tableUri)
-    getFactory(uri, conf)
-  }
-
-  /**
-   * Get an EntityIdFactory for the table specified. This method memoizes EntityId factory
-   * construction and will not fetch the most up-to-date factory from the addressed table.
-   *
-   * @param tableUri of the Kiji table to fetch an EntityId factory from.
-   * @param conf identifying the cluster to use when building EntityIds.
-   * @return an EntityIdFactory associated with the addressed table.
-   */
-  private[express] def getFactory(
-      tableUri: KijiURI,
-      conf: Configuration): EntityIdFactory = {
-    factoryCache(tableUri, serializeConf(conf))
-  }
-
-  /**
-   * Serializes a configuration into a string.
-   *
-   * @param conf to serialize.
-   * @return the serialized configuration.
-   */
-  private[express] def serializeConf(conf: Configuration): ByteBuffer = {
-    val confOutputStreamWriter = new ByteArrayOutputStream()
-    val dataOutputStream = new DataOutputStream(confOutputStreamWriter)
-    conf.write(dataOutputStream)
-    dataOutputStream.close
-    return ByteBuffer.wrap(confOutputStreamWriter.toByteArray)
-  }
-
-  /**
-   * Deserializes a conf from a string.
-   *
-   * @param serializedConf to deserialize
-   * @return A configuration deserialized from `serializedConf`.
-   */
-  private[express] def deserializeConf(serializedConf: ByteBuffer): Configuration = {
-    val in = new ByteArrayInputStream(serializedConf.array())
-    val conf = new Configuration()
-    conf.readFields(new DataInputStream(in))
-    in.close()
-    return conf
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/GenericCellSpecs.scala b/src/main/scala/org/kiji/express/flow/util/GenericCellSpecs.scala
deleted file mode 100644
index 109264f6d29db463f9723b08977e9b4a52b1eea6..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/GenericCellSpecs.scala
+++ /dev/null
@@ -1,93 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import java.util.{Map => JMap}
-
-import scala.collection.JavaConverters.asScalaSetConverter
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.schema.GenericCellDecoderFactory
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiTable
-import org.kiji.schema.layout.CellSpec
-import org.kiji.schema.layout.KijiTableLayout
-
-/**
- * A factory for mappings from columns in a Kiji table to generic cell specifications for those
- * columns. This factory can be used to easily construct generic cell specifications for a table,
- * which can then be passed to a Kiji table reader to allow for reading data generically. See
- * [[org.kiji.schema.KijiReaderFactory]] for more information on how to use these cell specs to
- * obtain a Kiji table reader that decodes data using the Avro GenericData API.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-object GenericCellSpecs {
-
-  /**
-   * Gets a generic cell specification mapping for a Kiji table.
-   *
-   * @param table used to get the generic cell specification mapping.
-   * @return a map from names of columns in the table to generic cell specifications for the
-   *     columns.
-   */
-  def apply(table: KijiTable): JMap[KijiColumnName, CellSpec] = {
-    apply(table.getLayout())
-  }
-
-  /**
-   * Gets a generic cell specification mapping for a layout of a Kiji table.
-   *
-   * @param layout used to get the generic cell specification mapping.
-   * @return a map from names of columns in the layout to generic cell specifications for the
-   *     columns.
-   */
-  def apply(layout: KijiTableLayout): JMap[KijiColumnName, CellSpec] = {
-    return createCellSpecMap(layout)
-  }
-
-  /**
-   * Creates a mapping from columns in a table layout to generic cell specifications for those
-   * columns.
-   *
-   * @param layout used to create the generic cell specifications.
-   * @return a map from the names of columns in the layout to generic cell specifications for
-   *     those columns.
-   */
-  private def createCellSpecMap(layout: KijiTableLayout): JMap[KijiColumnName, CellSpec] = {
-    // Fold the column names in the layout into a map from column name to a generic cell
-    // specification for the column.
-    val specMap = new java.util.HashMap[KijiColumnName, CellSpec]()
-    layout
-        .getColumnNames
-        .asScala
-        .foreach { columnName: KijiColumnName =>
-          val cellSpec = layout
-              .getCellSpec(columnName)
-              .setDecoderFactory(GenericCellDecoderFactory.get())
-          if (cellSpec.isAvro) {
-            cellSpec.setUseWriterSchema()
-          }
-          specMap.put(columnName, cellSpec)
-        }
-    specMap
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/GenericRowDataConverter.scala b/src/main/scala/org/kiji/express/flow/util/GenericRowDataConverter.scala
deleted file mode 100644
index ed841b1a03c53f192d67226b3e9600b42addf92c..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/GenericRowDataConverter.scala
+++ /dev/null
@@ -1,142 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import java.util.HashMap
-import java.util.{Map => JMap}
-
-import scala.collection.JavaConverters.collectionAsScalaIterableConverter
-import scala.collection.mutable.{Map => MMap}
-
-import org.apache.hadoop.conf.Configuration
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.GenericCellDecoderFactory
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiRowData
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiURI
-import org.kiji.schema.impl.HBaseKijiRowData
-import org.kiji.schema.layout.CellSpec
-import org.kiji.schema.layout.KijiTableLayout
-import org.kiji.schema.layout.impl.CellDecoderProvider
-import org.kiji.schema.util.ResourceUtils
-
-/**
- * A factory for [[org.kiji.schema.KijiRowData]] that use the Avro GenericData API to decode
- * their data.
- *
- * @param uri to a Kiji instance for rows that will be converted.
- * @param conf is the hadoop configuration used when accessing Kiji.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-@Inheritance.Sealed
-final class GenericRowDataConverter(uri: KijiURI, conf: Configuration) {
-
-  /**
-   * The Kiji instance containing the tables whose rows will be converted.
-   */
-  private val kiji = Kiji.Factory.open(uri, conf)
-
-  /**
-   * A schema table that will be given to decoders.
-   */
-  private val schemaTable = kiji.getSchemaTable()
-
-  /**
-   * A cache of decoder providers for table layouts, that provide generic decoders that decode
-   * data without a reader schema.
-   */
-  private val decoderProviderCache = MMap[KijiTableLayout, CellDecoderProvider]()
-
-  /**
-   * Gets a row data that is a copy of the original, but configured to decode data using the Avro
-   * GenericData API.
-   *
-   * In practice this is used to enable the use of the dynamic Avro API throughout KijiExpress.
-   * The row data copy performed is shallow and thus reasonably efficient,
-   * and care should be taken to maintain this.
-   *
-   * @param original row data to reconfigure.
-   * @return a copy of the row data, configured to decode data generically.
-   */
-  def apply(original: KijiRowData): KijiRowData = {
-    // Downcast the original data so we can access its guts.
-    val hbaseRowData = original.asInstanceOf[HBaseKijiRowData]
-    // Create a provider for generic data cell decoders.
-    val decoderProvider = getDecoderProvider(hbaseRowData.getTable())
-    // Create a new original data instance configured with the new decoder.
-    new HBaseKijiRowData(hbaseRowData.getTable(), hbaseRowData.getDataRequest(),
-        hbaseRowData.getEntityId(), hbaseRowData.getHBaseResult(), decoderProvider)
-  }
-
-  /**
-   * Closes the resources associated with this converter.
-   */
-  def close() {
-    // Since we are the only ones retaining this kiji, releasing it will close it,
-    // which will in turn close the schema table.
-    ResourceUtils.releaseOrLog(kiji)
-  }
-
-  /**
-   * Retrieves a cell decoder provider for a Kiji table, that provides decoders that decode data
-   * generically and without reader schema.
-   *
-   * @param table to retrieve the cell decoder provider for.
-   * @return the cell decoder provider that decodes data generically and without reader schema.
-   */
-  private def getDecoderProvider(table: KijiTable): CellDecoderProvider = {
-    // Retrieve a provider from the cache, or generate and cache a new one for the table.
-    // Generating a new decoder provider involves creating a map of cell specifications used to
-    // "overwrite" the reader schemas in the layout to null, so that no reader schema is used
-    // when decoding data.
-    val layout = table.getLayout()
-    decoderProviderCache.get(layout).getOrElse {
-      val provider = new CellDecoderProvider(layout, schemaTable,
-          GenericCellDecoderFactory.get(), createCellSpecMap(layout))
-      decoderProviderCache.put(layout, provider)
-      provider
-    }
-  }
-
-  /**
-   * Creates a map from the names of columns in the layout into cell specifications without
-   * reader schemas.
-   *
-   * @param layout to use when generation the cell spec map.
-   * @return a map from columns in the layout to cell specifications without reader schema.
-   */
-  private def createCellSpecMap(layout: KijiTableLayout): JMap[KijiColumnName, CellSpec] = {
-    // Fold the columns in the layout into a map from columns to cell specifications without a
-    // reader schema.
-    layout
-        .getColumnNames()
-        .asScala
-        .foldLeft(new HashMap[KijiColumnName, CellSpec]()) { (specMap, columnName) =>
-          specMap.put(columnName, layout.getCellSpec(columnName).setUseWriterSchema())
-          specMap
-        }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/Memoize.scala b/src/main/scala/org/kiji/express/flow/util/Memoize.scala
deleted file mode 100644
index 822658d3cfdac74d0636b8897dd3c1a3dd04a32f..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/Memoize.scala
+++ /dev/null
@@ -1,103 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import scala.collection.mutable
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-
-/**
- * Class used to wrap a function with a single parameter, caching parameter/return value
- * combinations. The wrapped function will only execute when encountering new parameters. This
- * should only be used to wrap pure functions.
- *
- * For example, lets define a function that has a potentially long execution time and memoize it:
- * {{{
- * // A slow function:
- * def factorial(n: Int): Int = {
- *   if (n == 0) {
- *     1
- *   } else {
- *     n * factorial(n - 1)
- *   }
- * }
- *
- * // This will be very fast for each cached input.
- * val cachedFactorial = Memoize(factorial)
- *
- * // This will take some time to run
- * cachedFactorial(100)
- * // This will be very fast now that the factorial of 100 has been cached already.
- * cachedFactorial(100)
- * }}}
- *
- * Note: Functions with multiple parameters can also be used with Memoize by first converting them
- * to a single parameter function where the parameter to the function is a tuple containing the
- * original parameters. This can be achieved with the scala standard library's `tupled` method on
- * functions.
- *
- * @param f is the function to wrap.
- * @tparam T is the input type of the wrapped function.
- * @tparam R is the return type of the wrapped function.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-class Memoize[-T, +R](f: T => R) extends (T => R) {
-  /** Mutable map used to store already computed parameter/return value combinations. */
-  private[this] val cache = mutable.Map.empty[T, R]
-
-  /**
-   * Fetches the return value associated with the provided parameter. This function will return a
-   * cached value first if possible.
-   *
-   * @param x is the parameter to the function.
-   * @return the value associated with the provided parameter.
-   */
-  def apply(x: T): R = {
-    // Check to see if 'x' is already in the cache.
-    if (cache.contains(x)) {
-      // Return the cached value.
-      cache(x)
-    } else {
-      // Compute the return value associated with 'x', add it to the cache, and return it.
-      val y = f(x)
-      cache += ((x, y))
-      y
-    }
-  }
-}
-
-/**
- * Companion object for memoization wrapper that contains factory methods.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-object Memoize {
-  /**
-   * Wraps a function with Memoize.
-   *
-   * @param f is a function to wrap.
-   * @return the wrapped function that will cache parameter/return value pairs.
-   */
-  def apply[T, R](f: T => R): Memoize[T, R] = new Memoize(f)
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/PipeConversions.scala b/src/main/scala/org/kiji/express/flow/util/PipeConversions.scala
deleted file mode 100644
index aeb6e3b21c8b482fad780413f9db251ff8be0d82..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/PipeConversions.scala
+++ /dev/null
@@ -1,81 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import cascading.flow.FlowDef
-import cascading.pipe.Pipe
-import com.twitter.scalding.Mode
-import com.twitter.scalding.RichPipe
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.KijiPipe
-import org.kiji.express.flow.KijiSource
-
-/**
- * PipeConversions contains implicit conversions necessary for KijiExpress that are not included in
- * Scalding's `Job`.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] trait PipeConversions {
-  /**
-   * Converts a Cascading Pipe to a KijiExpress KijiPipe. This method permits implicit conversions
-   * from Pipe to KijiPipe.
-   *
-   * @param pipe to convert to a KijiPipe.
-   * @return a KijiPipe wrapping the specified Pipe.
-   */
-  implicit def pipe2KijiPipe(pipe: Pipe): KijiPipe = new KijiPipe(pipe)
-
-  /**
-   * Converts a [[org.kiji.express.flow.KijiPipe]] to a [[cascading.pipe.Pipe]].  This
-   * method permits implicit conversion from KijiPipe to Pipe.
-   *
-   * @param kijiPipe to convert to [[cascading.pipe.Pipe]].
-   * @return Pipe instance wrapped by the input KijiPipe.
-   */
-  implicit def kijiPipe2Pipe(kijiPipe: KijiPipe): Pipe = kijiPipe.pipe
-
-  /**
-   * Converts a [[org.kiji.express.flow.KijiPipe]] to a [[com.twitter.scalding.RichPipe]].  This
-   * method permits implicit conversion from KijiPipe to RichPipe.
-   * @param kijiPipe to convert to [[com.twitter.scalding.RichPipe]].
-   * @return RichPipe instance of Pipe wrapped by input KijiPipe.
-   */
-  implicit def kijiPipe2RichPipe(kijiPipe: KijiPipe): RichPipe = new RichPipe(kijiPipe.pipe)
-
-  /**
-   * Converts a KijiSource to a KijiExpress KijiPipe. This method permits implicit conversions
-   * from Source to KijiPipe.
-   *
-   * We expect flowDef and mode implicits to be in scope.  This should be true in the context of a
-   * Job, KijiJob, or inside the ShellRunner.
-   *
-   * @param source to convert to a KijiPipe
-   * @return a KijiPipe read from the specified source.
-   */
-  implicit def source2RichPipe(
-      source: KijiSource)(
-      implicit flowDef: FlowDef,
-      mode: Mode): KijiPipe = new KijiPipe(source.read(flowDef, mode))
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/Resources.scala b/src/main/scala/org/kiji/express/flow/util/Resources.scala
deleted file mode 100644
index ace51751db6e25ebb017b12cd98d4acc2bdcad10..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/Resources.scala
+++ /dev/null
@@ -1,257 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import scala.io.Source
-
-import org.apache.hadoop.conf.Configuration
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.util.ReferenceCountable
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiTableReader
-import org.kiji.schema.KijiTableWriter
-import org.kiji.schema.KijiURI
-
-/**
- * The Resources object contains various convenience functions while dealing with
- * resources such as Kiji instances or Kiji tables, particularly around releasing
- * or closing them and handling exceptions.
- */
-@ApiAudience.Public
-@ApiStability.Experimental
-object Resources {
-  /**
-   * Exception that contains multiple exceptions. Typically used in the case where
-   * the user gets an exception within their function and further gets an exception
-   * during cleanup in finally.
-   *
-   * @param msg is the message to include with the exception.
-   * @param errors causing this exception.
-   */
-  @Inheritance.Sealed
-  final case class CompoundException(msg: String, errors: Seq[Exception]) extends Exception
-
-  /**
-   * Performs an operation with a resource that requires post processing. This method will throw a
-   * [[org.kiji.express.util.Resources.CompoundException]] when exceptions get thrown
-   * during the operation and while resources are being closed.
-   *
-   * @tparam T is the return type of the operation.
-   * @tparam R is the type of resource such as a Kiji instance or table.
-   * @param resource required by the operation.
-   * @param after is a function for any post processing on the resource, such as close or release.
-   * @param fn is the operation to perform using the resource, like getting the layout of a table.
-   * @return the result of the operation.
-   * @throws CompoundException if your function crashes as well as the close operation.
-   */
-  def doAnd[T, R](resource: => R, after: R => Unit)(fn: R => T): T = {
-    var error: Option[Exception] = None
-
-    // Build the resource.
-    val res: R = resource
-    try {
-      // Perform the operation.
-      fn(res)
-    } catch {
-      // Store the exception in case close fails.
-      case e: Exception => {
-        error = Some(e)
-        throw e
-      }
-    } finally {
-      try {
-        // Cleanup resources.
-        after(res)
-      } catch {
-        // Throw the exception(s).
-        case e: Exception => {
-          error match {
-            case Some(firstErr) => throw CompoundException("Exception was thrown while cleaning up "
-                + "resources after another exception was thrown.", Seq(firstErr, e))
-            case None => throw e
-          }
-        }
-      }
-    }
-  }
-
-  /**
-   * Performs an operation with a releaseable resource by first retaining the resource and releasing
-   * it upon completion of the operation.
-   *
-   * @tparam T is the return type of the operation.
-   * @tparam R is the type of resource, such as a Kiji table or instance.
-   * @param resource is the retainable resource object used by the operation.
-   * @param fn is the operation to perform using the releasable resource.
-   * @return the result of the operation.
-   */
-  def retainAnd[T, R <: ReferenceCountable[R]](
-      resource: => ReferenceCountable[R])(fn: R => T): T = {
-    doAndRelease[T, R](resource.retain())(fn)
-  }
-
-  /**
-   * Performs an operation with an already retained releaseable resource releasing it upon
-   * completion of the operation.
-   *
-   * @tparam T is the return type of the operation.
-   * @tparam R is the type of resource, such as a Kiji table or instance.
-   * @param resource is the retainable resource object used by the operation.
-   * @param fn is the operation to perform using the resource.
-   * @return the result of the operation.
-   */
-  def doAndRelease[T, R <: ReferenceCountable[R]](resource: => R)(fn: R => T): T = {
-    def after(r: R) { r.release() }
-    doAnd(resource, after)(fn)
-  }
-
-  /**
-   * Performs an operation with a closeable resource closing it upon completion of the operation.
-   *
-   * @tparam T is the return type of the operation.
-   * @tparam C is the type of resource, such as a Kiji table or instance.
-   * @param resource is the closeable resource used by the operation.
-   * @param fn is the operation to perform using the resource.
-   * @return the result of the operation.
-   */
-  def doAndClose[T, C <: { def close(): Unit }](resource: => C)(fn: C => T): T = {
-    def after(c: C) { c.close() }
-    doAnd(resource, after)(fn)
-  }
-
-  /**
-   * Performs an operation that uses a Kiji instance.
-   *
-   * @tparam T is the return type of the operation.
-   * @param uri of the Kiji instance to open.
-   * @param configuration identifying the cluster running Kiji.
-   * @param fn is the operation to perform.
-   * @return the result of the operation.
-   */
-  def withKiji[T](uri: KijiURI, configuration: Configuration)(fn: Kiji => T): T = {
-    doAndRelease(Kiji.Factory.open(uri, configuration))(fn)
-  }
-
-  /**
-   * Performs an operation that uses a Kiji table.
-   *
-   * @tparam T is the return type of the operation.
-   * @param kiji instance the desired table belongs to.
-   * @param table name of the Kiji table to open.
-   * @param fn is the operation to perform.
-   * @return the result of the operation.
-   */
-  def withKijiTable[T](kiji: Kiji, table: String)(fn: KijiTable => T): T = {
-    doAndRelease(kiji.openTable(table))(fn)
-  }
-
-  /**
-   * Performs an operation that uses a Kiji table.
-   *
-   * @tparam T is the return type of the operation.
-   * @param tableUri addressing the Kiji table to open.
-   * @param configuration identifying the cluster running Kiji.
-   * @param fn is the operation to perform.
-   * @return the result of the operation.
-   */
-  def withKijiTable[T](tableUri: KijiURI, configuration: Configuration)(fn: KijiTable => T): T = {
-    withKiji(tableUri, configuration) { kiji: Kiji =>
-      withKijiTable(kiji, tableUri.getTable)(fn)
-    }
-  }
-
-  /**
-   * Performs an operation that uses a Kiji table writer.
-   *
-   * @tparam T is the return type of the operation.
-   * @param table the desired Kiji table writer belongs to.
-   * @param fn is the operation to perform.
-   * @return the result of the operation.
-   */
-  def withKijiTableWriter[T](table: KijiTable)(fn: KijiTableWriter => T): T = {
-    doAndClose(table.openTableWriter)(fn)
-  }
-
-  /**
-   * Performs an operation that uses a Kiji table writer.
-   *
-   * @tparam T is the return type of the operation.
-   * @param tableUri addressing the Kiji table to open.
-   * @param configuration identifying the cluster running Kiji.
-   * @param fn is the operation to perform.
-   * @return the result of the operation.
-   */
-  def withKijiTableWriter[T](
-      tableUri: KijiURI,
-      configuration: Configuration)(fn: KijiTableWriter => T): T = {
-    withKijiTable(tableUri, configuration) { table: KijiTable =>
-      withKijiTableWriter(table)(fn)
-    }
-  }
-
-  /**
-   * Performs an operation that uses a Kiji table reader.
-   *
-   * @tparam T is the return type of the operation.
-   * @param table the desired Kiji table reader belongs to.
-   * @param fn is the operation to perform.
-   * @return the result of the operation.
-   */
-  def withKijiTableReader[T](table: KijiTable)(fn: KijiTableReader => T): T = {
-    doAndClose(table.openTableReader)(fn)
-  }
-
-  /**
-   * Performs an operation that uses a Kiji table reader.
-   *
-   * @tparam T is the return type of the operation.
-   * @param tableUri addressing the Kiji table to open.
-   * @param configuration identifying the cluster running Kiji.
-   * @param fn is the operation to perform.
-   * @return the result of the operation.
-   */
-  def withKijiTableReader[T](
-      tableUri: KijiURI,
-      configuration: Configuration)(fn: KijiTableReader => T): T = {
-    withKijiTable(tableUri, configuration) { table: KijiTable =>
-      withKijiTableReader(table)(fn)
-    }
-  }
-
-  /**
-   * Reads a resource from the classpath into a string.
-   *
-   * @param path to the desired resource (of the form: "path/to/your/resource").
-   * @return the contents of the resource as a string.
-   */
-  def resourceAsString(path: String): String = {
-    val inputStream = getClass()
-        .getClassLoader()
-        .getResourceAsStream(path)
-
-    doAndClose(Source.fromInputStream(inputStream)) { source =>
-      source.mkString
-    }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/SpecificCellSpecs.scala b/src/main/scala/org/kiji/express/flow/util/SpecificCellSpecs.scala
deleted file mode 100644
index 843ba73b8647a7f8fe37b9177bd3e3b2a35225f9..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/SpecificCellSpecs.scala
+++ /dev/null
@@ -1,228 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import java.io.ByteArrayInputStream
-import java.io.ByteArrayOutputStream
-import java.util.HashMap
-import java.util.{Map => JMap}
-import java.util.Properties
-
-import scala.collection.JavaConverters.asScalaSetConverter
-import scala.collection.JavaConverters.mapAsJavaMapConverter
-
-import org.apache.avro.specific.SpecificRecord
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.express.flow.ColumnInputSpec
-import org.kiji.express.flow.SchemaSpec
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiTable
-import org.kiji.schema.layout.CellSpec
-import org.kiji.schema.layout.KijiTableLayout
-
-import SchemaSpec.Specific
-
-@ApiAudience.Framework
-@ApiStability.Experimental
-object SpecificCellSpecs {
-  val CELLSPEC_OVERRIDE_CONF_KEY: String = "kiji.express.input.cellspec.overrides"
-
-  /**
-   * Convert a map from field names to column input spec into a serialized map from column names to
-   * specific AvroRecord class names to use as overriding reader schemas when reading from those
-   * columns.
-   *
-   * @param columns a mapping from field name to column input spec. The column name and overriding
-   *     AvroRecord class name from each column input spec will be used to populate the output
-   *     serialized map.
-   * @return a serialized form of a map from column name to AvroRecord class name.
-   */
-  def serializeOverrides(
-      columns: Map[String, ColumnInputSpec]
-  ): String = {
-    val serializableOverrides = collectOverrides(columns)
-        .map { entry: (KijiColumnName, Class[_ <: SpecificRecord]) =>
-          val (key, value) = entry
-
-          (key.toString, value.getName)
-        }
-        .toMap
-
-    return serializeMap(serializableOverrides)
-  }
-
-  /**
-   * Deserialize an XML representation of a mapping from column name to AvroRecord class name and
-   * create a mapping from KijiColumnName to CellSpec which can be used by a KijiTableReader
-   * constructor to override reader schemas for the given columns.
-   *
-   * @param table the KijiTable from which to retrieve base CellSpecs.
-   * @param serializedMap the XML representation of the columns for which to override reader schemas
-   *     and the associated AvroRecord classes to use as reader schemas.
-   * @return a map from column name to Cellspec which can be used by a KijiTableReader constructor
-   *     to override reader schemas for the given columns.
-   */
-  def deserializeOverrides(
-      table: KijiTable,
-      serializedMap: String
-  ): Map[KijiColumnName, CellSpec] = {
-    return innerBuildCellSpecs(table.getLayout, deserializeMap(serializedMap))
-  }
-
-  /**
-   * Merge generic and specific CellSpecs favoring specifics.
-   *
-   * @param generics complete mapping from all columns to associated generic CellSpecs.
-   * @param specifics mapping of columns whose reader schemas should be overridden by specific Avro
-   *      classes as specified in the associated CellSpecs.
-   * @return a mapping from column name to CellSpec containing all mappings from specifics and all
-   *      other mappings from generics.
-   */
-  def mergeCellSpecs(
-      generics: JMap[KijiColumnName, CellSpec],
-      specifics: Map[KijiColumnName, CellSpec]
-  ): JMap[KijiColumnName, CellSpec] = {
-    // This JMap is necessary instead of generics.putAll(specifics) because generics is cached in
-    // GenericCellSpec.
-    val merged: JMap[KijiColumnName, CellSpec] = new HashMap[KijiColumnName, CellSpec]
-    merged.putAll(generics)
-    merged.putAll(specifics.asJava)
-    return merged
-  }
-
-  /**
-   * Build overridden CellSpecs for a given set of ColumnInputSpec.
-   *
-   * @param layout is the layout of the KijiTable from which base CellSpecs are drawn.
-   * @param columns are the ColumnInputSpecs from which to build overriding CellSpecs.
-   * @return a mapping from KijiColumnName to overriding CellSpec for that column.
-   */
-  def buildCellSpecs(
-      layout: KijiTableLayout,
-      columns: Map[String, ColumnInputSpec]
-  ): Map[KijiColumnName, CellSpec] = {
-    return innerBuildCellSpecs(layout, collectOverrides(columns))
-  }
-
-  /**
-   * Collects specific AvroRecord classes to use as overriding reader schemas. Input map keys are
-   * ignored.  Output map keys will be column names retrieved from the ColumnInputSpec and output
-   * map values will be AvroRecord classes to use as overriding reader schemas for associated
-   * columns.
-   *
-   * @param columns a mapping from field name to ColumnInputSpec.
-   * @return a mapping from column name to the AvroRecord class to use as the reader schema when
-   *     reading values from that column.
-   */
-  private def collectOverrides(
-      columns: Map[String, ColumnInputSpec]
-  ): Map[KijiColumnName, Class[_ <: SpecificRecord]] = {
-    columns.values
-        // Need only those columns that have specific Avro classes defined
-        .map { col => (col.columnName, col.schemaSpec) }
-        .collect { case (name, Specific(klass)) => (name, klass) }
-        .toMap
-  }
-
-  /**
-   * Serialize a map from column name to AvroRecord class name into an XML string for storage in the
-   * job configuration.
-   *
-   * @param mapToSerialize the map from column name to AvroRecord class name.
-   * @return a serialized version of these reader schema overrides.
-   */
-  private def serializeMap(
-      mapToSerialize: Map[String, String]
-  ): String = {
-    val props: Properties = new Properties()
-    // Add all map entries to the props.
-    mapToSerialize
-        .foreach {
-          case (column: String, avroClass: String) => props.setProperty(column, avroClass)
-        }
-    // Write the properties to an XML string.
-    val outputStream: ByteArrayOutputStream = new ByteArrayOutputStream()
-    props.storeToXML(
-        outputStream,
-        "These properties represent specific AvroRecord reader schema overrides. "
-        + "Keys are columns, values are specific AvroRecord classes.",
-        "UTF-8")
-    return outputStream.toString("UTF-8")
-  }
-
-
-  /**
-   * Deserializes an XML representation of a mapping from columns to AvroRecord classes.
-   *
-   * @param serializedMap the XML representation of the map.
-   * @return a mapping from KijiColumnName to SpecificRecord class.
-   */
-  private def deserializeMap(
-      serializedMap: String
-  ): Map[KijiColumnName, Class[_ <: SpecificRecord]] = {
-    // Load the properties from the serialized xml string.
-    val props: Properties = new Properties()
-    props.loadFromXML(new ByteArrayInputStream(serializedMap.getBytes))
-
-    return props.stringPropertyNames().asScala
-        .map {
-          case (column: String) => {
-            val kcn: KijiColumnName = new KijiColumnName(column)
-            val avroClass: Class[_ <: SpecificRecord] = avroClassForName(props.getProperty(column))
-            (kcn, avroClass)
-          }
-        }
-        .toMap
-  }
-
-  /**
-   * Constructs CellSpecs from a KijiTableLayout and a collection of reader schema overrides.
-   *
-   * @param layout the table layout from which to retrieve base CellSpecs.
-   * @param overrides a mapping from column to overriding reader schema.
-   * @return a mapping from column name to CellSpec which can be used in a KijiTableReader
-   *     constructor to override reader schemas.
-   */
-  private def innerBuildCellSpecs(
-      layout: KijiTableLayout,
-      overrides: Map[KijiColumnName, Class[_ <: SpecificRecord]]
-  ): Map[KijiColumnName, CellSpec] = {
-    return overrides
-        .map { entry: (KijiColumnName, Class[_ <: SpecificRecord]) =>
-          val (column, avroClass) = entry
-
-          (column, layout.getCellSpec(column).setSpecificRecord(avroClass))
-        }
-  }
-
-  /**
-   * Gets the AvroRecord Class for a given classname.
-   *
-   * @param className the name of the Class to retrieve.
-   * @return the AvroRecord Class for the given name.
-   */
-  private def avroClassForName(
-      className: String
-  ): Class[_ <: SpecificRecord] = {
-    return Class.forName(className).asSubclass(classOf[SpecificRecord])
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/Tuples.scala b/src/main/scala/org/kiji/express/flow/util/Tuples.scala
deleted file mode 100644
index 20c70c2c5546fd4edc0fd4dac6a8675eb7067469..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/Tuples.scala
+++ /dev/null
@@ -1,189 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import scala.collection.JavaConverters.asScalaIteratorConverter
-
-import cascading.tuple.Fields
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-
-/**
- * The Tuples object contains various convenience functions for dealing with scala and
- * Cascading/Scalding tuples.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-object Tuples {
-  /**
-   * Converts a Cascading fields object into a sequence of the fields it contains. This only
-   * supports fields without a special [[cascading.tuple.Fields.Kind]].
-   *
-   * @param fields to convert.
-   * @return a sequence of field names.
-   */
-  def fieldsToSeq(fields: Fields): Seq[String] = {
-    fields
-        .iterator()
-        .asScala
-        .map { field => field.toString }
-        .toSeq
-  }
-
-  /**
-   * Converts a tuple into an appropriate representation for processing by a model phase function.
-   * Handles instances of Tuple1 as special cases and unpacks them to permit functions with only one
-   * parameter to be defined without expecting their argument to be wrapped in a Tuple1 instance.
-   *
-   * @tparam T is the type of the output function argument.
-   * @param tuple to convert.
-   * @return an argument ready to be passed to a model phase function.
-   */
-  def tupleToFnArg[T](tuple: Product): T = {
-    tuple match {
-      case Tuple1(x1) => x1.asInstanceOf[T]
-      case other => other.asInstanceOf[T]
-    }
-  }
-
-  /**
-   * Converts a function return value into a tuple. Handles the case where the provided result is
-   * not a tuple by wrapping it in a Tuple1 instance.
-   *
-   * @param result from a model phase function.
-   * @return a processed tuple.
-   */
-  def fnResultToTuple(result: Any): Product = {
-    result match {
-      case tuple: Tuple1[_] => tuple
-      case tuple: Tuple2[_, _] => tuple
-      case tuple: Tuple3[_, _, _] => tuple
-      case tuple: Tuple4[_, _, _, _] => tuple
-      case tuple: Tuple5[_, _, _, _, _] => tuple
-      case tuple: Tuple6[_, _, _, _, _, _] => tuple
-      case tuple: Tuple7[_, _, _, _, _, _, _] => tuple
-      case tuple: Tuple8[_, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple9[_, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple10[_, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple11[_, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple12[_, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple13[_, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple14[_, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple15[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple16[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple17[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple18[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple19[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple20[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple21[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case tuple: Tuple22[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _] => tuple
-      case other => Tuple1(other)
-    }
-  }
-
-  /**
-   * Converts a sequence to a tuple.
-   *
-   * @tparam T is the type of the output tuple.
-   * @param sequence to convert.
-   * @return a tuple converted from the provided sequence.
-   */
-  def seqToTuple[T <: Product](sequence: Seq[_]): T = {
-    val tuple = sequence match {
-      case Seq(x1) => {
-        Tuple1(x1)
-      }
-      case Seq(x1, x2) => {
-        Tuple2(x1, x2)
-      }
-      case Seq(x1, x2, x3) => {
-        Tuple3(x1, x2, x3)
-      }
-      case Seq(x1, x2, x3, x4) => {
-        Tuple4(x1, x2, x3, x4)
-      }
-      case Seq(x1, x2, x3, x4, x5) => {
-        Tuple5(x1, x2, x3, x4, x5)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6) => {
-        Tuple6(x1, x2, x3, x4, x5, x6)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7) => {
-        Tuple7(x1, x2, x3, x4, x5, x6, x7)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8) => {
-        Tuple8(x1, x2, x3, x4, x5, x6, x7, x8)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9) => {
-        Tuple9(x1, x2, x3, x4, x5, x6, x7, x8, x9)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10) => {
-        Tuple10(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11) => {
-        Tuple11(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12) => {
-        Tuple12(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13) => {
-        Tuple13(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14) => {
-        Tuple14(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15) => {
-        Tuple15(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16) => {
-        Tuple16(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17) => {
-        Tuple17(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18) => {
-        Tuple18(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
-          x19) => {
-        Tuple19(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
-            x19)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
-          x19, x20) => {
-        Tuple20(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
-            x19, x20)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
-          x19, x20, x21) => {
-        Tuple21(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
-            x19, x20, x21)
-      }
-      case Seq(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
-          x19, x20, x21, x22) => {
-        Tuple22(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18,
-            x19, x20, x21, x22)
-      }
-    }
-
-    tuple.asInstanceOf[T]
-  }
-}
diff --git a/src/main/scala/org/kiji/express/flow/util/package.scala b/src/main/scala/org/kiji/express/flow/util/package.scala
deleted file mode 100644
index 0eb151eb96061dc8b2a2a03ea9216348634b5250..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/flow/util/package.scala
+++ /dev/null
@@ -1,25 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-/**
- * Package containing utility classes for the KijiExpress flow API.
- */
-package object util
diff --git a/src/main/scala/org/kiji/express/package.scala b/src/main/scala/org/kiji/express/package.scala
deleted file mode 100644
index c969e7b4d1926cb5e8c523c70bd1dc783a43f6ea..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/package.scala
+++ /dev/null
@@ -1,65 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji
-
-/**
- * KijiExpress is a domain-specific language for analyzing and modeling data stored in Kiji.
- *
- * For more information on Kiji, please visit [[http://www.kiji.org]] and the Kiji Github page at
- * [[https://github.com/kijiproject]]. KijiExpress is built atop Scalding
- * ([[https://github.com/twitter/scalding]]), a library for writing MapReduce workflows.
- *
- * === Getting started. ===
- * To use KijiExpress, import the Scalding library, the `express` library, and members of the
- * flow module [[org.kiji.express.flow]].
- * {{{
- *   import com.twitter.scalding._
- *   import org.kiji.express._
- *   import org.kiji.express.flow._
- * }}}
- * Doing so will import several classes and functions that make it easy to author analysis
- * pipelines.
- *
- * === Working with data from a Kiji table. ===
- * Scalding represents distributed data sets as a collection of tuples with named fields.
- * Likewise, KijiExpress represents a row in a Kiji table as a tuple.
- * [[org.kiji.express.flow]] provides a factory, named `KijiInput`,
- * that make it easy to specify what columns you want to read from a Kiji table and what names
- * they should have in row tuples. For example, to read the value of a column named `info:text`
- * from a table named `postings` into the tuple field named `text`, you could write the following.
- *
- * {{{
- *   KijiInput("kiji://.env/default/postings", "info:text" -> "text")
- * }}}
- *
- * The result of the above expression is a [[org.kiji.express.flow.KijiSource]] (an implementation
- * of Scalding's `Source`) which represents the rows of the Kiji table as a collection of tuples.
- * At this point, functional operators can be used to manipulate and analyze the data. For example,
- * to split the contents of the column `info:text` into individual words,
- * you could write the following.
- * {{{
- *   KijiInput("kiji://.env/default/postings")("info:text" -> 'text)
- *       .flatMap('text -> 'word) { word: Seq[FlowCell[String]] =>
- *         word.head.datum.split("""\s+""")
- *       }
- * }}}
- */
-package object express
-
diff --git a/src/main/scala/org/kiji/express/repl/ExpressILoop.scala b/src/main/scala/org/kiji/express/repl/ExpressILoop.scala
deleted file mode 100644
index c84628b7f3ae2a4458544fc429ae25498873a74a..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/repl/ExpressILoop.scala
+++ /dev/null
@@ -1,107 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.repl
-
-import scala.tools.nsc.interpreter.ILoop
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.schema.shell.ShellMain
-
-/**
- * A class providing KijiExpress specific commands for inclusion in the KijiExpress REPL.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] class ExpressILoop extends ILoop {
-  /**
-   * Commands specific to the KijiExpress REPL. To define a new command use one of the following
-   * factory methods:
-   * - `LoopCommand.nullary` for commands that take no arguments
-   * - `LoopCommand.cmd` for commands that take one string argument
-   * - `LoopCommand.varargs` for commands that take multiple string arguments
-   */
-  private val expressCommands: List[LoopCommand] = List(
-      LoopCommand.varargs("schema-shell",
-          "",
-          "Runs the KijiSchema shell.",
-          schemaShellCommand)
-  )
-
-  /**
-   * Change the shell prompt to read express&gt;
-   *
-   * @return a prompt string to use for this REPL.
-   */
-  override def prompt: String = "\nexpress> "
-
-  /**
-   * Gets the list of commands that this REPL supports.
-   *
-   * @return a list of the command supported by this REPL.
-   */
-  override def commands: List[LoopCommand] = super.commands ++ expressCommands
-
-  /**
-   * Determines whether the kiji-schema-shell jar is on the classpath.
-   *
-   * @return `true` if kiji-schema-shell is on the classpath, `false` otherwise.
-   */
-  private def isSchemaShellEnabled: Boolean = {
-    try {
-      ShellMain.version()
-      true
-    } catch {
-      case _: NoClassDefFoundError => false
-    }
-  }
-
-  /**
-   * Runs an instance of the KijiSchema Shell within the Scala REPL.
-   *
-   * @param args that should be passed to the instance of KijiSchema Shell to run.
-   * @return the result of running the command, which should always be the default result.
-   */
-  private def schemaShellCommand(args: List[String]): Result = {
-    if (isSchemaShellEnabled) {
-      try {
-        // Create a shell runner and use it to run an instance of the REPL with no arguments.
-        val shellMain = new ShellMain()
-        // Run the shell.
-        val exitCode = shellMain.run()
-        if (exitCode == 0) {
-          Result.resultFromString("KijiSchema Shell exited with success.")
-        } else {
-          Result.resultFromString("KijiSchema Shell exited with code: " + exitCode)
-        }
-      } finally {
-        // Close all connections properly before exiting.
-        ShellMain.shellKijiSystem.shutdown()
-      }
-    } else {
-      Result.resultFromString("The KijiSchema Shell jar is not on the classpath. "
-          + "Set the environment variable SCHEMA_SHELL_HOME to the root of a KijiSchema Shell "
-          + "distribution before running the KijiExpress Shell to enable KijiSchema Shell "
-          + "features.")
-    }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/repl/ExpressShell.scala b/src/main/scala/org/kiji/express/repl/ExpressShell.scala
deleted file mode 100644
index 645bd3c4bb96b41170b506068d4aef15cbb0d05c..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/repl/ExpressShell.scala
+++ /dev/null
@@ -1,147 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.repl
-
-import java.io.File
-import java.io.FileOutputStream
-import java.util.jar.JarEntry
-import java.util.jar.JarOutputStream
-
-import scala.tools.nsc.GenericRunnerCommand
-import scala.tools.nsc.MainGenericRunner
-import scala.tools.nsc.interpreter.ILoop
-import scala.tools.nsc.io.VirtualDirectory
-
-import com.google.common.io.Files
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.annotations.Inheritance
-import org.kiji.express.flow.util.Resources._
-
-/**
- * A runner for a Scala REPL providing functionality extensions specific to working with
- * KijiExpress.
- */
-@ApiAudience.Private
-@ApiStability.Experimental
-@Inheritance.Sealed
-private[express] object ExpressShell extends MainGenericRunner {
-
-  /**
-   * An instance of the Scala REPL the user will interact with.
-   */
-  private[express] var expressREPL: Option[ILoop] = None
-
-  /**
-   * A comma-separated list of paths to jars to add to the distributed cache of run jobs,
-   * provided as a system property through the command line. Framework developers should use this
-   * value when determining jars to ship with jobs.
-   */
-  private[express] val tmpjars: Option[String] = Option(System.getProperty("tmpjars"))
-
-  /**
-   * The main entry point for executing the REPL.
-   *
-   * This method is lifted from [[scala.tools.nsc.MainGenericRunner]] and modified to allow
-   * for custom functionality, including determining at runtime if the REPL is running,
-   * and making custom REPL colon-commands available to the user.
-   *
-   * @param args passed from the command line.
-   * @return `true` if execution was successful, `false` otherwise.
-   */
-  override def process(args: Array[String]): Boolean = {
-    // Process command line arguments into a settings object, and use that to start the REPL.
-    val command = new GenericRunnerCommand(args.toList, (x: String) => errorFn(x))
-    import command.settings
-    expressREPL = Some(new ExpressILoop)
-    expressREPL.get.process(settings)
-  }
-
-  /**
-   * Runs an instance of the shell.
-   *
-   * @param args from the command line.
-   */
-  def main(args: Array[String]) {
-    val retVal = process(args)
-    if (!retVal) {
-      sys.exit(1)
-    }
-  }
-
-  /**
-   * Creates a jar file in a temporary directory containing the code thus far compiled by the REPL.
-   *
-   * @return some file for the jar created, or `None` if the REPL is not running.
-   */
-  private[express] def createReplCodeJar(): Option[File] = {
-    expressREPL.map { repl =>
-      val virtualDirectory = repl.virtualDirectory
-      val tempJar = new File(Files.createTempDir(),
-        "scalding-repl-session-" + System.currentTimeMillis() + ".jar")
-      createJar(virtualDirectory, tempJar)
-    }
-  }
-
-  /**
-   * Creates a jar file from the classes contained in a virtual directory.
-   *
-   * @param virtualDirectory containing classes that should be added to the jar.
-   * @param jarFile that will be written.
-   * @return the jarFile specified and written.
-   */
-  private[express] def createJar(virtualDirectory: VirtualDirectory, jarFile: File): File = {
-    doAndClose(new JarOutputStream(new FileOutputStream(jarFile))) { jarStream =>
-      addVirtualDirectoryToJar(virtualDirectory, "", jarStream)
-    }
-    jarFile
-  }
-
-  /**
-   * Add the contents of the specified virtual directory to a jar. This method will recursively
-   * descend into subdirectories to add their contents.
-   *
-   * @param dir is a virtual directory whose contents should be added.
-   * @param entryPath for classes found in the virtual directory.
-   * @param jarStream for writing the jar file.
-   */
-  private def addVirtualDirectoryToJar(
-      dir: VirtualDirectory,
-      entryPath: String,
-      jarStream: JarOutputStream) {
-    dir.foreach { file =>
-      if (file.isDirectory) {
-        // Recursively descend into subdirectories, adjusting the package name as we do.
-        val dirPath = entryPath + file.name + "/"
-        val entry: JarEntry = new JarEntry(dirPath)
-        jarStream.putNextEntry(entry)
-        jarStream.closeEntry()
-        addVirtualDirectoryToJar(file.asInstanceOf[VirtualDirectory], dirPath, jarStream)
-      } else if (file.hasExtension("class")) {
-        // Add class files as an entry in the jar file and write the class to the jar.
-        val entry: JarEntry = new JarEntry(entryPath + file.name)
-        jarStream.putNextEntry(entry)
-        jarStream.write(file.toByteArray)
-        jarStream.closeEntry()
-      }
-    }
-  }
-}
diff --git a/src/main/scala/org/kiji/express/repl/Implicits.scala b/src/main/scala/org/kiji/express/repl/Implicits.scala
deleted file mode 100644
index d7a352df5a05b69639b3315f056ed27b6f40b202..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/repl/Implicits.scala
+++ /dev/null
@@ -1,149 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.repl
-
-import cascading.flow.FlowDef
-import cascading.pipe.Pipe
-import com.twitter.scalding.FieldConversions
-import com.twitter.scalding.IterableSource
-import com.twitter.scalding.RichPipe
-import com.twitter.scalding.Source
-import com.twitter.scalding.TupleConversions
-import com.twitter.scalding.TupleConverter
-import com.twitter.scalding.TupleSetter
-
-import org.kiji.annotations.ApiAudience
-import org.kiji.annotations.ApiStability
-import org.kiji.express.flow.util.AvroTupleConversions
-import org.kiji.express.flow.util.PipeConversions
-
-/**
- * Object containing various implicit conversions required to create Scalding flows in the REPL.
- * Most of these conversions come from Scalding's Job class.
- */
-@ApiAudience.Framework
-@ApiStability.Experimental
-object Implicits
-    extends PipeConversions
-    with TupleConversions
-    with FieldConversions
-    with AvroTupleConversions {
-  /** Implicit flowDef for this KijiExpress shell session. */
-  implicit var flowDef: FlowDef = getEmptyFlowDef
-
-  /**
-   * Sets the flow definition in implicit scope to an empty flow definition.
-   */
-  def resetFlowDef() {
-    flowDef = getEmptyFlowDef
-  }
-
-  /**
-   * Gets a new, empty, flow definition.
-   *
-   * @return a new, empty flow definition.
-   */
-  private[express] def getEmptyFlowDef: FlowDef = {
-    val fd = new FlowDef
-    fd.setName("ExpressShell")
-    fd
-  }
-
-  /**
-   * Converts a Cascading Pipe to a Scalding RichPipe. This method permits implicit conversions from
-   * Pipe to RichPipe.
-   *
-   * @param pipe to convert to a RichPipe.
-   * @return a RichPipe wrapping the specified Pipe.
-   */
-  implicit def pipeToRichPipe(pipe: Pipe): RichPipe = new RichPipe(pipe)
-
-  /**
-   * Converts a Scalding RichPipe to a Cascading Pipe. This method permits implicit conversions from
-   * RichPipe to Pipe.
-   *
-   * @param richPipe to convert to a Pipe.
-   * @return the Pipe wrapped by the specified RichPipe.
-   */
-  implicit def richPipeToPipe(richPipe: RichPipe): Pipe = richPipe.pipe
-
-  /**
-   * Converts a Source to a RichPipe. This method permits implicit conversions from Source to
-   * RichPipe.
-   *
-   * @param source to convert to a RichPipe.
-   * @return a RichPipe wrapping the result of reading the specified Source.
-   */
-  implicit def sourceToRichPipe(source: Source): RichPipe = RichPipe(source.read)
-
-  /**
-   * Converts a Source to a Pipe. This method permits implicit conversions from Source to Pipe.
-   *
-   * @param source to convert to a Pipe.
-   * @return a Pipe that is the result of reading the specified Source.
-   */
-  implicit def sourceToPipe(source: Source): Pipe = source.read
-
-  /**
-   * Converts an iterable into a Source with index (int-based) fields.
-   *
-   * @param iterable to convert into a Source.
-   * @param setter implicitly retrieved and used to convert the specified iterable into a Source.
-   * @param converter implicitly retrieved and used to convert the specified iterable into a Source.
-   * @return a Source backed by the specified iterable.
-   */
-  implicit def iterableToSource[T](
-      iterable: Iterable[T])
-      (implicit setter: TupleSetter[T],
-          converter: TupleConverter[T]): Source = {
-    IterableSource[T](iterable)(setter, converter)
-  }
-
-  /**
-   * Converts an iterable into a Pipe with index (int-based) fields.
-   *
-   * @param iterable to convert into a Pipe.
-   * @param setter implicitly retrieved and used to convert the specified iterable into a Pipe.
-   * @param converter implicitly retrieved and used to convert the specified iterable into a Pipe.
-   * @return a Pipe backed by the specified iterable.
-   */
-  implicit def iterableToPipe[T](
-      iterable: Iterable[T])
-      (implicit setter: TupleSetter[T],
-          converter: TupleConverter[T]): Pipe = {
-    iterableToSource(iterable)(setter, converter).read
-  }
-
-  /**
-   * Converts an iterable into a RichPipe with index (int-based) fields.
-   *
-   * @param iterable to convert into a RichPipe.
-   * @param setter implicitly retrieved and used to convert the specified iterable into a RichPipe.
-   * @param converter implicitly retrieved and used to convert the specified iterable into a
-   *     RichPipe.
-   * @return a RichPipe backed by the specified iterable.
-   */
-  implicit def iterableToRichPipe[T](
-      iterable: Iterable[T])
-      (implicit setter: TupleSetter[T],
-          converter: TupleConverter[T]): RichPipe = {
-    RichPipe(iterableToPipe(iterable)(setter, converter))
-  }
-}
diff --git a/src/main/scala/org/kiji/express/repl/package.scala b/src/main/scala/org/kiji/express/repl/package.scala
deleted file mode 100644
index b184a200a37951b7c8dbc61cccc716851ac170ec..0000000000000000000000000000000000000000
--- a/src/main/scala/org/kiji/express/repl/package.scala
+++ /dev/null
@@ -1,31 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express
-
-/**
- * Package containing classes/objects supporting the KijiExpress shell. Includes classes required
- * for integrating with the scala REPL. To launch the KijiExpress shell, run the following express
- * command:
- *
- * {{{
- *   express shell
- * }}}
- */
-package object repl
diff --git a/src/main/scalastyle/scalastyle_config.xml b/src/main/scalastyle/scalastyle_config.xml
deleted file mode 100644
index 3079d60919f85a0f24b44fc6db302a8ae16c00a3..0000000000000000000000000000000000000000
--- a/src/main/scalastyle/scalastyle_config.xml
+++ /dev/null
@@ -1,73 +0,0 @@
-<!--
-    (c) Copyright 2013 WibiData, Inc.
-
-    See the NOTICE file distributed with this work for additional
-    information regarding copyright ownership.
-
-    Licensed under the Apache License, Version 2.0 (the "License");
-    you may not use this file except in compliance with the License.
-    You may obtain a copy of the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
--->
-<scalastyle>
-  <name>Kiji scalastyle configuration</name>
-  <check enabled="true" class="org.scalastyle.file.FileLineLengthChecker" level="error">
-    <parameters>
-      <parameter name="maxLineLength"><![CDATA[100]]></parameter>
-      <parameter name="tabSize"><![CDATA[2]]></parameter>
-    </parameters>
-  </check>
-  <check enabled="true" class="org.scalastyle.file.FileTabChecker" level="error"></check>
-  <check enabled="true" class="org.scalastyle.file.HeaderMatchesChecker" level="error">
-    <parameters>
-      <parameter name="header"><![CDATA[/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */]]>
-      </parameter>
-    </parameters>
-  </check>
-  <check enabled="true" class="org.scalastyle.file.NewLineAtEofChecker" level="error"></check>
-  <check enabled="true" class="org.scalastyle.file.WhitespaceEndOfLineChecker" level="error"></check>
-  <check enabled="true" class="org.scalastyle.scalariform.ClassNamesChecker" level="error">
-    <parameters>
-      <parameter name="regex"><![CDATA[^[A-Z][A-Za-z0-9]*$]]></parameter>
-    </parameters>
-  </check>
-  <check enabled="true" class="org.scalastyle.scalariform.ObjectNamesChecker" level="error">
-    <parameters>
-      <parameter name="regex"><![CDATA[^[A-Z][A-Za-z0-9]*$]]></parameter>
-    </parameters>
-  </check>
-  <check enabled="true" class="org.scalastyle.scalariform.PackageObjectNamesChecker" level="error">
-    <parameters>
-      <parameter name="regex"><![CDATA[^[a-z][A-Za-z0-9]*$]]></parameter>
-    </parameters>
-  </check>
-  <check enabled="true" class="org.scalastyle.scalariform.IfBraceChecker" level="error"></check>
-  <check enabled="true" class="org.scalastyle.scalariform.NoFinalizeChecker" level="error"></check>
-  <check enabled="true" class="org.scalastyle.scalariform.NoWhitespaceAfterLeftBracketChecker" level="error"></check>
-  <check enabled="true" class="org.scalastyle.scalariform.NoWhitespaceBeforeLeftBracketChecker" level="error"></check>
-  <check enabled="true" class="org.scalastyle.scalariform.PublicMethodsHaveTypeChecker" level="error"></check>
-</scalastyle>
diff --git a/src/main/scripts/express b/src/main/scripts/express
deleted file mode 100755
index 3c27bbd7885d1b1efadd71066acbe7efde001acc..0000000000000000000000000000000000000000
--- a/src/main/scripts/express
+++ /dev/null
@@ -1,581 +0,0 @@
-#!/usr/bin/env bash
-#
-#   (c) Copyright 2013 WibiData, Inc.
-#
-#   See the NOTICE file distributed with this work for additional
-#   information regarding copyright ownership.
-#
-#   Licensed under the Apache License, Version 2.0 (the "License");
-#   you may not use this file except in compliance with the License.
-#   You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-#
-#
-#   The express script provides tools for running KijiExpress scripts and interacting with the
-#   KijiExpress system.
-#   Tools are run as:
-#
-#   bash> $EXPRESS_HOME/bin/express <tool-name> [options]
-#
-#   For full usage information, use:
-#
-#   bash> $EXPRESS_HOME/bin/express help
-#
-
-# Resolve a symlink to its absolute target, like how 'readlink -f' works on Linux.
-function resolve_symlink() {
-  TARGET_FILE=${1}
-
-  if [ -z "$TARGET_FILE" ]; then
-    echo ""
-    return 0
-  fi
-
-  cd $(dirname "$TARGET_FILE")
-  TARGET_FILE=$(basename "$TARGET_FILE")
-
-  # Iterate down a (possible) chain of symlinks
-  count=0
-  while [ -L "$TARGET_FILE" ]; do
-    if [ "$count" -gt 1000 ]; then
-      # Just stop here, we've hit 1,000 recursive symlinks. (cycle?)
-      break
-    fi
-
-    TARGET_FILE=$(readlink "$TARGET_FILE")
-    cd $(dirname "$TARGET_FILE")
-    TARGET_FILE=$(basename "$TARGET_FILE")
-    count=$(( $count + 1 ))
-  done
-
-  # Compute the canonicalized name by finding the physical path
-  # for the directory we're in and appending the target file.
-  PHYS_DIR=$(pwd -P)
-  RESULT="$PHYS_DIR/$TARGET_FILE"
-  echo "$RESULT"
-}
-
-prgm="$0"
-prgm=`resolve_symlink "$prgm"`
-bin=`dirname "$prgm"`
-bin=`cd "${bin}" && pwd`
-
-EXPRESS_HOME="${EXPRESS_HOME:-${bin}/../}"
-
-# Any arguments you want to pass to KijiExpress's jvm may be done via this env var.
-EXPRESS_JAVA_OPTS=${EXPRESS_JAVA_OPTS:-""}
-
-# This is a workaround for OS X Lion, where a bug in JRE 1.6
-# creates a lot of 'SCDynamicStore' errors.
-if [ "$(uname)" == "Darwin" ]; then
-  EXPRESS_JAVA_OPTS="$EXPRESS_JAVA_OPTS -Djava.security.krb5.realm= -Djava.security.krb5.kdc="
-fi
-
-# An existing set of directories to use for the java.library.path property should
-# be set with JAVA_LIBRARY_PATH.
-JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH:-""}
-
-# Try CDH defaults.
-HBASE_HOME="${HBASE_HOME:-/usr/lib/hbase}"
-HADOOP_HOME="${HADOOP_HOME:-/usr/lib/hadoop}"
-
-# First make sure we have everything we need in the environment.
-if [ -z "${EXPRESS_HOME}" -o ! -d "${EXPRESS_HOME}" ]; then
-  echo "Please set your EXPRESS_HOME environment variable."
-  exit 1
-fi
-if [ -z "${HBASE_HOME}" -o ! -d "${HBASE_HOME}" ]; then
-  echo "Please set your HBASE_HOME environment variable."
-  exit 1
-fi
-if [ -z "${HADOOP_HOME}" -o ! -d "${HADOOP_HOME}" ]; then
-  echo "Please set your HADOOP_HOME environment variable."
-  exit 1
-fi
-
-if [ -z "${1}" ]; then
-  echo "express: Tool launcher for KijiExpress."
-  echo "Run 'express help' to see a list of available tools."
-  exit 1
-fi
-
-# Removes classpath entries that match the given regexp (partial match, not full
-# match).
-function remove_classpath_entries() {
-  local cp=${1}
-  local regex=${2}
-
-  echo $cp | sed "s/[^:]*$regex[^:]*/::/g" | sed 's/::*/:/g'
-  return 0
-}
-
-# Helper to build classpaths correctly
-function append_path() {
-  if [ -z "${1}" ]; then
-    echo ${2}
-  else
-    echo ${1}:${2}
-  fi
-}
-
-# Scrubs classpaths of a given jar. Mutate will dig into *s, only mutating them
-# if it finds the given jar.
-# mutate_classpath scrubme.jar "$(hadoop classpath)"
-function mutate_classpath () {
-  local mutated_classpath
-  local jar_to_scrub=${1}
-  shift
-
-  # Stop expanding globs
-  set -f
-  IFS=: read -r -a classpath <<< ${@}
-
-  for path in $classpath; do
-    # If it ends with a glob we'll need to dig deeper for jars
-    if [ "${path: -1:1}" = "*" ]; then
-      set +f
-      local expanded_classpath=$(JARS=(${path}.jar); IFS=:; echo "${JARS[*]}")
-      set -f
-
-      # If the expanded classpath contains the jar in question, we'll
-      # scrub it later.
-      if [[ $expanded_classpath =~ .*$jar_to_scrub.* ]]; then
-        mutated_classpath=$(append_path $mutated_classpath $expanded_classpath)
-
-      # If the expanded classpath doesn't contain the jar in question, use
-      # the glob version to reduce clutter.
-      else
-        mutated_classpath=$(append_path $mutated_classpath $path)
-      fi
-    # No glob just use the path
-    else
-      mutated_classpath=$(append_path $mutated_classpath $path)
-    fi
-  done
-
-  # Scrub all instances of the jar
-  mutated_classpath=$(remove_classpath_entries "$mutated_classpath" "$jar_to_scrub")
-  echo $mutated_classpath
-
-  set +f
-}
-
-# Detect and extract the current Hadoop version number. e.g. "Hadoop 2.x-..." -> "2" You can
-# override this with $KIJI_HADOOP_DISTRO_VER (e.g. "hadoop1" or "hadoop2").
-function extract_hadoop_major_version() {
-  hadoop_major_version=$(${HADOOP_HOME}/bin/hadoop version | head -1 | cut -c 8)
-  if [ -z "${hadoop_major_version}" -a -z "${KIJI_HADOOP_DISTRO_VER}" ]; then
-    echo "Warning: Unknown Hadoop version. May not be able to load all Kiji jars."
-    echo "Set KIJI_HADOOP_DISTRO_VER to 'hadoop1' or 'hadoop2' to load these."
-  else
-    KIJI_HADOOP_DISTRO_VER=${KIJI_HADOOP_DISTRO_VER:-"hadoop${hadoop_major_version}"}
-  fi
-}
-
-# Extracts the --libjars value from the command line and appends/prepends this value to any
-# classpath variables.To ensure that --libjars doesn't get passed into the scalding_tool, this
-# function effectively rebuilds the remainder of the command line arguments by removing the
-# --libjars argument and value. The result is stored in a new global variable called ${COMMAND_ARGS}
-# which is used by other parts of the script in lieu of ${@}.
-function extract_classpath() {
-  return_args=""
-  while (( "${#}" )); do
-    if [ "${1}" == "--libjars" ]; then
-      libjars_cp=${2}
-      shift
-    else
-      return_args="${return_args} ${1}"
-    fi
-    shift
-  done
-  COMMAND_ARGS=${return_args}
-
-  # Gather the express dependency jars.
-  if [ -z "${KIJI_HOME}" -o ! -d "${KIJI_HOME}" ]; then
-   echo "Please set your KIJI_HOME environment variable."
-   exit 1
-  fi
-  if [ -z "${KIJI_MR_HOME}" -o ! -d "${KIJI_MR_HOME}" ]; then
-   echo "Please set your KIJI_MR_HOME environment variable."
-   exit 1
-  fi
-
-  # Add KijiExpress specific jars.
-  express_libjars="${EXPRESS_HOME}/lib/*"
-
-  # If SCHEMA_SHELL_HOME is set, add kiji-schema-shell jars to the classpath to enable features
-  # that require it.
-  schema_shell_libjars=""
-  if [[ -n "${SCHEMA_SHELL_HOME}" ]]; then
-    schema_shell_libjars="${SCHEMA_SHELL_HOME}/lib/*"
-  fi
-  # We may have Hadoop distribution-specific jars to load in
-  # $KIJI_HOME/lib/distribution/hadoopN, where N is the major digit of the Hadoop
-  # version. Only load at most one such set of jars.
-  extract_hadoop_major_version
-
-  # If MODELING_HOME is set, add kiji-modeling jars to the classpath to enable using the express
-  # script with kiji-modeling.  THIS IS A HACK!
-  # TODO(EXP-265): Remove this when kiji-modeling has its own tool launcher.
-  modeling_libjars=""
-  if [[ -n "${MODELING_HOME}" ]]; then
-    modeling_libjars="${MODELING_HOME}/lib/*"
-  fi
-
-  # Add KijiMR distribution specific jars.
-  if [[ -n "${KIJI_MR_HOME}" && "${KIJI_HOME}" != "${KIJI_MR_HOME}" ]]; then
-    mr_libjars="${KIJI_MR_HOME}/lib/*"
-    mr_distrodirs="${KIJI_MR_HOME}/lib/distribution/${KIJI_HADOOP_DISTRO_VER}"
-    if [ -d "${mr_distrodirs}" ]; then
-      mr_distrojars="${mr_distrodirs}/*"
-    fi
-  fi
-
-  # Add KijiSchema distribution specific jars.
-  schema_libjars="${libjars}:${KIJI_HOME}/lib/*"
-  schema_distrodir="$KIJI_HOME/lib/distribution/$KIJI_HADOOP_DISTRO_VER"
-  if [ -d "${schema_distrodir}" ]; then
-    schema_distrojars="${schema_distrodir}/*"
-  fi
-
-  # Compose everything together into a classpath.
-  libjars="${express_libjars}:${mr_distrojars}:${mr_libjars}:${schema_distrojars}:${schema_libjars}:${schema_shell_libjars}:${modeling_libjars}"
-
-  # Gather the HBase classpath.
-  hbase_cp=$(${HBASE_HOME}/bin/hbase classpath)
-  hbase_cp=$(mutate_classpath 'slf4j-log4j12' "${hbase_cp}")
-
-  # Hadoop classpath
-  hadoop_cp=$(${HADOOP_HOME}/bin/hadoop classpath)
-  hadoop_cp=$(mutate_classpath 'slf4j-log4j12' "${hadoop_cp}")
-
-  # Note that we put the libjars before the hbase jars, in case there are conflicts.
-  express_conf=${EXPRESS_HOME}/conf
-  # We put $libjars_cp at the beginning classpath to allow users to win when there are
-  # conflicts.
-  express_cp="${libjars_cp}:${express_conf}:${libjars}:${hadoop_cp}:${hbase_cp}"
-
-  # Use parts of the classpath to determine jars to send with jobs through the distributed cache.
-  tmpjars_cp="${libjars_cp}:${libjars}"
-  tmpjars=$(java -cp ${express_cp} org.kiji.express.flow.tool.TmpJarsTool ${tmpjars_cp})
-
-  # Determine location of Hadoop native libraries and set java.library.path.
-  if [ -d "${HADOOP_HOME}/lib/native" ]; then
-    JAVA_PLATFORM=`java -cp ${hadoop_cp} -Xmx32m org.apache.hadoop.util.PlatformName | sed -e "s/ /_/g"`
-    if [ -d "${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}" ]; then
-      # if $HADOOP_HOME/lib/native/$JAVA_PLATFORM exists, use native libs from there.
-      if [ ! -z "${JAVA_LIBRARY_PATH}" ]; then
-        JAVA_LIBRARY_PATH="${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}"
-      else
-        JAVA_LIBRARY_PATH="${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}"
-      fi
-    elif [ -d "${HADOOP_HOME}/lib/native" ]; then
-      # If not, check for a global $HADOOP_HOME/lib/native/ and just use that dir.
-      if [ ! -z "${JAVA_LIBRARY_PATH}" ]; then
-        JAVA_LIBRARY_PATH="${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native/"
-      else
-        JAVA_LIBRARY_PATH="${HADOOP_HOME}/lib/native/"
-      fi
-    fi
-  fi
-}
-
-function jar_usage() {
-  echo "Usage: express jar <jarFile> <mainClass> [args...]"
-  echo "       express job <jarFile> <jobClass> [args...]"
-}
-
-function jar_command() {
-  if [[ ${#} > 0 && ${1} == "--help" ]]; then
-    jar_usage
-    echo
-    exit 0
-  fi
-  user_target=${1}
-  class=${2}
-  shift 2
-  COMMAND_ARGS=${@}
-  if [ -z "${user_target}" ]; then
-    echo "Error: no jar file specified."
-    jar_usage
-    exit 1
-  fi
-  if [ ! -f "${user_target}" ]; then
-    echo "Error: cannot find jar file ${user_target}"
-    jar_usage
-    exit 1
-  fi
-  if [ -z "${class}" ]; then
-    echo "Error: no main class specified."
-    jar_usage
-    exit 1
-  fi
-  express_cp="${user_target}:${express_cp}"
-}
-
-function print_tool_usage() {
-  echo 'The express script can run programs written using KijiExpress.'
-  echo
-  echo 'USAGE'
-  echo
-  echo '  express <command> [--libjars <list of dependency jars separated by colon> <args>'
-  echo
-  echo 'COMMANDS'
-  echo
-  echo '  help          - Displays this help message. Use --verbose for more information.'
-  echo '  shell         - Starts an interactive shell for running KijiExpress code.'
-  echo '  schema-shell  - Starts KijiSchema Shell loaded with KijiExpress extensions.'
-    'removed in a future release.'
-  echo '  job           - Runs a compiled KijiExpress job.'
-  echo '  jar           - Runs an arbitrary Scala or Java program.'
-  echo '  classpath     - Prints the classpath used to run KijiExpress.'
-  echo
-}
-
-function print_env_usage() {
-  echo
-  echo "ENVIRONMENT VARIABLES"
-  echo
-  echo "  Users can set several environment variables to change the behavior of the express"
-  echo "  script."
-  echo "  These include:"
-  echo
-  echo "  EXPRESS_JAVA_OPTS   Should contain extra arguments to pass to the JVM used to run"
-  echo "                      KijiExpress. By default, EXPRESS_JAVA_OPTS is empty."
-  echo
-  echo "  JAVA_LIBRARY_PATH   Should contain a colon-separated list of paths to additional native"
-  echo "                      libraries to pass to the JVM (through the java.library.path"
-  echo "                      property). Note the express script will always pass the native"
-  echo "                      libraries included with your Hadoop distribution to the JVM. By"
-  echo "                      default JAVA_LIBRARY_PATH is empty."
-}
-
-command=${1}
-
-case ${command} in
-  help)
-    shift
-    print_tool_usage
-    if [[ ${1} == "--verbose" ]]; then
-      print_env_usage
-    fi
-    exit 0
-    ;;
-
-  classpath)
-    shift
-    extract_classpath "${@}"
-    echo "${express_cp}"
-    exit 0
-    ;;
-
-  job)
-    shift  # pop off the command
-    extract_classpath "${@}"
-    jar_command ${COMMAND_ARGS}
-    if [[ "${COMMAND_ARGS}" != *--hdfs* ]] && [[ "${COMMAND_ARGS}" != *--local* ]]; then
-      # Default run mode is local.
-      run_mode_flag="--local"
-    fi
-    scalding_tool="com.twitter.scalding.Tool"
-    ;;
-
-  jar)
-    shift  # pop off the command
-    extract_classpath "${@}"
-    jar_command ${COMMAND_ARGS}
-    ;;
-
-  schema-shell)
-    shift # pop off command
-    # Check if SCHEMA_SHELL_HOME is set. If not we cannot run the shell.
-    if [ -z "${SCHEMA_SHELL_HOME}" ]; then
-      echo "The environment variable SCHEMA_SHELL_HOME is undefined, and so KijiSchema Shell"
-      echo "cannot be run. Please set SCHEMA_SHELL_HOME to the path to a KijiSchema Shell"
-      echo "distribution and try again."
-      exit 1
-    fi
-    extract_classpath "${@}"
-    schema_shell_script="${SCHEMA_SHELL_HOME}/bin/kiji-schema-shell"
-    # We'll add express dependencies to KIJI_CLASSPATH so that they are picked up by
-    # kiji-schema-shell.
-    export KIJI_CLASSPATH="${KIJI_CLASSPATH}:${express_cp}"
-    # Pass tmpjars to kiji-schema-shell using a JVM property, which express's kiji-schema-shell
-    # module knows to read and use to populate tmpjars for launched jobs.
-    JAVA_OPTS="${JAVA_OPTS} -Dexpress.tmpjars=${tmpjars}"
-    # Also specify that schema validation should be disabled.
-    JAVA_OPTS="${JAVA_OPTS} -Dorg.kiji.schema.impl.AvroCellEncoder.SCHEMA_VALIDATION=DISABLED"
-    export JAVA_OPTS
-    # Invoke the kiji-schema-shell command with the express modules preloaded.
-    ${schema_shell_script} --modules=modeling ${COMMAND_ARGS}
-    exit $?
-    ;;
-
-  shell)
-    shift # pop off the command
-
-    # Pass a classpath to run the shell with and jars to ship with the distributed cache to
-    # the shell runner.
-    extract_classpath "${@}"
-    export EXPRESS_CP="${express_cp}"
-    export TMPJARS="${tmpjars}"
-
-    # Pass a run mode to the shell runner.
-    local_mode_script="${bin}/local-mode.scala"
-    hdfs_mode_script="${bin}/hdfs-mode.scala"
-    if [[ "${COMMAND_ARGS}" == *--hdfs* ]]; then
-      export EXPRESS_MODE="${hdfs_mode_script}"
-      COMMAND_ARGS=$(echo "${COMMAND_ARGS}" | sed s/--hdfs//)
-    elif [[ "${COMMAND_ARGS}" == *--local* ]]; then
-      export EXPRESS_MODE="${local_mode_script}"
-      COMMAND_ARGS=$(echo "${COMMAND_ARGS}" | sed s/--local//)
-    else
-      export EXPRESS_MODE="${local_mode_script}"
-    fi
-
-    # Run the shell
-    "${bin}/express-shell" ${COMMAND_ARGS}
-    exit $?
-    ;;
-  *)
-    echo "Unknown command: ${command}"
-    echo "Try:"
-    echo "  express help"
-    exit 1
-    ;;
-esac
-
-export EXPRESS_JAVA_OPTS
-
-java_opts=
-if [ ! -z "${JAVA_LIBRARY_PATH}" ]; then
-  java_opts="${java_opts} -Djava.library.path=${JAVA_LIBRARY_PATH}"
-fi
-
-# Run it!
-if [ -z "${scalding_tool}" ]; then
-  # In this case the user is running an arbitrary jar with express code on the classpath.
-  # TODO EXP-243: Until we add a keyword in the express/modeling projects to run a model
-  # lifecycle tool, we use `express jar` to run it. You may need to add the following
-  # flags for this to work: "-Dtmpjars=${tmpjars}" and
-  # "-Dmapreduce.task.classpath.user.precedence=true"
-  exec java \
-    -cp "${express_cp}" ${java_opts} ${EXPRESS_JAVA_OPTS} \
-    "${class}" ${COMMAND_ARGS}
-else
-  canonical_user_target=$(resolve_symlink ${user_target})
-  if [ -z "${canonical_user_target}" ]; then
-    echo "File does not exist: ${user_target}"
-    exit 1
-  fi
-
-  # If the user has put any -Dhadoop.arg=value elements in their arguments ($COMMAND_ARGS),
-  # then we need to extract these left-justified arguments from COMMAND_ARGS and add them
-  # to HADOOP_ARGS to pass as argments to scalding_tool to be parsed by Hadoop's
-  # GenericOptionsParser. The remaining arguments must be delivered as the final arguments
-  # to scalding_tool after its other arguments.
-
-  # We define and then execute two methods on $COMMAND_ARGS to do this separation.
-
-  function get_hadoop_argv() {
-    out=""
-    while [ ! -z "${1}" ]; do
-      if [ "${1}" == "-D" ]; then
-        out="${out} -D ${2}"
-        shift # Consume -D
-        shift # Consume prop=val
-      elif [[ "${1}" == -D* ]]; then
-        # Argument matches -Dprop=val.
-        # Note [[ eval ]] and lack of "quotes" around -D*.
-        out="${out} ${1}"
-        shift
-      elif [ "${1}" == "-conf" ]; then
-        out="${out} -conf ${2}"
-        shift # Consume -conf
-        shift # Consume <configuration file>
-      elif [ "${1}" == "-fs" ]; then
-        out="${out} -fs ${2}"
-        shift # Consume -fs
-        shift # Consume <local|namenode:port>
-      elif [ "${1}" == "-jt" ]; then
-        out="${out} -jt ${2}"
-        shift # Consume -jt
-        shift # Consume <local|jobtracker:port>
-      elif [ "${1}" == "-archives" ]; then
-        out="${out} -archives ${2}"
-        shift # Consume -archives
-        shift # Consume <comma separated list of jars>
-      else
-        break # Matched a non -D argument; stop parsing.
-      fi
-    done
-
-    # Echo all the -Dargs.
-    echo "$out"
-  }
-
-  # Return the part of COMMAND_ARGS that does not contain leading -D prop=val or -Dprop-val
-  function get_user_argv() {
-    while [ ! -z "${1}" ]; do
-      if [ "${1}" == "-D" ]; then
-        shift
-        shift # Consume this and the following prop=val
-      elif [[ "${1}" == -D* ]]; then
-        shift # Consume -Dprop=val
-      elif [ "${1}" == "-conf" ]; then
-        shift # Consume -conf
-        shift # Consume <configuration file>
-      elif [ "${1}" == "-fs" ]; then
-        shift # Consume -fs
-        shift # Consume <local|namenode:port>
-      elif [ "${1}" == "-jt" ]; then
-        shift # Consume -jt
-        shift # Consume <local|jobtracker:port>
-      elif [ "${1}" == "-archives" ]; then
-        shift # Consume -archives
-        shift # Consume <comma separated list of jars>
-      else
-        break
-      fi
-    done
-
-    # Echo the remaining args
-    echo $*
-  }
-
-  hadoop_argv=$(get_hadoop_argv $COMMAND_ARGS)
-  user_argv=$(get_user_argv $COMMAND_ARGS)
-
-  if [ -z "${class}" ]; then
-    # In this case the user is running an uncompiled script.
-    exec java \
-      -cp "${express_cp}" ${java_opts} ${EXPRESS_JAVA_OPTS} \
-      -Dorg.kiji.schema.impl.AvroCellEncoder.SCHEMA_VALIDATION=DISABLED \
-      ${scalding_tool} \
-      "-Dtmpjars=${tmpjars}" \
-      "-Dmapreduce.task.classpath.user.precedence=true" \
-      ${hadoop_argv} \
-      "${canonical_user_target}" \
-      "${run_mode_flag}" \
-      ${user_argv}
-  else
-    # In this case the user is running a compiled Scalding Job in a jar.
-    exec java \
-      -cp "${express_cp}" ${java_opts} ${EXPRESS_JAVA_OPTS} \
-      -Dorg.kiji.schema.impl.AvroCellEncoder.SCHEMA_VALIDATION=DISABLED \
-      ${scalding_tool} \
-      "-Dtmpjars=file://${canonical_user_target},${tmpjars}" \
-      "-Dmapreduce.task.classpath.user.precedence=true" \
-      ${hadoop_argv} \
-      "${class}" \
-      "${run_mode_flag}" \
-      ${user_argv}
-  fi
-fi
diff --git a/src/main/scripts/express-shell b/src/main/scripts/express-shell
deleted file mode 100755
index 3ec658dcb37932d6257c5ada44f506296e7e23bb..0000000000000000000000000000000000000000
--- a/src/main/scripts/express-shell
+++ /dev/null
@@ -1,172 +0,0 @@
-#!/bin/bash --posix
-#
-#   (c) Copyright 2013 WibiData, Inc.
-#
-#   See the NOTICE file distributed with this work for additional
-#   information regarding copyright ownership.
-#
-#   Licensed under the Apache License, Version 2.0 (the "License");
-#   you may not use this file except in compliance with the License.
-#   You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-#
-# Provides an entry point for running a KijiExpress-specific Scala shell.
-
-# Identify the bin dir in the distribution from which this script is running.
-bin=$(dirname $0)
-bin=$(cd ${bin} && pwd)
-
-# Set the main class used to run the KijiExpress Scala REPL.
-main_class="org.kiji.express.repl.ExpressShell"
-
-# This script should be run from the express script, which should export an environment variable
-# named EXPRESS_CP
-if [[ -z "${EXPRESS_CP}" ]]; then
-  echo "The environment variable EXPRESS_CP is undefined."
-  echo "If you are trying to run the KijiExpress shell, use the express command to do so."
-fi
-
-# It should also export TMPJARS, which should contain a comma-separated list of paths to jars
-# to add to the distributed cache of run jobs.
-if [[ -z "${TMPJARS}" ]]; then
-  echo "The environment variable TMPJARS is undefined."
-  echo "If you are trying to run the KijiExpress shell, use the express command to do so."
-fi
-
-# It should also export EXPRESS_MODE, which should be the path to a script of Scala code the
-# REPL should preload so that Hadoop jobs execute in the Cascading local runner or a Hadoop
-# cluster.
-if [[ -z "${EXPRESS_MODE}" ]]; then
-  echo "The environment variable EXPRESS_MODE is undefined."
-  echo "If you are trying to run the KijiExpress shell, use the express command to do so."
-fi
-
-# There should be a file containing import statements the REPL should execute on startup in the
-# bin directory.
-imports_script="${bin}/imports.scala"
-
-# Not sure what the right default is here: trying nonzero.
-scala_exit_status=1
-saved_stty=""
-
-# restore stty settings (echo in particular)
-function restoreSttySettings() {
-  if [[ -n ${SCALA_RUNNER_DEBUG} ]]; then
-    echo "restoring stty: ${saved_stty}"
-  fi
-
-  stty ${saved_stty}
-  saved_stty=""
-}
-
-function onExit() {
-  if [[ "${saved_stty}" != "" ]]; then
-    restoreSttySettings
-    exit ${scala_exit_status}
-  fi
-}
-
-# to reenable echo if we are interrupted before completing.
-trap onExit INT
-
-# save terminal settings
-saved_stty=$(stty -g 2>/dev/null)
-# clear on error so we don't later try to restore them
-if [[ ! $? ]]; then
-  saved_stty=""
-fi
-if [[ -n ${SCALA_RUNNER_DEBUG} ]]; then
-  echo "saved stty: ${saved_stty}"
-fi
-
-cygwin=false;
-case "$(uname)" in
-    CYGWIN*) cygwin=true ;;
-esac
-
-CYGWIN_JLINE_TERMINAL=
-if ${cygwin}; then
-    if [ "${OS}" = "Windows_NT" ] && cygpath -m .>/dev/null 2>/dev/null ; then
-        format=mixed
-    else
-        format=windows
-    fi
-    WIBI_SHELL_CLASSPATH=`cygpath --path --$format "$WIBI_SHELL_CLASSPATH"`
-    case "${TERM}" in
-        rxvt* | xterm*)
-            stty -icanon min 1 -echo
-            CYGWIN_JLINE_TERMINAL="-Djline.terminal=scala.tools.jline.UnixTerminal"
-        ;;
-    esac
-fi
-
-[ -n "${JAVA_OPTS}" ] || JAVA_OPTS="-Xmx256M -Xms32M"
-
-# break out -D and -J options and add them to JAVA_OPTS as well
-# so they reach the underlying JVM in time to do some good.  The
-# -D options will be available as system properties.
-declare -a java_args
-declare -a scala_args
-
-# Don't use the bootstrap classloader.
-CPSELECT="-classpath "
-
-while [ $# -gt 0 ]; do
-  case "$1" in
-    -D*)
-      # pass to scala as well: otherwise we lose it sometimes when we
-      # need it, e.g. communicating with a server compiler.
-      java_args=("${java_args[@]}" "$1")
-      scala_args=("${scala_args[@]}" "$1")
-      shift
-      ;;
-    -J*)
-      # as with -D, pass to scala even though it will almost
-      # never be used.
-      java_args=("${java_args[@]}" "${1:2}")
-      scala_args=("${scala_args[@]}" "$1")
-      shift
-      ;;
-    -toolcp)
-      TOOL_CLASSPATH="$TOOL_CLASSPATH:$2"
-      shift 2
-      ;;
-    *)
-      scala_args=("${scala_args[@]}" "$1")
-      shift
-      ;;
-  esac
-done
-
-# reset "$@" to the remaining args
-set -- "${scala_args[@]}"
-
-if [ -z "${JAVACMD}" -a -n "${JAVA_HOME}" -a -x "${JAVA_HOME}/bin/java" ]; then
-  JAVACMD="${JAVA_HOME}/bin/java"
-fi
-
-"${JAVACMD:=java}" \
-  ${JAVA_OPTS} \
-  "${java_args[@]}" \
-  ${CPSELECT}"${TOOL_CLASSPATH}:${EXPRESS_CP}" \
-  -Dscala.usejavacp=true \
-  -Denv.emacs="${EMACS}" \
-  -Dtmpjars="${TMPJARS}" \
-  ${CYGWIN_JLINE_TERMINAL} \
-  ${main_class} ${@} \
-  -i "${imports_script}" \
-  -i "${EXPRESS_MODE}" \
-  -Yrepl-sync
-# The -Yrepl-sync option is a fix for the 2.9.1 REPL. This should probably not be necessary in the future.
-
-# record the exit status lest it be overwritten:
-# then reenable echo and propagate the code.
-scala_exit_status=$?
-onExit
diff --git a/src/main/scripts/hdfs-mode.scala b/src/main/scripts/hdfs-mode.scala
deleted file mode 100644
index 4b4d8715be488db506372cf9e8b75486123aa2bf..0000000000000000000000000000000000000000
--- a/src/main/scripts/hdfs-mode.scala
+++ /dev/null
@@ -1,21 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-com.twitter.scalding.Mode.mode =
-    com.twitter.scalding.Hdfs(false, org.apache.hadoop.hbase.HBaseConfiguration.create())
-
diff --git a/src/main/scripts/imports.scala b/src/main/scripts/imports.scala
deleted file mode 100644
index 35a032dd5be9dbd49a17e05cbbc0c21c9e3f464e..0000000000000000000000000000000000000000
--- a/src/main/scripts/imports.scala
+++ /dev/null
@@ -1,23 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import com.twitter.scalding._
-import org.kiji.express._
-import org.kiji.express.flow._
-import org.kiji.express.repl.Implicits._
-
diff --git a/src/main/scripts/local-mode.scala b/src/main/scripts/local-mode.scala
deleted file mode 100644
index 342c4e4f1f678147574f6c206d4f59be14ff28a6..0000000000000000000000000000000000000000
--- a/src/main/scripts/local-mode.scala
+++ /dev/null
@@ -1,20 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-com.twitter.scalding.Mode.mode = com.twitter.scalding.Local(false)
-
diff --git a/src/test/avro/TestRecords.avdl b/src/test/avro/TestRecords.avdl
deleted file mode 100644
index e66516a47764ad3cf1c5274819e7febf950ad818..0000000000000000000000000000000000000000
--- a/src/test/avro/TestRecords.avdl
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** This protocol defines avro objects used for testing. */
-@namespace("org.kiji.express.avro")
-protocol Testing {
-  /** Used to test overriding a HashSpec reader schema with a specific record. */
-  record SpecificRecordTest {
-    /** hash_size field exists in HasSpec, so we are able to read it with this specific record. */
-    int hash_size;
-  }
-
-  record SimpleRecord {
-    long l;
-    string s;
-    string o = "default-value";
-  }
-
-  record NameFormats {
-    int snake_case_ugh;
-    int CamelCaseEvenWorse;
-    int camelPupCase;
-    int SCREAMING_SNAKE_CASE_YAH;
-  }
-}
diff --git a/src/test/resources/data/input_lines.txt b/src/test/resources/data/input_lines.txt
deleted file mode 100644
index 719695c6a62aa7b846b0664ba549e137469892fe..0000000000000000000000000000000000000000
--- a/src/test/resources/data/input_lines.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-key1 10
-key1 20
-key1 30
-key2 10
-key2 20
\ No newline at end of file
diff --git a/src/test/resources/layout/avro-types-1.3.json b/src/test/resources/layout/avro-types-1.3.json
deleted file mode 100644
index 70f12cf3cbd89691d98360f36c7dfe631a02369c..0000000000000000000000000000000000000000
--- a/src/test/resources/layout/avro-types-1.3.json
+++ /dev/null
@@ -1,108 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-{
-  name : "table",
-  keys_format : {
-    encoding : "FORMATTED",
-    salt : {
-      hash_size : 2
-    },
-    components : [ {
-      name : "dummy",
-      type : "STRING"
-    }]
-  },
-  locality_groups : [ {
-    name : "default",
-    in_memory : false,
-    max_versions : 2147483647,
-    ttl_seconds : 2147483647,
-    compression_type : "GZ",
-    families : [ {
-      name : "family",
-      columns : [ {
-        name : "column1",
-        column_schema : {
-          type : "INLINE",
-          value : '{"type": "fixed", "size": 16, "name": "md5"}'
-        }
-      }, {
-        name : "column2",
-        column_schema : {
-          type : "INLINE",
-          value : '"bytes"'
-        }
-      }, {
-        name : "column3",
-        column_schema : {
-          type : "AVRO",
-          default_reader: {
-            json: "{\"type\":\"record\",\"name\":\"HashSpec\",\"namespace\":\"org.kiji.schema.avro\",\"fields\":[{\"name\":\"hash_type\",\"type\":{\"type\":\"enum\",\"name\":\"HashType\",\"doc\":\"MD5 hashing (16 bytes).\",\"symbols\":[\"MD5\"]},\"default\":\"MD5\"},{\"name\":\"hash_size\",\"type\":\"int\",\"default\":16},{\"name\":\"suppress_key_materialization\",\"type\":\"boolean\",\"default\":false}]}"
-          }
-        }
-      }, {
-        name : "column_validated",
-        column_schema : {
-          type : "AVRO",
-          avro_validation_policy : "DEVELOPER",
-          default_reader : {
-            json: '{"type": "record", "name": "stringcontainer", "doc": "A simple inline record that contains a string", "fields": [{"name": "contained_string", "type": "string", "id": "int"}]}'
-          }
-        }
-      }, {
-        name : "double_column",
-        column_schema : {
-          type : "INLINE",
-          value : '"double"'
-        }
-      }, {
-        name : "column_compatibility",
-        column_schema : {
-          avro_validation_policy : "SCHEMA_1_0",
-          type : "INLINE",
-          value : '{"type": "record", "name": "stringcontainer", "doc": "A simple inline record that contains a string", "fields": [{"name": "contained_string", "type": "string", "id": "int"}]}'
-        }
-      }]
-    }, {
-      name : "searches",
-      description : "A map-type column family",
-      map_schema : {
-        type: "INLINE",
-        value: '"int"'
-      }
-    }, {
-      name : "searches_dev",
-      description : "A map-type column family in dev mode",
-      map_schema : {
-        avro_validation_policy : "DEVELOPER",
-        type: "AVRO",
-        value: '"int"',
-        writers: [{json: '"int"'}]
-      }
-    }, {
-      name : "animals",
-      description : "A map-type column family for testing numeric-initial strings",
-      map_schema : {
-        type: "INLINE",
-        value: '"string"'
-      }
-    }]
-  } ],
-  version : "layout-1.3.0"
-}
diff --git a/src/test/resources/layout/avro-types-complete.json b/src/test/resources/layout/avro-types-complete.json
deleted file mode 100644
index 2fe9d47ba8089431811f6f5a8ba0e6881df57db0..0000000000000000000000000000000000000000
--- a/src/test/resources/layout/avro-types-complete.json
+++ /dev/null
@@ -1,191 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-{
-  name: "avro_types_complete",
-  keys_format: {
-    encoding: "FORMATTED",
-    salt: {
-      hash_size: 2
-    },
-    components: [ {
-      name: "dummy",
-      type: "STRING"
-    } ]
-  },
-  locality_groups:  [ {
-    name: "default",
-    in_memory: false,
-    max_versions: 2147483647,
-    ttl_seconds: 2147483647,
-    bloom_type: "ROW",
-    compression_type: "GZ",
-    families: [ {
-      name: "strict",
-      columns: [ {
-        name: "counter",
-        column_schema: {
-          storage: "FINAL",
-          type: "COUNTER"
-        }
-      }, {
-        name: "raw",
-        column_schema: {
-          type: "RAW_BYTES"
-        }
-      },{
-        name: "null",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '"null"'},
-          readers: [{json: '"null"'}],
-          writers: [{json: '"null"'}]
-        }
-      },{
-        name: "boolean",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '"boolean"'},
-          readers: [{json: '"boolean"'}],
-          writers: [{json: '"boolean"'}]
-        }
-      }, {
-        name: "int",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '"int"'},
-          readers: [{json: '["int", "long"]'}],
-          writers: [{json: '"int"'}]
-        }
-      }, {
-        name: "long",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '"long"'},
-          readers: [{json: '"long"'}],
-          writers: [{json: '["int", "long"]'}]
-        }
-      }, {
-        name: "float",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '"float"'},
-          readers: [{json: '["float", "double"]'}],
-          writers: [{json: '"float"'}]
-        }
-      }, {
-        name: "double",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '"double"'},
-          readers: [{json: '"double"'}],
-          writers: [{json: '["float", "double"]'}]
-        }
-        }, {
-        name: "bytes",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '"bytes"'},
-          readers: [{json: '"bytes"'}],
-          writers: [{json: '"bytes"'}]
-        }
-      }, {
-        name: "string",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '{ "type": "string", "avro.java.string": "String" }'},
-          readers: [{json: '{ "type": "string", "avro.java.string": "String" }'}],
-          writers: [{json: '{ "type": "string" }'}]
-        }
-      }, {
-        name: "specific",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          specific_reader_schema_class: "org.kiji.express.avro.SimpleRecord",
-          readers: [{json: '{"type":"record","name":"SimpleRecord","namespace":"org.kiji.express.avro","fields":[{"name":"l","type":"long"},{"name":"s","type":{"type":"string","avro.java.string":"String"}},{"name":"o","type":{"type":"string","avro.java.string":"String"},"default":"default-value"}]}'}],
-          writers: [{json: '{"type":"record","name":"SimpleRecord","namespace":"org.kiji.express.avro","fields":[{"name":"l","type":"long"},{"name":"s","type":{"type":"string","avro.java.string":"String"}},{"name":"o","type":{"type":"string","avro.java.string":"String"},"default":"default-value"}]}'}]
-        }
-      }, {
-        name: "generic",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '{"type": "record", "name": "Vector", "fields": [{"name": "length", "type": "int"}, {"name": "angle", "type": "float"}]}'},
-          readers: [{json: '{"type": "record", "name": "Vector", "fields": [{"name": "length", "type": "int"}, {"name": "angle", "type": "float"}]}'}],
-          writers: [{json: '{"type": "record", "name": "Vector", "fields": [{"name": "length", "type": "int"}, {"name": "angle", "type": "float"}]}'}]
-        }
-      }, {
-        name: "enum",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '{"type": "enum", "name": "Direction", "symbols": ["NORTH", "EAST", "SOUTH", "WEST"]}'},
-          readers: [{json: '{"type": "enum", "name": "Direction", "symbols": ["NORTH", "EAST", "SOUTH", "WEST"]}'}],
-          writers: [{json: '{"type": "enum", "name": "Direction", "symbols": ["NORTH", "EAST", "SOUTH", "WEST"]}'}]
-        }
-      }, {
-        name: "array",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '{"type": "array", "items": { "type": "string", "avro.java.string": "String" }}'},
-          readers: [{json: '{"type": "array", "items": { "type": "string", "avro.java.string": "String" }}'}],
-          writers: [{json: '{"type": "array", "items": { "type": "string", "avro.java.string": "String" }}'}]
-        }
-      }, {
-        name: "map",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '{"type": "map", "values": "int"}'},
-          readers: [{json: '{"type": "map", "values": "int"}'}],
-          writers: [{json: '{"type": "map", "values": "int"}'}]
-        }
-      }, {
-        name: "union",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '[{ "type": "string", "avro.java.string": "String" }, "int"]'},
-          readers: [{json: '[{ "type": "string", "avro.java.string": "String" }, "int"]'}],
-          writers: [{json: '["string", "int"]'}]
-        }
-      }, {
-        name: "fixed",
-        column_schema: {
-          type: "AVRO",
-          avro_validation_policy: "STRICT",
-          default_reader: {json: '{"type": "fixed", "size": 10, "name": "hash"}'},
-          readers: [{json: '{"type": "fixed", "size": 10, "name": "hash"}'}],
-          writers: [{json: '{"type": "fixed", "size": 10, "name": "hash"}'}]
-        }
-      } ]
-    } ]
-  } ],
-  version: "layout-1.3.0"
-}
diff --git a/src/test/resources/layout/avro-types.json b/src/test/resources/layout/avro-types.json
deleted file mode 100644
index e05befd9bedf25cab6479d064c95759afebe4a09..0000000000000000000000000000000000000000
--- a/src/test/resources/layout/avro-types.json
+++ /dev/null
@@ -1,101 +0,0 @@
-/**
- * (c) Copyright 2012 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-{
-  name : "table",
-  keys_format : {
-    encoding : "FORMATTED",
-    salt : {
-      hash_size : 2
-    },
-    components : [ {
-      name : "dummy",
-      type : "STRING"
-    }]
-  },
-  locality_groups : [ {
-    name : "default",
-    in_memory : false,
-    max_versions : 2147483647,
-    ttl_seconds : 2147483647,
-    compression_type : "GZ",
-    families : [ {
-      name : "family",
-      columns : [ {
-        name : "column1",
-        column_schema : {
-          type : "INLINE",
-          value : '{"type": "fixed", "size": 16, "name": "md5"}'
-        }
-      }, {
-        name : "column2",
-        column_schema : {
-          type : "INLINE",
-          value : '"bytes"'
-        }
-      }, {
-        name : "double_column",
-        column_schema : {
-          type : "INLINE",
-          value : '"double"'
-        }
-      }, {
-        name : "column3",
-        column_schema : {
-          type : "CLASS",
-          value : "org.kiji.schema.avro.HashSpec"
-        }
-      }, {
-        name : "column4",
-        column_schema : {
-          type : "INLINE",
-          value : '{"type": "record", "name": "stringcontainer", "doc": "A simple inline record that contains a string", "fields": [{"name": "contained_string", "type": "string", "id": "int"}]}'
-        }
-      }, {
-        name : "column5",
-        description: "A class that does not exist on the classpath, for testing purposes.",
-        column_schema : {
-          type : "CLASS",
-          value : "org.kiji.nonexistent.class"
-        }
-      }, {
-        name : "simple",
-        column_schema : {
-          avro_validation_policy : "SCHEMA_1_0",
-          type : "INLINE",
-          value : "{\"type\":\"record\",\"name\":\"SimpleRecord\",\"namespace\":\"org.kiji.express.avro\",\"fields\":[{\"name\":\"l\",\"type\":\"long\"},{\"name\":\"s\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"o\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"},\"default\":\"default-value\"}]}"
-        }
-      }]
-    }, {
-      name : "searches",
-      description : "A map-type column family",
-      map_schema : {
-        type: "INLINE",
-        value: '"int"'
-      }
-    }, {
-      name : "animals",
-      description : "A map-type column family for testing numeric-initial strings",
-      map_schema : {
-        type: "INLINE",
-        value: '"string"'
-      }
-    }]
-  } ],
-  version : "layout-1.1.0"
-}
diff --git a/src/test/resources/log4j.properties b/src/test/resources/log4j.properties
deleted file mode 100644
index c5518b6664509ead8d0c9aad1f14544bbbfd6f53..0000000000000000000000000000000000000000
--- a/src/test/resources/log4j.properties
+++ /dev/null
@@ -1,69 +0,0 @@
-#
-#   (c) Copyright 2013 WibiData, Inc.
-#
-#   See the NOTICE file distributed with this work for additional
-#   information regarding copyright ownership.
-#
-#   Licensed under the Apache License, Version 2.0 (the "License");
-#   you may not use this file except in compliance with the License.
-#   You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-log4j.rootLogger=${kiji.logger}
-log4j.logger.cleanup=${kiji.cleanupLogger}
-
-# By default, log INFO to the console.
-kiji.logger=INFO,console
-kiji.cleanupLogger=DEBUG,cleanup
-
-# Uncomment the following to log to the console:
-#kiji.logger=DEBUG,console
-
-# Define a console appender.
-log4j.appender.console=org.apache.log4j.ConsoleAppender
-log4j.appender.console.target=System.err
-log4j.appender.console.layout=org.apache.log4j.PatternLayout
-log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c: %m%n
-
-# Define the cleanup appender.
-log4j.appender.cleanup=org.apache.log4j.RollingFileAppender
-log4j.appender.cleanup.Append=false
-log4j.appender.cleanup.File=target/cleanup.log
-log4j.appender.cleanup.layout=org.apache.log4j.PatternLayout
-log4j.appender.cleanup.layout.ConversionPattern=CLEANUP: %d{yy/MM/dd HH:mm:ss} %p %c: %m%n
-
-# Quiet down zookeeper; it's too noisy.
-log4j.logger.org.apache.zookeeper=WARN
-log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=WARN
-log4j.logger.org.apache.hadoop.hbase.zookeeper=WARN
-log4j.logger.org.apache.hadoop.hbase.client.HBaseAdmin=WARN
-
-# Hadoop 1.x / HBase 0.92 emits many SASL exceptions to stdout; silence these.
-log4j.logger.org.apache.zookeeper.client.ZooKeeperSaslClient=ERROR
-
-# Suppress Kiji installer messages for tests
-log4j.logger.org.kiji.schema.KijiInstaller=WARN
-
-# We do want kiji debug logging for all classes that have explicit finalizers.
-# These classes have special-purpose loggers just for their leak cleanup traces.
-# They are enabled below.
-log4j.logger.org.kiji.schema.impl.HBaseKiji.Cleanup=DEBUG
-log4j.logger.org.kiji.schema.impl.HBaseKijiTable.Cleanup=DEBUG
-log4j.logger.org.kiji.schema.impl.HBaseKijiRowScanner.Cleanup=DEBUG
-log4j.logger.org.kiji.schema.impl.HBaseSchemaTable.Cleanup=DEBUG
-log4j.logger.org.kiji.schema.impl.HBaseSystemTable.Cleanup=DEBUG
-log4j.logger.org.kiji.schema.impl.HBaseKijiTableReader.Cleanup=DEBUG
-log4j.logger.org.kiji.schema.impl.HBaseKijiTableWriter.Cleanup=DEBUG
-log4j.logger.org.kiji.schema.KijiMetaTable.Cleanup=DEBUG
-log4j.logger.org.kiji.schema.KijiTablePool.Cleanup=DEBUG
-
-# Loggers in the Kiji framework:
-log4j.logger.org.kiji.schema=WARN
-log4j.logger.org.kiji.mapreduce=WARN
-log4j.logger.org.kiji.express=DEBUG
diff --git a/src/test/resources/two-double-columns.json b/src/test/resources/two-double-columns.json
deleted file mode 100644
index 349e95fc65d9d401e84ce0b696345108648ec22c..0000000000000000000000000000000000000000
--- a/src/test/resources/two-double-columns.json
+++ /dev/null
@@ -1,50 +0,0 @@
-/**
- * (c) Copyright 2012 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-{
-  name : "table",
-  keys_format : {
-    encoding : "HASH",
-    hash_type : "MD5",
-    hash_size : 16
-  },
-  locality_groups : [ {
-    name : "default",
-    in_memory : false,
-    max_versions : 2147483647,
-    ttl_seconds : 2147483647,
-    compression_type : "GZ",
-    families : [ {
-      name : "family",
-      columns : [ {
-        name : "column1",
-        column_schema : {
-          type : "INLINE",
-          value : '"double"'
-        }
-      }, {
-        name : "column2",
-        column_schema : {
-          type : "INLINE",
-          value : '"double"'
-        }
-      } ]
-    } ]
-  } ],
-  version : "layout-1.0"
-}
diff --git a/src/test/scala/org/kiji/express/KijiSuite.scala b/src/test/scala/org/kiji/express/KijiSuite.scala
deleted file mode 100644
index c2a25c1539b7ba8a1682491923d548a3806d375e..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/KijiSuite.scala
+++ /dev/null
@@ -1,178 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express
-
-import java.io.InputStream
-import java.util.concurrent.atomic.AtomicInteger
-
-import com.twitter.scalding.TupleConversions
-import org.scalatest.FunSuite
-
-import org.kiji.express.flow.FlowCell
-import org.kiji.express.flow.util.Resources._
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiTable
-import org.kiji.schema.layout.KijiTableLayout
-import org.kiji.schema.layout.KijiTableLayouts
-import org.kiji.schema.shell.api.Client
-import org.kiji.schema.util.InstanceBuilder
-
-/** Contains convenience methods for writing tests that use Kiji. */
-trait KijiSuite
-    extends FunSuite
-    with TupleConversions {
-  // Counter for incrementing instance names by.
-  val counter: AtomicInteger = new AtomicInteger(0)
-
-  /**
-   * Builds a slice containing no values.  This can be used to test for behavior of missing
-   * values.
-   *
-   * @tparam T type of the values in the returned slice.
-   * @return an empty slice.
-   */
-  def missing[T](): Seq[FlowCell[T]] = Seq()
-
-  /**
-   * Builds a slice from a group type column name and list of version, value pairs.
-   *
-   * @tparam T type of the values contained within desired slice.
-   * @param columnName for a group type family, of the form "family:qualifier"
-   * @param values pairs of (version, value) to build the slice with.
-   * @return a slice containing the specified cells.
-   */
-  def slice[T](columnName: String, values: (Long, T)*): Seq[FlowCell[T]] = {
-    val parsedName = new KijiColumnName(columnName)
-    require(
-        parsedName.isFullyQualified,
-        "Fully qualified column names must be of the form \"family:qualifier\"."
-    )
-
-    values
-        .map { entry: (Long, T) =>
-          val (version, value) = entry
-          FlowCell(parsedName.getFamily, parsedName.getQualifier, version, value)
-        }
-  }
-
-  /**
-   * Builds a slice from a map type column name and a list of qualifier, version, value triples.
-   *
-   * @tparam T type of the values contained within desired slice.
-   * @param columnName for a map type family, of the form "family"
-   * @param values are triples of (qualifier, version, value) to build the slice with.
-   * @return a slice containing the specified cells.
-   */
-  def mapSlice[T](columnName: String, values: (String, Long, T)*): Seq[FlowCell[T]] = {
-    val parsedName = new KijiColumnName(columnName)
-    require(
-        !parsedName.isFullyQualified,
-        "Column family names must not contain any ':' characters."
-    )
-
-    values
-        .map { entry: (String, Long, T) =>
-          val (qualifier, version, value) = entry
-          FlowCell(parsedName.getFamily, qualifier, version, value)
-        }
-  }
-
-  /**
-   * Constructs and starts a test Kiji instance that uses fake-hbase.
-   *
-   * @param instanceName Name of the test Kiji instance.
-   * @return A handle to the Kiji instance that just got constructed. Note: This object must be
-   *     {{{release()}}}'d once it is no longer needed.
-   */
-  def makeTestKiji(instanceName: String = "default"): Kiji = {
-    new InstanceBuilder(instanceName).build()
-  }
-
-  /**
-   * Constructs and starts a test Kiji instance and creates a Kiji table.
-   *
-   * @param layout Layout of the test table.
-   * @param instanceName Name of the Kiji instance to create.
-   * @return A handle to the Kiji table that just got constructed. Note: This object must be
-   *     {{{release()}}}'d once it is no longer needed.
-   */
-  def makeTestKijiTable(
-      layout: KijiTableLayout,
-      instanceName: String = "default_%s".format(counter.incrementAndGet())
-  ): KijiTable = {
-    val tableName = layout.getName
-    val kiji: Kiji = new InstanceBuilder(instanceName)
-        .withTable(tableName, layout)
-        .build()
-
-    val table: KijiTable = kiji.openTable(tableName)
-    kiji.release()
-    return table
-  }
-
-  /**
-   * Loads a [[org.kiji.schema.layout.KijiTableLayout]] from the classpath. See
-   * [[org.kiji.schema.layout.KijiTableLayouts]] for some layouts that get put on the classpath
-   * by KijiSchema.
-   *
-   * @param resourcePath Path to the layout definition file.
-   * @return The layout contained within the provided resource.
-   */
-  def layout(resourcePath: String): KijiTableLayout = {
-    val tableLayoutDef = KijiTableLayouts.getLayout(resourcePath)
-    KijiTableLayout.newLayout(tableLayoutDef)
-  }
-
-  /**
-   * Executes a series of KijiSchema Shell DDL commands, separated by `;`.
-   *
-   * @param kiji to execute the commands against.
-   * @param commands to execute against the Kiji instance.
-   */
-  def executeDDLString(kiji: Kiji, commands: String) {
-    doAndClose(Client.newInstance(kiji.getURI)) { ddlClient =>
-      ddlClient.executeUpdate(commands)
-    }
-  }
-
-  /**
-   * Executes a series of KijiSchema Shell DDL commands, separated by `;`.
-   *
-   * @param kiji to execute the commands against.
-   * @param stream to read a series of commands to execute against the Kiji instance.
-   */
-  def executeDDLStream(kiji: Kiji, stream: InputStream) {
-    doAndClose(Client.newInstance(kiji.getURI)) { ddlClient =>
-      ddlClient.executeStream(stream)
-    }
-  }
-
-  /**
-   * Executes a series of KijiSchema Shell DDL commands, separated by `;`.
-   *
-   * @param kiji to execute the commands against.
-   * @param resourcePath to the classpath resource that a series of commands to execute
-   *     against the Kiji instance will be read from.
-   */
-  def executeDDLResource(kiji: Kiji, resourcePath: String) {
-    executeDDLStream(kiji, getClass.getClassLoader.getResourceAsStream(resourcePath))
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/ColumnSpecSuite.scala b/src/test/scala/org/kiji/express/flow/ColumnSpecSuite.scala
deleted file mode 100644
index 18b81add03b87d392dbdbfb7ca2c9818afd86c9f..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/ColumnSpecSuite.scala
+++ /dev/null
@@ -1,141 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.apache.avro.Schema
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-
-@RunWith(classOf[JUnitRunner])
-class ColumnSpecSuite extends FunSuite {
-  val filter = RegexQualifierFilterSpec(".*")
-  val colFamily = "myfamily"
-  val colQualifier = "myqualifier"
-  val qualifierSelector = 'qualifierSym
-  val schema = Some(Schema.create(Schema.Type.LONG))
-
-  // TODO(CHOP-37): Test with different filters once the new method of specifying filters
-  // correctly implements the .equals() and hashCode() methods.
-  // Should be able to change the following line to:
-  // def filter = new RegexQualifierColumnFilter(".*")
-  val maxVersions = 2
-
-  val colWithOptions: QualifiedColumnInputSpec = QualifiedColumnInputSpec(
-      family = colFamily,
-      qualifier = colQualifier,
-      maxVersions = maxVersions,
-      filter = Some(filter)
-  )
-
-  test("Fields of a ColumnFamilyInputSpec are the same as those it is constructed with.") {
-    val col: ColumnFamilyInputSpec = new ColumnFamilyInputSpec(family = colFamily)
-    assert(colFamily === col.family)
-  }
-
-  test("ColumnInputSpec factory method creates ColumnFamilyInputSpec.") {
-    val col = ColumnInputSpec(colFamily)
-    assert(col.isInstanceOf[ColumnFamilyInputSpec])
-    assert(colFamily === col.asInstanceOf[ColumnFamilyInputSpec].family)
-  }
-
-  test("Fields of a ColumnFamilyOutputSpec are the same as those it is constructed with.") {
-    val col: ColumnFamilyOutputSpec = ColumnFamilyOutputSpec(colFamily, qualifierSelector)
-    assert(colFamily === col.family)
-    assert(qualifierSelector === col.qualifierSelector)
-    assert(None === col.schemaSpec.schema)
-  }
-
-  test("ColumnFamilyOutputSpec factory method creates ColumnFamilyOutputSpec.") {
-    val col = ColumnFamilyOutputSpec(colFamily, qualifierSelector, schema.get)
-
-    assert(colFamily === col.family)
-    assert(qualifierSelector === qualifierSelector)
-    assert(schema === col.schemaSpec.schema)
-  }
-
-  test("Fields of a QualifiedColumnInputSpec are the same as those it is constructed with.") {
-    val col: QualifiedColumnInputSpec = QualifiedColumnInputSpec(colFamily, colQualifier)
-    assert(colFamily === col.family)
-    assert(colQualifier === col.qualifier)
-  }
-
-  test("ColumnInputSpec factory method creates QualifiedColumnInputSpec.") {
-    val col = QualifiedColumnInputSpec(colFamily, colQualifier)
-    assert(col.isInstanceOf[QualifiedColumnInputSpec])
-    assert(colFamily === col.asInstanceOf[QualifiedColumnInputSpec].family)
-    assert(colQualifier === col.asInstanceOf[QualifiedColumnInputSpec].qualifier)
-  }
-
-  test("Fields of a QualifiedColumnOutputSpec are the same as those it is constructed with.") {
-    val col: QualifiedColumnOutputSpec =
-        QualifiedColumnOutputSpec(colFamily, colQualifier)
-
-    assert(colFamily === col.family)
-    assert(colQualifier === col.qualifier)
-    assert(None === col.schemaSpec.schema)
-  }
-
-  test("QualifiedColumnOutputSpec factory method creates QualifiedColumnOutputSpec.") {
-    val col = QualifiedColumnOutputSpec(colFamily, colQualifier, schema.get)
-    assert(colQualifier === col.qualifier)
-    assert(colFamily === col.family)
-    assert(schema === col.schemaSpec.schema)
-  }
-
-  test("Two ColumnFamilys with the same parameters are equal and hash to the same value.") {
-    val col1 = new ColumnFamilyInputSpec(colFamily)
-    val col2 = new ColumnFamilyInputSpec(colFamily)
-
-    assert(col1 === col2)
-    assert(col1.hashCode() === col2.hashCode())
-  }
-
-  test("Two qualified columns with the same parameters are equal and hash to the same value.") {
-    val col1 = new QualifiedColumnInputSpec(colFamily, colQualifier)
-    val col2 = new QualifiedColumnInputSpec(colFamily, colQualifier)
-
-    assert(col1 === col2)
-    assert(col1.hashCode() === col2.hashCode())
-  }
-
-  test("maxVersions is the same as constructed with.") {
-    assert(maxVersions == colWithOptions.maxVersions)
-  }
-
-  test("Default maxVersions is 1.") {
-    assert(1 == QualifiedColumnInputSpec(colFamily, colQualifier).maxVersions)
-  }
-
-  test("Filter is the same as constructed with.") {
-    assert(Some(filter) == colWithOptions.filter)
-  }
-
-  test("ColumnInputSpec with the same maxVersions & filter are equal and hash to the same value.") {
-
-    val col2: QualifiedColumnInputSpec = new QualifiedColumnInputSpec(
-        colFamily, colQualifier,
-        filter = Some(filter),
-        maxVersions = maxVersions
-    )
-    assert(col2 === colWithOptions)
-    assert(col2.hashCode() === colWithOptions.hashCode())
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/EntityIdSuite.scala b/src/test/scala/org/kiji/express/flow/EntityIdSuite.scala
deleted file mode 100644
index d1574a67619517df9576bff5a15f2376a61ac413..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/EntityIdSuite.scala
+++ /dev/null
@@ -1,451 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import scala.collection.JavaConverters.seqAsJavaListConverter
-import scala.collection.mutable.Buffer
-
-import com.twitter.scalding.Args
-import com.twitter.scalding.JobTest
-import com.twitter.scalding.TextLine
-import com.twitter.scalding.Tsv
-import org.apache.hadoop.conf.Configuration
-import org.apache.hadoop.hbase.HBaseConfiguration
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.flow.EntityId.HashedEntityId
-import org.kiji.express.flow.util.Resources.doAndRelease
-import org.kiji.schema.EntityIdFactory
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiURI
-import org.kiji.schema.layout.KijiTableLayout
-import org.kiji.schema.layout.KijiTableLayouts
-
-/**
- * Unit tests for [[org.kiji.express.flow.EntityId]].
- */
-@RunWith(classOf[JUnitRunner])
-class EntityIdSuite extends KijiSuite {
-
-  import org.kiji.express.flow.EntityIdSuite._
-
-  /** Table layout with formatted entity IDs to use for tests. */
-  val formattedEntityIdLayout: KijiTableLayout = layout(KijiTableLayouts.FORMATTED_RKF)
-  // Create a table to use for testing
-  val formattedTableUri: KijiURI =
-      doAndRelease(makeTestKijiTable(formattedEntityIdLayout)) { table: KijiTable => table.getURI }
-
-  /** Table layout with hashed entity IDs to use for tests. */
-  val hashedEntityIdLayout: KijiTableLayout = layout(KijiTableLayouts.HASHED_FORMATTED_RKF)
-  // Create a table to use for testing
-  val hashedTableUri: KijiURI =
-      doAndRelease(makeTestKijiTable(hashedEntityIdLayout)) { table: KijiTable => table.getURI }
-
-  val configuration: Configuration = HBaseConfiguration.create()
-
-  val formattedEidFactory = EntityIdFactory.getFactory(formattedEntityIdLayout)
-  val hashedEidFactory = EntityIdFactory.getFactory(hashedEntityIdLayout)
-
-  // ------- "Unit tests" for comparisons and creation. -------
-  test("Create an Express EntityId from a Kiji EntityId and vice versa in a formatted table.") {
-    val expressEid = EntityId("test", "1", "2", 1, 7L)
-    val kijiEid = expressEid.toJavaEntityId(formattedEidFactory)
-    val expected: java.util.List[AnyRef] =
-        Seq[AnyRef]("test", "1", "2", 1: java.lang.Integer, 7L: java.lang.Long).asJava
-
-    assert(expected === kijiEid.getComponents)
-
-    val recreate = EntityId.fromJavaEntityId(kijiEid)
-
-    assert(expressEid === recreate)
-    assert(recreate(0) === "test")
-  }
-
-  test("Create an Express EntityId from a Kiji EntityId and vice versa in a hashed table.") {
-    val origKijiEid = hashedEidFactory.getEntityId("test")
-
-    val expressEid = HashedEntityId(origKijiEid.getHBaseRowKey())
-    val expressToKijiEid = expressEid.toJavaEntityId(hashedEidFactory)
-
-    val recreate = EntityId.fromJavaEntityId(expressToKijiEid)
-    assert(recreate.components.equals(List(origKijiEid.getHBaseRowKey())))
-  }
-
-  test("Creating an EntityId from a Hashed table fails if there is more than one component.") {
-    val eid: EntityId = EntityId("one", 2)
-    val exception = intercept[org.kiji.schema.EntityIdException] {
-      eid.toJavaEntityId(hashedEidFactory)
-    }
-    assert(exception.getMessage.contains("Too many components"))
-  }
-
-  test("Test equality between two EntityIds.") {
-    val eidComponents1: EntityId = EntityId("test", 1)
-    val eidComponents2: EntityId = EntityId("test", 1)
-
-    assert(eidComponents1 === eidComponents2)
-    assert(eidComponents2 === eidComponents1)
-  }
-
-  test("Test comparison between two EntityIds.") {
-    val eidComponents1: EntityId = EntityId("test", 2)
-    val eidComponents2: EntityId = EntityId("test", 3)
-
-    assert(eidComponents2 > eidComponents1)
-    assert(eidComponents1 < eidComponents2)
-  }
-
-  test("Test comparison between two EntityIds with different lengths.") {
-    val eidComponents1: EntityId = EntityId("test", 2)
-    val eidComponents2: EntityId = EntityId("test", 2, 1)
-
-    assert(eidComponents2 > eidComponents1)
-    assert(eidComponents1 < eidComponents2)
-  }
-
-  test("Test comparison between two EntityIds with different formats fails.") {
-    val eidComponents1: EntityId = EntityId("test", 2)
-    val eidComponents2: EntityId = EntityId("test", 2L)
-
-    val exception = intercept[EntityIdFormatMismatchException] {
-      eidComponents1 < eidComponents2
-    }
-
-    // Exception message should be something like:
-    // Mismatched Formats: Components: [java.lang.String,java.lang.Integer] and  Components:
-    // [java.lang.String,java.lang.Long] do not match.
-
-    assert(exception.getMessage.contains("String"))
-    assert(exception.getMessage.contains("Integer"))
-    assert(exception.getMessage.contains("Long"))
-  }
-
-  // ------- "integration tests" for joins. -------
-  /** Simple table layout to use for tests. The row keys are hashed. */
-  val simpleLayout: KijiTableLayout = layout(KijiTableLayouts.SIMPLE_TWO_COLUMNS)
-
-  /** Table layout using Avro schemas to use for tests. The row keys are formatted. */
-  val avroLayout: KijiTableLayout = layout("layout/avro-types.json")
-
-  test("Runs a job that joins two pipes, on user-created EntityIds.") {
-    // Create main input.
-    val mainInput: List[(String, String)] = List(
-      ("0", "0row"),
-      ("1", "1row"),
-      ("2", "2row"))
-
-    // Create input from side data.
-    val sideInput: List[(String, String)] = List(("0", "0row"), ("1", "2row"))
-
-    // Validate output.
-    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
-      assert(outputBuffer.size === 2)
-    }
-
-    // Create the JobTest for this test.
-    val jobTest = JobTest(new JoinUserEntityIdsJob(_))
-      .arg("input", "mainInputFile")
-      .arg("side-input", "sideInputFile")
-      .arg("output", "outputFile")
-      .source(TextLine("mainInputFile"), mainInput)
-      .source(TextLine("sideInputFile"), sideInput)
-      .sink(Tsv("outputFile"))(validateTest)
-
-    // Run the test in local mode.
-    jobTest.run.finish
-
-    // Run the test in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-
-  test("Runs a job that joins two pipes, on user-created and from a table (formatted) EntityIds.") {
-    // URI of the Kiji table to use.
-    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Create input from Kiji table.
-    val joinKijiInput: List[(EntityId, Seq[FlowCell[String]])] = List(
-      (EntityId("0row"), mapSlice("animals", ("0column", 0L, "0 dogs"))),
-      (EntityId("1row"), mapSlice("animals", ("0column", 0L, "1 cat"))),
-      (EntityId("2row"), mapSlice("animals", ("0column", 0L, "2 fish"))))
-
-    // Create input from side data.
-    val sideInput: List[(String, String)] = List(("0", "0row"), ("1", "2row"))
-
-    // Validate output.
-    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
-      assert(outputBuffer.size === 2)
-    }
-
-    // Create the JobTest for this test.
-    val jobTest = JobTest(new JoinUserAndFormattedFromTableJob(_))
-      .arg("input", uri)
-      .arg("side-input", "sideInputFile")
-      .arg("output", "outputFile")
-      .source(KijiInput(uri, ("animals" -> 'animals)), joinKijiInput)
-      .source(TextLine("sideInputFile"), sideInput)
-      .sink(Tsv("outputFile"))(validateTest)
-
-    // Run the test in local mode.
-    jobTest.run.finish
-
-
-    // Run the test in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-
-  test("Runs a job that joins two pipes, on EntityIds from a table (hashed), in local mode.") {
-    // URI of the hashed Kiji table to use.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Create input from hashed Kiji table.
-    val joinInput1: List[(EntityId, Seq[FlowCell[String]])] = List(
-      (EntityId("0row"), slice("family:column1", (0L, "0 dogs"))),
-      (EntityId("1row"), slice("family:column1", (0L, "1 cat"))),
-      (EntityId("2row"), slice("family:column1", (0L, "2 fish"))))
-
-
-    // Create input from hashed Kiji table.
-    val joinInput2: List[(EntityId, Seq[FlowCell[String]])] = List(
-      (EntityId("0row"), slice("family:column2", (0L, "0 boop"))),
-      (EntityId("2row"), slice("family:column2", (1L, "1 cat")))
-      )
-
-    // Validate output.
-    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
-      assert(outputBuffer.size === 2)
-    }
-
-    // Create the JobTest for this test.
-    val jobTest = JobTest(new JoinHashedEntityIdsJob(_))
-      .arg("input1", uri)
-      .arg("input2", uri)
-      .arg("output", "outputFile")
-      .source(KijiInput(uri, ("family:column1" -> 'animals)), joinInput1)
-      .source(KijiInput(uri, ("family:column2" -> 'slice)), joinInput2)
-      .sink(Tsv("outputFile"))(validateTest)
-
-    // Run the test in local mode.
-    jobTest.run.finish
-  }
-
-  test("Runs a job that joins two pipes, on EntityIds from a table (hashed), in hadoop mode.") {
-    // URI of the hashed Kiji table to use.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Create input from hashed Kiji table.
-    val joinInput1: List[(EntityId, Seq[FlowCell[String]])] = List(
-      (EntityId("0row"), slice("family:column1", (0L, "0 dogs"))),
-      (EntityId("1row"), slice("family:column1", (0L, "1 cat"))),
-      (EntityId("2row"), slice("family:column1", (0L, "2 fish"))))
-
-    // Create input from hashed Kiji table.
-    val joinInput2: List[(EntityId, Seq[FlowCell[String]])] = List(
-      (EntityId("0row"), slice("family:column2", (0L, "0 boop"))),
-      (EntityId("2row"), slice("family:column2", (0L, "2 beep"))))
-
-    // Validate output.
-    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
-      assert(outputBuffer.size === 2)
-    }
-
-    // Create the JobTest for this test.
-    val jobTest = JobTest(new JoinHashedEntityIdsJob(_))
-      .arg("input1", uri)
-      .arg("input2", uri)
-      .arg("output", "outputFile")
-      .source(KijiInput(uri, ("family:column1" -> 'animals)), joinInput1)
-      .source(KijiInput(uri, ("family:column2" -> 'slice)), joinInput2)
-      .sink(Tsv("outputFile"))(validateTest)
-
-    // Run the test in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-
-  test("A job that joins two pipes, on EntityIds from a table (formatted) in local mode.") {
-    // URI of a formatted Kiji table to use.
-    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Create input from formatted Kiji table.
-    val joinInput1: List[(EntityId, Seq[FlowCell[Int]])] = List(
-      (EntityId("0row"), mapSlice("searches", ("0column", 0L, 0))),
-      (EntityId("2row"), mapSlice("searches", ("0column", 0L, 2))))
-
-    // Create input from formatted Kiji table.
-    val joinInput2: List[(EntityId, Seq[FlowCell[String]])] = List(
-      (EntityId("0row"), mapSlice("animals", ("0column", 0L, "0 dogs"))),
-      (EntityId("1row"), mapSlice("animals", ("0column", 0L, "1 cat"))),
-      (EntityId("2row"), mapSlice("animals", ("0column", 0L, "2 fish"))))
-
-    // Validate output.
-    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
-      assert(outputBuffer.size === 2)
-    }
-
-    // Create the JobTest for this test.
-    val jobTest = JobTest(new JoinFormattedEntityIdsJob(_))
-      .arg("input1", uri)
-      .arg("input2", uri)
-      .arg("output", "outputFile")
-      .source(KijiInput(uri, ("searches" -> 'searches)), joinInput1)
-      .source(KijiInput(uri, ("animals" -> 'animals)), joinInput2)
-      .sink(Tsv("outputFile"))(validateTest)
-
-    // Run the test in local mode.
-    jobTest.run.finish
-  }
-
-  test("A job that joins two pipes, on EntityIds from a table (formatted) in hadoop mode.") {
-    // URI of a formatted Kiji table to use.
-    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Create input from formatted Kiji table.
-    val joinInput1: List[(EntityId, Seq[FlowCell[Int]])] = List(
-      (EntityId("0row"), mapSlice("searches", ("0column", 0L, 0))),
-      (EntityId("2row"), mapSlice("searches", ("0column", 0L, 2))))
-
-    // Create input from formatted Kiji table.
-    val joinInput2: List[(EntityId, Seq[FlowCell[String]])] = List(
-      (EntityId("0row"), mapSlice("animals", ("0column", 0L, "0 dogs"))),
-      (EntityId("1row"), mapSlice("animals", ("0column", 0L, "1 cat"))),
-      (EntityId("2row"), mapSlice("animals", ("0column", 0L, "2 fish"))))
-
-    // Validate output.
-    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
-      assert(outputBuffer.size === 2)
-    }
-
-    // Create the JobTest for this test.
-    val jobTest = JobTest(new JoinFormattedEntityIdsJob(_))
-      .arg("input1", uri)
-      .arg("input2", uri)
-      .arg("output", "outputFile")
-      .source(KijiInput(uri, ("searches" -> 'searches)), joinInput1)
-      .source(KijiInput(uri, ("animals" -> 'animals)), joinInput2)
-      .sink(Tsv("outputFile"))(validateTest)
-
-    // Run the test in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-}
-
-/** Companion object for EntityIdSuite. Contains test jobs. */
-object EntityIdSuite {
-  /**
-   * A job that tests joining two pipes, on user-constructed EntityIds.
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
-   *     Kiji table, and "output", which specifies the path to a text file.
-   */
-  class JoinUserEntityIdsJob(args: Args) extends KijiJob(args) {
-    val sidePipe = TextLine(args("side-input"))
-      .read
-      .map('line -> 'entityId) { line: String => EntityId(line) }
-      .project('entityId)
-
-    TextLine(args("input"))
-      .map('line -> 'entityId) { line: String => EntityId(line) }
-      .joinWithSmaller('entityId -> 'entityId, sidePipe)
-      .write(Tsv(args("output")))
-  }
-
-  /**
-   * A job that tests joining two pipes, one with a user-constructed EntityId and one with
-   * a formatted EntityId from a Kiji table.
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
-   *     Kiji table, and "output", which specifies the path to a text file.
-   */
-  class JoinUserAndFormattedFromTableJob(args: Args) extends KijiJob(args) {
-    val sidePipe = TextLine(args("side-input"))
-      .read
-      .map('line -> 'entityId) { line: String => EntityId(line) }
-
-    KijiInput(args("input"), ("animals" -> 'animals))
-
-      .map('animals -> 'terms) { animals: Seq[FlowCell[CharSequence]] => animals.toString }
-      .joinWithSmaller('entityId -> 'entityId, sidePipe)
-      .write(Tsv(args("output")))
-  }
-
-  /**
-   * A job that tests joining two pipes, one with a user-constructed EntityId and one with
-   * a hashed EntityId from a Kiji table.
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
-   *     Kiji table, and "output", which specifies the path to a text file.
-   */
-  class JoinUserAndHashedFromTableJob(args: Args) extends KijiJob(args) {
-    val sidePipe = TextLine(args("side-input"))
-      .read
-      .map('line -> 'entityId) { line: String => EntityId(line) }
-      .project('entityId)
-
-    KijiInput(args("input"), ("family:column1" -> 'slice))
-      .map('slice -> 'terms) { slice: Seq[FlowCell[CharSequence]] => slice.head.datum.toString }
-      .joinWithSmaller('entityId -> 'entityId, sidePipe)
-      .write(Tsv(args("output")))
-  }
-
-  /**
-   * A job that tests joining two pipes, on EntityIds from a table with row key format HASHED.
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
-   *     Kiji table, and "output", which specifies the path to a text file.
-   */
-  class JoinHashedEntityIdsJob(args: Args) extends KijiJob(args) {
-    val pipe1 = KijiInput(args("input1"), ("family:column1" -> 'animals))
-
-    KijiInput(args("input2"), ("family:column2" -> 'slice))
-      .map('animals -> 'animal) { slice: Seq[FlowCell[CharSequence]] => slice.head.datum.toString }
-
-    KijiInput(args("input2"), ("family:column2" -> 'slice))
-      .map('slice -> 'terms) { slice:Seq[FlowCell[CharSequence]] => slice.head.datum.toString }
-      .joinWithSmaller('entityId -> 'entityId, pipe1)
-      .write(Tsv(args("output")))
-  }
-
-  /**
-   * A job that tests joining two pipes, on EntityIds from a table with row key format formatted.
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
-   *     Kiji table, and "output", which specifies the path to a text file.
-   */
-  class JoinFormattedEntityIdsJob(args: Args) extends KijiJob(args) {
-    val pipe1 = KijiInput(args("input1"), ("searches" -> 'searches))
-      .map('searches -> 'term) { slice:Seq[FlowCell[Int]] => slice.head.datum }
-
-    KijiInput(args("input2"), ("animals" -> 'animals))
-      .map('animals -> 'animal) { slice: Seq[FlowCell[CharSequence]] => slice.head.datum.toString }
-      .joinWithSmaller('entityId -> 'entityId, pipe1)
-      .write(Tsv(args("output")))
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/FlowModuleSuite.scala b/src/test/scala/org/kiji/express/flow/FlowModuleSuite.scala
deleted file mode 100644
index 9dd9dcd70a28a7c59d9b5dfb80610ddba08253fc..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/FlowModuleSuite.scala
+++ /dev/null
@@ -1,179 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.flow.framework.KijiScheme
-import org.kiji.schema.KijiInvalidNameException
-
-@RunWith(classOf[JUnitRunner])
-class FlowModuleSuite extends FunSuite {
-  val tableURI = "kiji://.env/default/table"
-
-  test("Flow module forbids creating an input map-type column with a qualifier in the column "
-      + "name.") {
-    intercept[KijiInvalidNameException] {
-      new ColumnFamilyInputSpec("info:word")
-    }
-  }
-
-  test("Flow module forbids creating an output map-type column with a qualifier in the column "
-      + "name.") {
-    intercept[KijiInvalidNameException] {
-      new ColumnFamilyOutputSpec("info:word", 'foo)
-    }
-  }
-
-  test("Flow module permits creating an output map-type column specifying the qualifier field") {
-    new ColumnFamilyOutputSpec("searches", 'terms)
-  }
-
-  test("Flow module permits specifying a qualifier regex on ColumnFamilyInputSpec.") {
-    val colReq = new ColumnFamilyInputSpec(
-        "search",
-        filter = Some(RegexQualifierFilterSpec(""".*\.com"""))
-    )
-
-    // TODO: Test it filters keyvalues correctly.
-    assert(colReq.filter.get.isInstanceOf[RegexQualifierFilterSpec])
-  }
-
-  test("Flow module permits specifying a qualifier regex (with filter) on ColumnFamilyInputSpec.") {
-    val colReq = new ColumnFamilyInputSpec("search",
-      filter=Some(RegexQualifierFilterSpec(""".*\.com""")))
-
-    // TODO: Test it filters keyvalues correctly.
-    assert(colReq.filter.get.isInstanceOf[RegexQualifierFilterSpec])
-  }
-
-  test("Flow module permits specifying versions on map-type columns without qualifier regex.") {
-    val colReq = ColumnFamilyInputSpec("search", maxVersions=2)
-    assert(2 === colReq.maxVersions)
-  }
-
-  test("Flow module permits specifying versions on a group-type column.") {
-    val colReq = QualifiedColumnInputSpec("info", "word", maxVersions=2)
-    assert(2 === colReq.maxVersions)
-  }
-
-  test("Flow module uses default versions of 1 for all ColumnInputSpecs.") {
-    val groupReq = QualifiedColumnInputSpec("info", "word")
-    val mapReq = ColumnFamilyInputSpec("searches")
-
-    assert(1 === groupReq.maxVersions)
-    assert(1 === mapReq.maxVersions)
-  }
-
-  test("Flow module permits creating inputs and outputs with no mappings.") {
-    val input: KijiSource = KijiInput(tableURI, columns = Map[ColumnInputSpec, Symbol]())
-    val output: KijiSource = KijiOutput(tableURI, columns = Map[Symbol, ColumnOutputSpec]())
-
-    assert(input.inputColumns.isEmpty)
-    assert(input.outputColumns.isEmpty)
-    assert(output.inputColumns.isEmpty)
-    assert(output.outputColumns.isEmpty)
-  }
-
-  test("Flow module permits creating KijiSources as inputs with default options.") {
-    val input: KijiSource = KijiInput(tableURI, "info:word" -> 'word)
-    val expectedScheme = new KijiScheme(
-        timeRange = All,
-        timestampField = None,
-        inputColumns = Map("word" -> QualifiedColumnInputSpec("info", "word")))
-
-    assert(expectedScheme === input.hdfsScheme)
-  }
-
-  test("Flow module permits specifying timerange for KijiInput.") {
-    val input = KijiInput(tableURI, timeRange=Between(0L,40L), columns="info:word" -> 'word)
-    val expectedScheme = new KijiScheme(
-        Between(0L, 40L),
-        None,
-        Map("word" -> QualifiedColumnInputSpec("info", "word")))
-
-    assert(expectedScheme === input.hdfsScheme)
-  }
-
-  test("Flow module permits creating KijiSources with multiple columns.") {
-    val input: KijiSource = KijiInput(tableURI, "info:word" -> 'word, "info:title" -> 'title)
-    val expectedScheme: KijiScheme = {
-      new KijiScheme(
-          All,
-          None,
-          Map(
-              "word" -> QualifiedColumnInputSpec("info", "word"),
-              "title" -> QualifiedColumnInputSpec("info", "title")))
-    }
-
-    assert(expectedScheme === input.hdfsScheme)
-  }
-
-  test("Flow module permits specifying options for a column.") {
-    KijiInput(
-        tableURI,
-        Map(QualifiedColumnInputSpec("info", "word") -> 'word)
-    )
-
-    KijiInput(
-        tableURI,
-        Map(QualifiedColumnInputSpec("info", "word", maxVersions = 1) -> 'word)
-    )
-
-    KijiInput(
-        tableURI,
-        Map(
-            ColumnFamilyInputSpec(
-                "searches",
-                maxVersions = 1,
-                filter = Some(new RegexQualifierFilterSpec(".*"))
-            ) -> 'word
-        )
-    )
-  }
-
-  test("Flow module permits specifying different options for different columns.") {
-    KijiInput(
-        tableURI,
-        Map(
-            QualifiedColumnInputSpec("info", "word", maxVersions = 1) -> 'word,
-            QualifiedColumnInputSpec("info", "title", maxVersions = 2) -> 'title))
-  }
-
-  test("Flow module permits creating KijiSource with the default timestamp field") {
-    val output: KijiSource = KijiOutput(tableURI, 'words -> "info:words")
-    val expectedScheme: KijiScheme = new KijiScheme(
-        timeRange = All,
-        timestampField = None,
-        outputColumns = Map("words" -> QualifiedColumnOutputSpec("info", "words")))
-    assert(expectedScheme === output.hdfsScheme)
-  }
-
-  test("Flow module permits creating KijiSource with a timestamp field") {
-    val output: KijiSource = KijiOutput(tableURI, 'time, 'words -> "info:words")
-    val expectedScheme: KijiScheme = new KijiScheme(
-        timeRange = All,
-        timestampField = Some('time),
-        outputColumns = Map("words" -> QualifiedColumnOutputSpec("info", "words")))
-    assert(expectedScheme === output.hdfsScheme)
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/KijiJobSuite.scala b/src/test/scala/org/kiji/express/flow/KijiJobSuite.scala
deleted file mode 100644
index 5286694c226197299634f8c60aca14cc66aa9e8d..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/KijiJobSuite.scala
+++ /dev/null
@@ -1,274 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import scala.collection.mutable
-
-import cascading.tuple.Fields
-import com.twitter.scalding.Args
-import com.twitter.scalding.JobTest
-import com.twitter.scalding.TextLine
-import com.twitter.scalding.Tsv
-import org.apache.avro.generic.GenericRecord
-import org.apache.avro.generic.GenericRecordBuilder
-import org.apache.avro.specific.SpecificRecord
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.avro.SimpleRecord
-import org.kiji.express.flow.util.Resources.doAndRelease
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiURI
-import org.kiji.schema.layout.KijiTableLayout
-
-@RunWith(classOf[JUnitRunner])
-class KijiJobSuite extends KijiSuite {
-  val avroLayout: KijiTableLayout = layout("layout/avro-types.json")
-  val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
-    table.getURI().toString()
-  }
-
-  val rawInputs: List[(Long, String)] = List(
-    (1, "input 1"),
-    (2, "input 2"),
-    (3, "input 3"),
-    (4, "input 4"),
-    (5, "input 5"))
-
-  val eids: List[EntityId] = List("row-1", "row-2", "row-3", "row-4", "row-5").map(EntityId(_))
-
-  val genericInputs: List[GenericRecord] = {
-    val builder = new GenericRecordBuilder(SimpleRecord.getClassSchema)
-    rawInputs.map { case (l: Long, s: String) => builder.set("l", l).set("s", s).build }
-  }
-
-  val specificInputs: List[SimpleRecord] = {
-    val builder = SimpleRecord.newBuilder()
-    rawInputs.map { case (l: Long, s: String) => builder.setL(l).setS(s).build }
-  }
-
-  def validateUnpacking(output: mutable.Buffer[(Long, String, String)]): Unit = {
-    val inputMap = rawInputs.toMap
-    output.foreach { case (l: Long, s: String, o: String) =>
-      assert(inputMap(l) === s)
-      assert("default-value" === o)
-    }
-  }
-
-  test("A KijiJob can pack a generic Avro record.") {
-    def validatePacking(outputs: mutable.Buffer[(EntityId, Seq[FlowCell[GenericRecord]])]) {
-      val inputMap = rawInputs.toMap
-      outputs.foreach { case (_: EntityId, slice: Seq[FlowCell[GenericRecord]]) =>
-        val record = slice.head.datum
-        assert(inputMap(record.get("l").asInstanceOf[Long]) === record.get("s"))
-        assert("default-value" === record.get("o"))
-      }
-    }
-
-    val jobTest = JobTest(new PackGenericRecordJob(_))
-        .arg("input", "inputFile")
-        .arg("uri", uri)
-        .source(Tsv("inputFile", fields = new Fields("l", "s")), rawInputs)
-        .sink(KijiOutput(uri, 'record -> "family:simple"))(validatePacking)
-
-    // Run in local mode.
-    jobTest.run.finish
-    // Run in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-
-  test("A KijiJob can pack a specific Avro record.") {
-    def validatePacking(outputs: mutable.Buffer[(EntityId, Seq[FlowCell[SimpleRecord]])]) {
-      val inputMap = rawInputs.toMap
-      outputs.foreach { case (_: EntityId, slice: Seq[FlowCell[SimpleRecord]]) =>
-        val record = slice.head.datum
-        assert(inputMap(record.getL) === record.getS)
-        assert("default-value" === record.getO)
-      }
-    }
-
-    val jobTest = JobTest(new PackSpecificRecordJob(_))
-        .arg("input", "inputFile")
-        .arg("uri", uri)
-        .source(Tsv("inputFile", fields = new Fields("l", "s")), rawInputs)
-        .sink(KijiOutput(uri,
-            Map('record ->
-                QualifiedColumnOutputSpec("family", "simple", classOf[SimpleRecord]))))(
-                    validatePacking)
-
-    // Run in local mode.
-    jobTest.run.finish
-    // Run in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-
-  test("A KijiJob can unpack a generic record.") {
-    val slices: List[Seq[FlowCell[GenericRecord]]] = genericInputs.map { record: GenericRecord =>
-      List(FlowCell("family", "simple", datum = record))
-    }
-    val input: List[(EntityId, Seq[FlowCell[GenericRecord]])] = eids.zip(slices)
-
-    val jobTest = JobTest(new UnpackGenericRecordJob(_))
-        .arg("input", uri)
-        .arg("output", "outputFile")
-        .source(KijiInput(uri,
-            Map(QualifiedColumnInputSpec("family", "simple", SimpleRecord.getClassSchema)
-                -> 'slice)), input)
-        .sink(Tsv("outputFile"))(validateUnpacking)
-
-    // Run in local mode.
-    jobTest.run.finish
-    // Run in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-
-  test("A KijiJob can unpack a specific record.") {
-    val slices: List[Seq[FlowCell[SpecificRecord]]] = specificInputs
-        .map { record: SpecificRecord =>
-          List(FlowCell("family", "simple", datum = record))
-        }
-    val input: List[(EntityId, Seq[FlowCell[SpecificRecord]])] = eids.zip(slices)
-
-    val jobTest = JobTest(new UnpackSpecificRecordJob(_))
-        .arg("input", uri)
-        .arg("output", "outputFile")
-        .source(KijiInput(uri,
-      Map(QualifiedColumnInputSpec("family", "simple", classOf[SimpleRecord])
-          -> 'slice)), input)
-        .sink(Tsv("outputFile"))(validateUnpacking)
-
-    // Run in local mode.
-    jobTest.run.finish
-    // Run in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-
-  test("A KijiJob is not run if the Kiji instance in the output doesn't exist.") {
-    class BasicJob(args: Args) extends KijiJob(args) {
-      TextLine(args("input"))
-        .map ('line -> 'entityId) { line: String => EntityId(line) }
-        .write(KijiOutput(args("output"), 'line -> "family:column1"))
-    }
-
-    val nonexistentInstanceURI: String = KijiURI.newBuilder(uri)
-        .withInstanceName("nonexistent_instance")
-        .build()
-        .toString
-
-    val basicInput: List[(String, String)] = List[(String, String)]()
-
-    def validateBasicJob(outputBuffer: mutable.Buffer[String]) { /** Nothing to validate. */ }
-
-    val jobTest = JobTest(new BasicJob(_))
-        .arg("input", "inputFile")
-        .arg("output", nonexistentInstanceURI)
-        .source(TextLine("inputFile"), basicInput)
-        .sink(KijiOutput(nonexistentInstanceURI, 'line -> "family:column1"))(validateBasicJob)
-
-    val hadoopException = intercept[InvalidKijiTapException] { jobTest.runHadoop.finish }
-    val localException = intercept[InvalidKijiTapException] { jobTest.run.finish }
-
-    assert(localException.getMessage === hadoopException.getMessage)
-    assert(localException.getMessage.contains("nonexistent_instance"))
-  }
-
-  test("A KijiJob is not run if the Kiji table in the output doesn't exist.") {
-    class BasicJob(args: Args) extends KijiJob(args) {
-      TextLine(args("input"))
-        .write(KijiOutput(args("output"), 'line -> "family:column1"))
-    }
-
-    val nonexistentTableURI: String = KijiURI.newBuilder(uri)
-        .withTableName("nonexistent_table")
-        .build()
-        .toString
-
-    val basicInput: List[(String, String)] = List[(String, String)]()
-
-    def validateBasicJob(outputBuffer: mutable.Buffer[String]) { /** Nothing to validate. */ }
-
-    val jobTest = JobTest(new BasicJob(_))
-        .arg("input", "inputFile")
-        .arg("output", nonexistentTableURI)
-        .source(TextLine("inputFile"), basicInput)
-        .sink(KijiOutput(nonexistentTableURI, 'line -> "family:column1"))(validateBasicJob)
-
-    val localException = intercept[InvalidKijiTapException] { jobTest.run.finish }
-    val hadoopException = intercept[InvalidKijiTapException] { jobTest.runHadoop.finish }
-
-    assert(localException.getMessage === hadoopException.getMessage)
-    assert(localException.getMessage.contains("nonexistent_table"))
-  }
-
-  test("A KijiJob is not run if any of the columns don't exist.") {
-    class BasicJob(args: Args) extends KijiJob(args) {
-      TextLine(args("input"))
-        .write(KijiOutput(args("output"), 'line -> "family:nonexistent_column"))
-    }
-
-    val basicInput: List[(String, String)] = List[(String, String)]()
-
-    def validateBasicJob(outputBuffer: mutable.Buffer[String]) { /** Nothing to validate. */ }
-
-    val jobTest = JobTest(new BasicJob(_))
-        .arg("input", "inputFile")
-        .arg("output", uri)
-        .source(TextLine("inputFile"), basicInput)
-        .sink(KijiOutput(uri, 'line -> "family:nonexistent_column"))(validateBasicJob)
-
-    val localException = intercept[InvalidKijiTapException] { jobTest.run.finish }
-    val hadoopException = intercept[InvalidKijiTapException] { jobTest.runHadoop.finish }
-
-    assert(localException.getMessage === hadoopException.getMessage)
-    assert(localException.getMessage.contains("nonexistent_column"))
-  }
-}
-
-class PackGenericRecordJob(args: Args) extends KijiJob(args) {
-  Tsv(args("input"), fields = ('l, 's)).read
-      .packGenericRecordTo(('l, 's) -> 'record)(SimpleRecord.getClassSchema)
-      .insert('entityId, EntityId("foo"))
-      .write(KijiOutput(args("uri"), 'record -> "family:simple"))
-}
-
-class PackSpecificRecordJob(args: Args) extends KijiJob(args) {
-  Tsv(args("input"), fields = ('l, 's)).read
-      .packTo[SimpleRecord](('l, 's) -> 'record)
-      .insert('entityId, EntityId("foo"))
-      .write(KijiOutput(args("uri"),
-          Map('record -> QualifiedColumnOutputSpec("family", "simple", classOf[SimpleRecord]))))
-}
-
-class UnpackGenericRecordJob(args: Args) extends KijiJob(args) {
-  KijiInput(args("input"),
-      Map(QualifiedColumnInputSpec("family", "simple", SimpleRecord.getClassSchema) -> 'slice))
-      .mapTo('slice -> 'record) { slice: Seq[FlowCell[GenericRecord]] => slice.head.datum }
-      .unpackTo[GenericRecord]('record -> ('l, 's, 'o))
-      .write(Tsv(args("output")))
-}
-
-class UnpackSpecificRecordJob(args: Args) extends KijiJob(args) {
-  KijiInput(args("input"),
-      Map(QualifiedColumnInputSpec("family", "simple", classOf[SimpleRecord]) -> 'slice))
-      .map('slice -> 'record) { slice: Seq[FlowCell[SimpleRecord]] => slice.head.datum }
-      .unpackTo[SimpleRecord]('record -> ('l, 's, 'o))
-      .write(Tsv(args("output")))
-}
diff --git a/src/test/scala/org/kiji/express/flow/KijiPipeSuite.scala b/src/test/scala/org/kiji/express/flow/KijiPipeSuite.scala
deleted file mode 100644
index 0bbd53730b92a88049db9bc69b41b14f3c206412..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/KijiPipeSuite.scala
+++ /dev/null
@@ -1,103 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import scala.collection.mutable.Buffer
-
-import com.twitter.scalding.Args
-import com.twitter.scalding.Job
-import com.twitter.scalding.JobTest
-import com.twitter.scalding.Tsv
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.flow.util.Resources._
-import org.kiji.express.repl.Implicits._
-import org.kiji.schema.KijiTable
-import org.kiji.schema.layout.KijiTableLayout
-import org.kiji.schema.layout.KijiTableLayouts
-
-@RunWith(classOf[JUnitRunner])
-class KijiPipeSuite extends KijiSuite {
-  /** Table layout to use for tests. */
-  val layout: KijiTableLayout = layout(KijiTableLayouts.SIMPLE_TWO_COLUMNS)
-
-  /** Input tuples to use for word count tests. */
-  def wordCountInput(uri: String): List[(EntityId, Seq[FlowCell[String]])] = {
-    List(
-      ( EntityId("row01"), slice("family:column1", (1L, "hello")) ),
-      ( EntityId("row02"), slice("family:column1", (2L, "hello")) ),
-      ( EntityId("row03"), slice("family:column1", (1L, "world")) ),
-      ( EntityId("row04"), slice("family:column1", (3L, "hello")) ))
-  }
-
-  /**
-   * Validates output from the word count tests.
-
-   * @param outputBuffer containing data that output buffer has in it after the job has been run.
-   */
-  def validateWordCount(outputBuffer: Buffer[(String, Int)]) {
-    val outMap = outputBuffer.toMap
-
-    // Validate that the output is as expected.
-    assert(3 === outMap("hello"))
-    assert(1 === outMap("world"))
-  }
-
-  // Create test Kiji table.
-  val uri: String = doAndRelease(makeTestKijiTable(layout)) { table: KijiTable =>
-    table.getURI().toString()
-  }
-
-  // A name for a dummy file for test job output.
-  val outputFile: String = "outputFile"
-
-  // A job obtained by converting a Cascading Pipe to a KijiPipe, which is then used to obtain
-  // a Scalding Job from the pipe.
-  def jobToRun(args: Args): Job = {
-    // Setup input to bind values from the "family:column1" column to the symbol 'word.
-    KijiInput(uri, "family:column1" -> 'word)
-    // Sanitize the word.
-    .map('word -> 'cleanword) { words: Seq[FlowCell[CharSequence]] =>
-      words.head.datum
-          .toString()
-          .toLowerCase()
-    }
-    // Count the occurrences of each word.
-    .groupBy('cleanword) { occurences => occurences.size }
-    // Write the result to a file.
-    .write(Tsv("outputFile"))
-    .getJob(args)
-  }
-
-  /** The job tester we'll use to run the test job in either local or hadoop mode. */
-  val jobTest = JobTest(jobToRun(_))
-      .source(KijiInput(uri, "family:column1" -> 'word), wordCountInput(uri))
-      .sink(Tsv("outputFile"))(validateWordCount)
-
-  test("A KijiPipe can be used to obtain a Scalding job that is run in local mode.") {
-    jobTest.run.finish
-  }
-
-  test("A KijiPipe can be used to obtain a Scalding job that is run with Hadoop.") {
-    jobTest.runHadoop.finish
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/KijiSourceSuite.scala b/src/test/scala/org/kiji/express/flow/KijiSourceSuite.scala
deleted file mode 100644
index be7802f6dc8d9fa468ee6c3c4d0686082fa20ca0..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/KijiSourceSuite.scala
+++ /dev/null
@@ -1,995 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import java.util.UUID
-
-import scala.collection.mutable.Buffer
-
-import cascading.tuple.Fields
-import com.twitter.scalding.Args
-import com.twitter.scalding.JobTest
-import com.twitter.scalding.TextLine
-import com.twitter.scalding.Tsv
-import org.apache.avro.generic.GenericRecord
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.avro.SimpleRecord
-import org.kiji.express.flow.SchemaSpec.Specific
-import org.kiji.express.flow.util.Resources.doAndRelease
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiTable
-import org.kiji.schema.avro.AvroValidationPolicy
-import org.kiji.schema.avro.HashSpec
-import org.kiji.schema.avro.HashType
-import org.kiji.schema.avro.TableLayoutDesc
-import org.kiji.schema.avro.TestRecord
-import org.kiji.schema.layout.KijiTableLayout
-import org.kiji.schema.layout.KijiTableLayouts
-import org.kiji.schema.layout.TableLayoutBuilder
-
-@RunWith(classOf[JUnitRunner])
-class KijiSourceSuite
-    extends KijiSuite {
-  import KijiSourceSuite._
-
-  /** Simple table layout to use for tests. The row keys are hashed. */
-  val simpleLayout: KijiTableLayout = layout(KijiTableLayouts.SIMPLE_TWO_COLUMNS)
-
-  /** Table layout using Avro schemas to use for tests. The row keys are formatted. */
-  val avroLayout: KijiTableLayout = layout("layout/avro-types.json")
-
-  /** Input tuples to use for word count tests. */
-  def wordCountInput(uri: String): List[(EntityId, Seq[FlowCell[String]])] = {
-    List(
-        ( EntityId("row01"), slice("family:column1", (1L, "hello")) ),
-        ( EntityId("row02"), slice("family:column1", (2L, "hello")) ),
-        ( EntityId("row03"), slice("family:column1", (1L, "world")) ),
-        ( EntityId("row04"), slice("family:column1", (3L, "hello")) ))
-  }
-
-  /**
-   * Validates output from [[com.twitter.scalding.examples.WordCountJob]].
-   *
-   * @param outputBuffer containing data that output buffer has in it after the job has been run.
-   */
-  def validateWordCount(outputBuffer: Buffer[(String, Int)]) {
-    val outMap = outputBuffer.toMap
-
-    // Validate that the output is as expected.
-    assert(3 === outMap("hello"))
-    assert(1 === outMap("world"))
-  }
-
-  test("a word-count job that reads from a Kiji table is run using Scalding's local mode") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Build test job.
-    JobTest(new WordCountJob(_))
-        .arg("input", uri)
-        .arg("output", "outputFile")
-        .source(KijiInput(uri, "family:column1" -> 'word), wordCountInput(uri))
-        .sink(Tsv("outputFile"))(validateWordCount)
-        // Run the test job.
-        .run
-        .finish
-  }
-
-  test("a word-count job that reads from a Kiji table is run using Hadoop") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Build test job.
-    JobTest(new WordCountJob(_))
-        .arg("input", uri)
-        .arg("output", "outputFile")
-        .source(KijiInput(uri, "family:column1" -> 'word), wordCountInput(uri))
-        .sink(Tsv("outputFile"))(validateWordCount)
-        // Run the test job.
-        .runHadoop
-        .finish
-  }
-
-  /** Input tuples to use for import tests. */
-  val importMultipleTimestamps: List[(String, String)] = List(
-      ( "0", "1 eid1 word1" ),
-      ( "1", "3 eid1 word2" ),
-      ( "2", "5 eid2 word3" ),
-      ( "3", "7 eid2 word4" ))
-
-  /**
-   * Validates output from [[org.kiji.express.flow.KijiSourceSuite.ImportJob]].
-   *
-   * @param outputBuffer containing data that the Kiji table has in it after the job has been run.
-   */
-
-  def validateMultipleTimestamps(outputBuffer: Buffer[(EntityId, Seq[FlowCell[CharSequence]])]) {
-    assert(outputBuffer.size === 2)
-
-    // There should be two Cells written to each column.
-    assert(outputBuffer(0)._2.size === 2)
-    assert(outputBuffer(1)._2.size === 2)
-
-    assert(outputBuffer(0)._2.head.datum.toString === "word2")
-    assert(outputBuffer(0)._2.last.datum.toString === "word1")
-    assert(outputBuffer(1)._2.head.datum.toString === "word4")
-    assert(outputBuffer(1)._2.last.datum.toString === "word3")
-  }
-
-  test("An import job with multiple timestamps imports all timestamps in local mode.") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Build test job.
-    JobTest(new MultipleTimestampsImportJob(_))
-        .arg("input", "inputFile")
-        .arg("output", uri)
-        .source(TextLine("inputFile"), importMultipleTimestamps)
-        .sink(KijiOutput(uri, 'timestamp, 'word -> "family:column1"))(validateMultipleTimestamps)
-        // Run the test job.
-        .run
-        .finish
-  }
-
-  test("An import with multiple timestamps imports all timestamps using Hadoop.") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Build test job.
-    JobTest(new MultipleTimestampsImportJob(_))
-        .arg("input", "inputFile")
-        .arg("output", uri)
-        .source(TextLine("inputFile"), importMultipleTimestamps)
-        .sink(KijiOutput(uri, 'timestamp, 'word -> "family:column1"))(validateMultipleTimestamps)
-        // Run the test job.
-        .run
-        .finish
-  }
-
-  /** Input tuples to use for import tests. */
-  val importInput: List[(String, String)] = List(
-      ( "0", "hello hello hello world world hello" ),
-      ( "1", "world hello   world      hello" ))
-
-  /**
-   * Validates output from [[org.kiji.express.flow.KijiSourceSuite.ImportJob]].
-   *
-   * @param outputBuffer containing data that the Kiji table has in it after the job has been run.
-   */
-
-  def validateImport(outputBuffer: Buffer[(EntityId, Seq[FlowCell[CharSequence]])]) {
-    assert(10 === outputBuffer.size)
-
-    // Perform a non-distributed word count.
-    val wordCounts: (Int, Int) = outputBuffer
-        // Extract words from each row.
-        .flatMap { row =>
-          val (_, slice) = row
-          slice
-              .map { cell: FlowCell[CharSequence] =>
-              cell.datum
-              }
-        }
-        // Count the words.
-        .foldLeft((0, 0)) { (counts, word) =>
-          // Unpack the counters.
-          val (helloCount, worldCount) = counts
-
-          // Increment the appropriate counter and return both.
-          word.toString() match {
-            case "hello" => (helloCount + 1, worldCount)
-            case "world" => (helloCount, worldCount + 1)
-          }
-        }
-
-    // Make sure that the counts are as expected.
-    assert((6, 4) === wordCounts)
-  }
-
-  test("an import job that writes to a Kiji table is run using Scalding's local mode") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Build test job.
-    JobTest(new ImportJob(_))
-        .arg("input", "inputFile")
-        .arg("output", uri)
-        .source(TextLine("inputFile"), importInput)
-        .sink(KijiOutput(uri, 'word -> "family:column1"))(validateImport)
-        // Run the test job.
-        .run
-        .finish
-  }
-
-  test("an import job that writes to a Kiji table is run using Hadoop") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Build test job.
-    JobTest(new ImportJob(_))
-        .arg("input", "inputFile")
-        .arg("output", uri)
-        .source(TextLine("inputFile"), importInput)
-        .sink(KijiOutput(uri, 'word -> "family:column1"))(validateImport)
-        // Run the test job.
-        .runHadoop
-        .finish
-  }
-
-  val importWithTimeInput = (0L, "Line-0") :: (1L, "Line-1") :: (2L, "Line-2") :: Nil
-
-  /**
-   * Validates output from [[org.kiji.express.flow.KijiSourceSuite.ImportJobWithTime]].
-   *
-   * @param outputBuffer containing data that the Kiji table has in it after the job has been run.
-   */
-  def validateImportWithTime(outputBuffer: Buffer[(EntityId, Seq[FlowCell[CharSequence]])]) {
-    // There should be one cell per row in the output.
-    val cellsPerRow = outputBuffer.unzip._2.map { m => (m.head.version, m.head.datum) }
-    // Sort by timestamp.
-    val cellsSortedByTime = cellsPerRow.sortBy { case(ts, line) => ts }
-    // Verify contents.
-    (0 until 2).foreach { index =>
-      val cell = cellsSortedByTime(index)
-      assert(index.toLong === cell._1)
-      assert("Line-" + index === cell._2.toString)
-    }
-  }
-
-  test("an import job that writes to a Kiji table with timestamps is run using local mode") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Build test job.
-    JobTest(new ImportJobWithTime(_))
-    .arg("input", "inputFile")
-    .arg("output", uri)
-    .source(TextLine("inputFile"), importWithTimeInput)
-    .sink(KijiOutput(uri, 'offset, 'line -> "family:column1"))(validateImportWithTime)
-    // Run the test job.
-    .run
-    .finish
-  }
-
-  test("an import job that writes to a Kiji table with timestamps is run using Hadoop") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Build test job.
-    JobTest(new ImportJobWithTime(_))
-    .arg("input", "inputFile")
-    .arg("output", uri)
-    .source(TextLine("inputFile"), importWithTimeInput)
-    .sink(KijiOutput(uri, 'offset, 'line -> "family:column1"))(validateImportWithTime)
-    // Run the test job.
-    .runHadoop
-    .finish
-  }
-
-  // Input tuples to use for version count tests.
-  def versionCountInput(uri: String): List[(EntityId, Seq[FlowCell[String]])] = {
-    List(
-        ( EntityId("row01"), slice("family:column1", (10L, "two"), (20L, "two")) ),
-        ( EntityId("row02"), slice("family:column1",
-            (10L, "three"),
-            (20L, "three"),
-            (30L, "three") ) ),
-        ( EntityId("row03"), slice("family:column1", (10L, "hello")) ))
-  }
-
-  test("a job that requests maxVersions gets them") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    def validateVersionCount(outputBuffer: Buffer[(Int, Int)]) {
-      val outMap = outputBuffer.toMap
-      // There should be two rows with 2 returned versions
-      // since one input row has 3 versions but we only requested two.
-      assert(1 == outMap(1))
-      assert(2 == outMap(2))
-    }
-
-    // Build test job.
-    val source =
-        KijiInput(uri, Map((ColumnInputSpec("family:column1", maxVersions=2) -> 'words)))
-    JobTest(new VersionsJob(source)(_))
-        .arg("output", "outputFile")
-        .source(source, versionCountInput(uri))
-        .sink(Tsv("outputFile"))(validateVersionCount)
-        // Run the test job.
-        .run
-        .finish
-  }
-
-  test("a job that requests a time range gets them") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-
-    def validateVersionCount(outputBuffer: Buffer[(Int, Int)]) {
-      val outMap = outputBuffer.toMap
-      // There should be two rows with 2 returned versions
-      // since one input row has 3 versions but we only requested two.
-      assert(1 === outMap.size)
-      assert(2 === outMap(1))
-    }
-
-    // Build test job.
-    val source = KijiInput(uri, Between(15L, 25L), "family:column1" -> 'words)
-    JobTest(new VersionsJob(source)(_))
-        .arg("output", "outputFile")
-        .source(source, versionCountInput(uri))
-        .sink(Tsv("outputFile"))(validateVersionCount)
-        // Run the test job.
-        .run
-        .finish
-  }
-
-  test("Specific records can be returned to JobTest's validate method") {
-    val uri: String = doAndRelease(makeTestKiji()) { kiji: Kiji =>
-      val baseDesc: TableLayoutDesc = KijiTableLayouts.getLayout(KijiTableLayouts.SCHEMA_REG_TEST)
-      baseDesc.setVersion("layout-1.3.0")
-
-      val desc = new TableLayoutBuilder(baseDesc, kiji)
-          .withAvroValidationPolicy(new KijiColumnName("info:fullname"), AvroValidationPolicy.NONE)
-          .build()
-
-      // Create test Kiji table.
-      doAndRelease {
-        kiji.createTable(desc)
-        kiji.openTable(desc.getName)
-      } { table: KijiTable =>
-        table.getURI.toString
-      }
-    }
-
-    // Build test job.
-    class TestSpecificRecordWriteJob(args: Args) extends KijiJob(args) {
-      Tsv(args("input"), ('entityId, 'fullname))
-          .write(
-              KijiOutput(
-                  tableUri = args("output"),
-                  columns = Map(
-                      'fullname -> QualifiedColumnOutputSpec(
-                          family = "info",
-                          qualifier = "fullname",
-                          schemaSpec = SchemaSpec.Specific(classOf[TestRecord])
-                      )
-                  )
-              )
-          )
-    }
-
-    val inputRecord1 = TestRecord
-        .newBuilder()
-        .setA("foo")
-        .setB(2)
-        .setC(4)
-        .build()
-    val inputRecord2 = TestRecord
-        .newBuilder()
-        .setA("bar")
-        .setB(1)
-        .setC(3)
-        .build()
-    val inputRecord3 = TestRecord
-        .newBuilder()
-        .setA("baz")
-        .setB(9)
-        .setC(8)
-        .build()
-    val inputRecords = Seq(
-        (EntityId("row01"), inputRecord1),
-        (EntityId("row02"), inputRecord2),
-        (EntityId("row03"), inputRecord3)
-    )
-
-    def validateSpecificWrite(outputBuffer: Buffer[(EntityId, Seq[FlowCell[TestRecord]])]) {
-      val outputMap = outputBuffer.toMap
-      assert(outputMap(EntityId("row01")).head.datum.getClass === classOf[TestRecord])
-      assert(outputMap(EntityId("row02")).head.datum.getClass === classOf[TestRecord])
-      assert(outputMap(EntityId("row03")).head.datum.getClass === classOf[TestRecord])
-      assert(outputMap(EntityId("row01")).head.datum === inputRecord1)
-      assert(outputMap(EntityId("row02")).head.datum === inputRecord2)
-      assert(outputMap(EntityId("row03")).head.datum === inputRecord3)
-    }
-
-    JobTest(new TestSpecificRecordWriteJob(_))
-        .arg("input", "inputFile")
-        .arg("output", uri)
-        .source(Tsv("inputFile", new Fields("entityId", "fullname")), inputRecords)
-        .sink(
-            KijiOutput(
-                uri,
-                Map(
-                    'fullname -> QualifiedColumnOutputSpec(
-                        family = "info",
-                        qualifier = "fullname",
-                        schemaSpec = SchemaSpec.Specific(classOf[TestRecord])
-                    )
-                )
-            )
-        ) (validateSpecificWrite)
-        .run
-        .runHadoop
-        .finish
-  }
-
-  // TODO(EXP-7): Write this test.
-  test("a word-count job that uses the type-safe api is run") {
-    pending
-  }
-
-  // TODO(EXP-6): Write this test.
-  test("a job that uses the matrix api is run") {
-    pending
-  }
-
-  test("test conversion of column value of type string between java and scala in Hadoop mode") {
-    def validateSimpleAvroChecker(outputBuffer: Buffer[(String, Int)]) {
-      val outMap = outputBuffer.toMap
-      // Validate that the output is as expected.
-      intercept[java.util.NoSuchElementException]{outMap("false")}
-      assert(6 === outMap("true"))
-    }
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-    // Input tuples to use for avro/scala conversion tests.
-    val avroCheckerInput: List[(EntityId, Seq[FlowCell[String]])] = List(
-        ( EntityId("row01"), slice("family:column1", (10L,"two"), (20L, "two")) ),
-        ( EntityId("row02"), slice("family:column1",
-            (10L, "three"),
-            (20L, "three"),
-            (30L, "three") ) ),
-        ( EntityId("row03"), slice("family:column1", (10L, "hello")) ))
-    // Build test job.
-    val testSource = KijiInput(
-        uri,
-        Map(ColumnInputSpec("family:column1", maxVersions=all) -> 'word))
-    JobTest(new AvroToScalaChecker(testSource)(_))
-      .arg("input", uri)
-      .arg("output", "outputFile")
-      .source(testSource, avroCheckerInput)
-      .sink(Tsv("outputFile"))(validateSimpleAvroChecker)
-    // Run the test job.
-      .runHadoop
-      .finish
-  }
-
-  test("A job that reads using the generic API is run.") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    val specificRecord = new HashSpec()
-    specificRecord.setHashType(HashType.MD5)
-    specificRecord.setHashSize(13)
-    specificRecord.setSuppressKeyMaterialization(true)
-    def genericReadInput(uri: String): List[(EntityId, Seq[FlowCell[HashSpec]])] = {
-      List((EntityId("row01"), slice("family:column3", (10L, specificRecord))))
-    }
-
-    def validateGenericRead(outputBuffer: Buffer[(Int, Int)]): Unit = {
-      assert (1 === outputBuffer.size)
-      // There exactly 1 record with hash_size of 13.
-      assert ((13, 1) === outputBuffer(0))
-    }
-
-    val jobTest = JobTest(new GenericAvroReadJob(_))
-        .arg("input", uri)
-        .arg("output", "outputFile")
-        .source(KijiInput(uri, Map (ColumnInputSpec("family:column3") -> 'records)),
-            genericReadInput(uri))
-        .sink(Tsv("outputFile"))(validateGenericRead)
-
-    // Run in local mode
-    jobTest.run.finish
-
-    // Run in hadoop mode
-    jobTest.runHadoop.finish
-  }
-
-  test("A job that reads using the specific API is run.") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    val specificRecord = new HashSpec()
-    specificRecord.setHashType(HashType.MD5)
-    specificRecord.setHashSize(13)
-    specificRecord.setSuppressKeyMaterialization(true)
-    def genericReadInput(uri: String): List[(EntityId, Seq[FlowCell[HashSpec]])] = {
-      List((EntityId("row01"), slice("family:column3", (10L, specificRecord))))
-    }
-
-    def validateSpecificRead(outputBuffer: Buffer[(Int, Int)]): Unit = {
-      assert (1 === outputBuffer.size)
-      // There exactly 1 record with hash_size of 13.
-      assert ((13, 1) === outputBuffer(0))
-    }
-
-    // Defining the KijiSource directly like this is unfortunate, but necessary to make sure that
-    // the KijiSource referenced here and the one used within SpecificAvroReadJob are identical (the
-    // KijiSources are used as keys for a map of buffers for test code).
-    val ksource = new KijiSource(
-        tableAddress = uri,
-        timeRange = All,
-        timestampField = None,
-        inputColumns = Map('records -> ColumnInputSpec(
-          "family:column3", schemaSpec = Specific(classOf[HashSpec]))),
-        outputColumns = Map('records -> QualifiedColumnOutputSpec("family:column3"))
-    )
-
-    val jobTest = JobTest(new SpecificAvroReadJob(_))
-        .arg("input", uri)
-        .arg("output", "outputFile")
-        .source(ksource, genericReadInput(uri))
-        .sink(Tsv("outputFile"))(validateSpecificRead)
-
-    // Run in local mode
-    jobTest.run.finish
-
-    // Run in hadoop mode
-    jobTest.runHadoop.finish
-  }
-
-  test("A job that writes using the generic API is run.") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Input to use with Text source.
-    val genericWriteInput: List[(String, String)] = List(
-        ( "0", "zero" ),
-        ( "1", "one" ))
-
-    // Validates the output buffer contains the same as the input buffer.
-    def validateGenericWrite(outputBuffer: Buffer[(EntityId, Seq[FlowCell[GenericRecord]])]) {
-      val inputMap: Map[Long, String] = genericWriteInput.map { t => t._1.toLong -> t._2 }.toMap
-      outputBuffer.foreach { t: (EntityId, Seq[FlowCell[GenericRecord]]) =>
-        val entityId = t._1
-        val record = t._2.head.datum
-
-        val s = record.get("s").asInstanceOf[String]
-        val l = record.get("l").asInstanceOf[Long]
-
-        assert(entityId(0) === s)
-        assert(inputMap(l) === s)
-      }
-    }
-
-    val jobTest = JobTest(new GenericAvroWriteJob(_))
-      .arg("input", "inputFile")
-      .arg("output", uri)
-      .source(TextLine("inputFile"), genericWriteInput)
-      .sink(KijiOutput(uri, 'record -> "family:column4"))(validateGenericWrite)
-
-    // Run in local mode.
-    jobTest.run.finish
-
-    // Run in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-
-  test ("A job that writes to map-type column families is run.") {
-    // URI of the Kiji table to use.
-    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Input text.
-    val mapTypeInput: List[(String, String)] = List(
-        ("0", "dogs 4"),
-        ("1", "cats 5"),
-        ("2", "fish 3"))
-
-    // Validate output.
-    def validateMapWrite(
-        outputBuffer: Buffer[(EntityId,Seq[FlowCell[GenericRecord]])]
-    ): Unit = {
-      assert (1 === outputBuffer.size)
-      val outputSlice = outputBuffer(0)._2
-      val outputSliceMap = outputSlice.groupBy(_.qualifier)
-      assert (4 === outputSliceMap("dogs").head.datum)
-      assert (5 === outputSliceMap("cats").head.datum)
-      assert (3 === outputSliceMap("fish").head.datum)
-    }
-
-    // Create the JobTest for this test.
-    val jobTest = JobTest(new MapWriteJob(_))
-        .arg("input", "inputFile")
-        .arg("table", uri)
-        .source(TextLine("inputFile"), mapTypeInput)
-        .sink(KijiOutput(uri, Map('resultCount ->
-            new ColumnFamilyOutputSpec("searches", 'terms))))(validateMapWrite)
-
-    // Run the test.
-    jobTest.run.finish
-    // Run the test in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-
-  test ("A job that writes to map-type column families with numeric column qualifiers is run.") {
-    // URI of the Kiji table to use.
-    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Create input using mapSlice.
-    val mapTypeInput: List[(EntityId, Seq[FlowCell[String]])] = List(
-        ( EntityId("0row"), mapSlice("animals", ("0column", 0L, "0 dogs")) ),
-        ( EntityId("1row"), mapSlice("animals", ("0column", 0L, "1 cat")) ),
-        ( EntityId("2row"), mapSlice("animals", ("0column", 0L, "2 fish")) ))
-
-    // Validate output.
-    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
-      assert(outputBuffer.size === 3)
-      val outputSet = outputBuffer.map { value: Tuple1[String] =>
-        value._1
-      }.toSet
-      assert (outputSet.contains("0 dogs"), "Failed on \"0 dogs\" test")
-      assert (outputSet.contains("1 cat"), "Failed on \"1 cat\" test")
-      assert (outputSet.contains("2 fish"), "Failed on \"2 fish\" test")
-    }
-
-    // Create the JobTest for this test.
-    val jobTest = JobTest(new MapSliceJob(_))
-        .arg("input", uri)
-        .arg("output", "outputFile")
-        .source(KijiInput(uri, "animals" -> 'terms), mapTypeInput)
-        .sink(Tsv("outputFile"))(validateTest)
-
-    // Run the test.
-    jobTest.run.finish
-    // Run the test in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-
-  test("A job that joins two pipes, on string keys, is run in both local and hadoop mode.") {
-    // URI of the Kiji table to use.
-    val uri: String = doAndRelease(makeTestKijiTable(avroLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    // Create input from Kiji table.
-    val joinKijiInput: List[(EntityId, Seq[FlowCell[String]])] = List(
-        ( EntityId("0row"), mapSlice("animals", ("0column", 0L, "0 dogs")) ),
-        ( EntityId("1row"), mapSlice("animals", ("0column", 0L, "1 cat")) ),
-        ( EntityId("2row"), mapSlice("animals", ("0column", 0L, "2 fish")) ))
-
-    // Create input from side data.
-    val sideInput: List[(String, String)] = List( ("0", "0row"), ("1", "2row") )
-
-    // Validate output.
-    def validateTest(outputBuffer: Buffer[Tuple1[String]]): Unit = {
-      assert(outputBuffer.size === 2)
-    }
-
-    // Create the JobTest for this test.
-    val jobTest = JobTest(new JoinOnStringsJob(_))
-        .arg("input", uri)
-        .arg("side-input", "sideInputFile")
-        .arg("output", "outputFile")
-        .source(KijiInput(uri, "animals" -> 'animals), joinKijiInput)
-        .source(TextLine("sideInputFile"), sideInput)
-        .sink(Tsv("outputFile"))(validateTest)
-
-    // Run the test in local mode.
-    jobTest.run.finish
-
-    // Run the test in hadoop mode.
-    jobTest.runHadoop.finish
-  }
-}
-
-/** Companion object for KijiSourceSuite. Contains test jobs. */
-object KijiSourceSuite {
-  /**
-   * A job that extracts the most recent string value from the column "family:column1" for all rows
-   * in a Kiji table, and then counts the number of occurrences of those strings across rows.
-   *
-   * @param args to the job. Two arguments are expected: "input", which should specify the URI
-   *     to the Kiji table the job should be run on, and "output", which specifies the output
-   *     Tsv file.
-   */
-  class WordCountJob(args: Args) extends KijiJob(args) {
-    // Setup input to bind values from the "family:column1" column to the symbol 'word.
-    KijiInput(args("input"), "family:column1" -> 'word)
-        // Sanitize the word.
-        .map('word -> 'cleanword) { words:Seq[FlowCell[CharSequence]] =>
-          words.head.datum
-              .toString()
-              .toLowerCase()
-        }
-        // Count the occurrences of each word.
-        .groupBy('cleanword) { occurences => occurences.size }
-        // Write the result to a file.
-        .write(Tsv(args("output")))
-  }
-
-  /**
-   * A job that takes the most recent string value from the column "family:column1" and adds
-   * the letter 's' to the end of it. It passes through the column "family:column2" without
-   * any changes.
-   *
-   * @param args to the job. Two arguments are expected: "input", which should specify the URI
-   *     to the Kiji table the job should be run on, and "output", which specifies the output
-   *     Tsv file.
-   */
-  class TwoColumnJob(args: Args) extends KijiJob(args) {
-    // Setup input to bind values from the "family:column1" column to the symbol 'word.
-    KijiInput(args("input"), "family:column1" -> 'word1, "family:column2" -> 'word2)
-        .map('word1 -> 'pluralword) { words: Seq[FlowCell[CharSequence]] =>
-          words.head.datum.toString() + "s"
-        }
-        .write(Tsv(args("output")))
-  }
-
-  /**
-   * A job that requests specific number of versions and buckets the results by the number of
-   * versions.  The result is pairs of number of versions and the number of rows with that number
-   * of versions.
-   *
-   * @param source that the job will use.
-   * @param args to the job. Two arguments are expected: "input", which should specify the URI
-   *     to the Kiji table the job should be run on, and "output", which specifies the output
-   *     Tsv file.
-   */
-  class VersionsJob(source: KijiSource)(args: Args) extends KijiJob(args) {
-    source
-        // Count the size of words (number of versions).
-        .map('words -> 'versioncount) { words: Seq[FlowCell[String]]=>
-          words.size
-        }
-        .groupBy('versioncount) (_.size)
-        .write(Tsv(args("output")))
-  }
-
-  class MultipleTimestampsImportJob(args: Args) extends KijiJob(args) {
-    // Setup input.
-    TextLine(args("input"))
-        .read
-        // new  the words in each line.
-        .map('line -> ('timestamp, 'entityId, 'word)) { line: String =>
-          val Array(timestamp, eid, token) = line.split("\\s+")
-          (timestamp.toLong, EntityId(eid), token)
-        }
-        // Write the results to the "family:column1" column of a Kiji table.
-        .write(KijiOutput(args("output"), 'timestamp, 'word -> "family:column1"))
-  }
-
-  /**
-   * A job that, for each line in a text file, splits the line into words,
-   * and for each word occurrence writes a copy of the word to the column "family:column1" in a
-   * row for that word in a Kiji table.
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the path to a
-   *     text file, and "output", which specifies the URI to a Kiji table.
-   */
-  class ImportJob(args: Args) extends KijiJob(args) {
-    // Setup input.
-    TextLine(args("input"))
-        .read
-        // new  the words in each line.
-        .flatMap('line -> 'word) { line : String => line.split("\\s+") }
-        // Generate an entityId for each word.
-        .map('word -> 'entityId) { _: String =>
-            EntityId(UUID.randomUUID().toString()) }
-        // Write the results to the "family:column1" column of a Kiji table.
-        .write(KijiOutput(args("output"), 'word -> "family:column1"))
-  }
-
-  /**
-   * A job that, given lines of text, writes each line to row for the line in a Kiji table,
-   * at the column "family:column1", with the offset provided for each line being used as the
-   * timestamp.
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the path to a
-   *     text file, and "output", which specifies the URI to a Kiji table.
-   */
-  class ImportJobWithTime(args: Args) extends KijiJob(args) {
-    // Setup input.
-    TextLine(args("input"))
-        .read
-        // Generate an entityId for each line.
-        .map('line -> 'entityId) { EntityId(_: String) }
-        // Write the results to the "family:column1" column of a Kiji table.
-        .write(KijiOutput(args("output"), 'offset, 'line -> "family:column1"))
-  }
-
-  /**
-   * A job that given input from a Kiji table, ensures the type is accurate.
-   *
-   * @param source that the job will use.
-   * @param args to the job. The input URI for the table and the output file.
-   */
-  class AvroToScalaChecker(source: KijiSource)(args: Args) extends KijiJob(args) {
-    source
-
-        .flatMap('word -> 'matches) { word: Seq[FlowCell[CharSequence]] =>
-          word.map { cell: FlowCell[CharSequence] =>
-            val value = cell.datum
-            if (value.isInstanceOf[CharSequence]) {
-              "true"
-            } else {
-              "false"
-            }
-          }
-        }
-        .groupBy('matches) (_.size)
-        .write(Tsv(args("output")))
-  }
-
-  /**
-   * A job that uses the generic API, getting the "hash_size" field from a generic record, and
-   * writes the number of records that have a certain hash_size.
-   *
-   * @param args to the job. Two arguments are expected: "input", which should specify the URI
-   *     to the Kiji table the job should be run on, and "output", which specifies the output
-   *     Tsv file.
-   */
-  class GenericAvroReadJob(args: Args) extends KijiJob(args) {
-    KijiInput(args("input"), "family:column3" -> 'records)
-        .map('records -> 'hashSizeField) { slice: Seq[FlowCell[GenericRecord]] =>
-          slice.head match {
-            case FlowCell(_, _, _, record: GenericRecord) => {
-              record
-                  .get("hash_size")
-                  .asInstanceOf[Int]
-            }
-          }
-        }
-        .groupBy('hashSizeField)(_.size)
-        .write(Tsv(args("output")))
-  }
-
-  /**
-   * A job that uses the generic API, getting the "hash_size" field from a generic record, and
-   * writes the number of records that have a certain hash_size.
-   *
-   * @param args to the job. Two arguments are expected: "input", which should specify the URI
-   *     to the Kiji table the job should be run on, and "output", which specifies the output
-   *     Tsv file.
-   */
-  class SpecificAvroReadJob(args: Args) extends KijiJob(args) {
-    // Want to read some data out to 'records and then write it back to a Tsv
-    val ksource = new KijiSource(
-        tableAddress = args("input"),
-        timeRange = All,
-        timestampField = None,
-        inputColumns = Map('records -> ColumnInputSpec(
-            "family:column3", schemaSpec = Specific(classOf[HashSpec]))),
-        outputColumns = Map('records -> QualifiedColumnOutputSpec("family:column3")))
-    ksource
-        .map('records -> 'hashSizeField) { slice: Seq[FlowCell[HashSpec]] =>
-          val FlowCell(_, _, _, record) = slice.head
-          record.getHashSize
-        }
-        .groupBy('hashSizeField)(_.size)
-        .write(Tsv(args("output")))
-  }
-
-  /**
-   * A job that uses the generic API, creating a record containing the text from the input,
-   * and writing it to a Kiji table.
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the path to a
-   *     text file, and "output", which specifies the URI to a Kiji table.
-   */
-  class GenericAvroWriteJob(args: Args) extends KijiJob(args) {
-    val tableUri: String = args("output")
-    TextLine(args("input"))
-        .read
-        .map('offset -> 'timestamp) { offset: String => offset.toLong }
-        .map('offset -> 'l) { offset: String => offset.toLong }
-        // Generate an entityId for each line.
-        .map('line -> 'entityId) { EntityId(_: String) }
-        .rename('line -> 's)
-        .packGenericRecord(('l, 's) -> 'record)(SimpleRecord.getClassSchema)
-        // Write the results to the "family:column4" column of a Kiji table.
-        .project('entityId, 'record)
-        .write(KijiOutput(args("output"), 'record -> "family:column4"))
-  }
-
-  /**
-   * A job that writes to a map-type column family.  It takes text from the input and uses it as
-   * search terms and the number of results returned for that term.  All of them belong to the same
-   * entity, "my_eid".
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the path to a
-   *     text file, and "output", which specifies the URI to a Kiji table.
-   */
-  class MapWriteJob(args: Args) extends KijiJob(args) {
-    TextLine(args("input"))
-        .read
-        // Create an entity ID for each line (always the same one, here)
-        .map('line -> 'entityId) { line: String => EntityId("my_eid") }
-        // new  the number of result for each search term
-        .map('line -> ('terms, 'resultCount)) { line: String =>
-          (line.split(" ")(0), line.split(" ")(1).toInt)
-        }
-        // Write the results to the "family:column1" column of a Kiji table.
-        .write(KijiOutput(args("table"), Map('resultCount ->
-          new ColumnFamilyOutputSpec("searches", 'terms))))
-  }
-
-  /**
-   * A job that tests map-type column families using sequences of cells and outputs the results to
-   * a TSV.
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
-   *     Kiji table, and "output", which specifies the path to a text file.
-   */
-  class MapSliceJob(args: Args) extends KijiJob(args) {
-    KijiInput(args("input"), "animals" -> 'terms)
-        .map('terms -> 'values) { terms: Seq[FlowCell[CharSequence]] => terms.head.datum }
-        .project('values)
-        .write(Tsv(args("output")))
-  }
-
-  /**
-   * A job that tests joining two pipes, on String keys.
-   *
-   * @param args to the job. Two arguments are expected: "input", which specifies the URI to a
-   *     Kiji table, and "output", which specifies the path to a text file.
-   */
-  class JoinOnStringsJob(args: Args) extends KijiJob(args) {
-    val sidePipe = TextLine(args("side-input"))
-        .read
-        .map('line -> 'entityId) { line: String => EntityId(line) }
-
-    KijiInput(args("input"), "animals" -> 'animals)
-        .map('animals -> 'terms) { animals: Seq[FlowCell[CharSequence]] =>
-          animals.head.datum.toString.split(" ")(0) + "row" }
-        .discard('entityId)
-        .joinWithSmaller('terms -> 'line, sidePipe)
-        .write(Tsv(args("output")))
-  }
-}
-
diff --git a/src/test/scala/org/kiji/express/flow/PackUnpackRecordSuite.scala b/src/test/scala/org/kiji/express/flow/PackUnpackRecordSuite.scala
deleted file mode 100644
index e4622a94dcbdf28fc26ba5e5fa502cc877b4c2fb..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/PackUnpackRecordSuite.scala
+++ /dev/null
@@ -1,145 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import cascading.pipe.Pipe
-import cascading.tuple.Fields
-import com.twitter.scalding.Args
-import com.twitter.scalding.TuplePacker
-import com.twitter.scalding.TupleUnpacker
-import org.apache.avro.generic.GenericRecord
-import org.apache.avro.generic.GenericRecordBuilder
-import org.apache.avro.specific.SpecificRecord
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.avro.SimpleRecord
-import org.kiji.express.flow.util.PipeRunner._
-
-@RunWith(classOf[JUnitRunner])
-class PackUnpackRecordSuite extends KijiSuite {
-
-  test("Avro tuple converters should be in implicit scope of KijiJobs.") {
-    class ImplicitJob(args: Args) extends KijiJob(args) {
-      assert(implicitly[TupleUnpacker[GenericRecord]].getClass ===
-          classOf[AvroGenericTupleUnpacker])
-      assert(implicitly[TuplePacker[SpecificRecord]].getClass ===
-          classOf[AvroSpecificTuplePacker[SpecificRecord]])
-      assert(implicitly[TuplePacker[SimpleRecord]].getClass ===
-          classOf[AvroSpecificTuplePacker[SimpleRecord]])
-    }
-  }
-
-  // Import implicits to simulate REPL environment
-  import org.kiji.express.repl.Implicits._
-
-  test("Avro tuple converters should be in implicit scope of express REPL.") {
-    assert(implicitly[TupleUnpacker[GenericRecord]].getClass ===
-        classOf[AvroGenericTupleUnpacker])
-    assert(implicitly[TuplePacker[SpecificRecord]].getClass ===
-        classOf[AvroSpecificTuplePacker[SpecificRecord]])
-    assert(implicitly[TuplePacker[SimpleRecord]].getClass ===
-        classOf[AvroSpecificTuplePacker[SimpleRecord]])
-  }
-
-  /** Schema of SimpleRecord Avro record. */
-  val schema = SimpleRecord.getClassSchema
-
-  /** Sample inputs for testing Avro packing / unpacking in tuple form. */
-  val input: List[(Long, String)] = List(
-    (1, "foobar"),
-    (2, "shoe"),
-    (3, "random"),
-    (99, "baloons"),
-    (356, "sumerians"))
-
-  /** Fields contained in the input. */
-  val inputFields: Fields = ('ls, 's)
-
-  /** Sample inputs for testing Avro packing / unpacking in Specific Record form. */
-  val specificRecords: List[SimpleRecord] = input.map { t =>
-    SimpleRecord.newBuilder.setL(t._1).setS(t._2).build()
-  }
-
-  /** Sample inputs for testing Avro packing / unpacking in Generic Record form. */
-  val genericRecords: List[GenericRecord] = input.map { t =>
-    new GenericRecordBuilder(schema).set("l", t._1).set("s", t._2).build()
-  }
-
-  /** Fields contained in record input. */
-  val recordFields: Fields = 'r
-
-  def validateSpecificSimpleRecord(outputs: Iterable[(Long, String, String, SimpleRecord)]) {
-    outputs.foreach { t =>
-      val (l, s, o, r) = t
-      assert(specificRecords.contains(r))
-      assert(r.getL === l)
-      assert(r.getS === s)
-      assert(r.getO === o)
-    }
-  }
-
-  def validateGenericSimpleRecord(outputs: Iterable[(Long, String, String, GenericRecord)]) {
-    outputs.foreach { t =>
-      val (l, s, o, r) = t
-      assert(genericRecords.contains(r))
-      assert(r.get("l") === l)
-      assert(r.get("s") === s)
-      assert(r.get("o") === o)
-    }
-  }
-
-  test("A KijiPipe can pack fields into a specific Avro record.") {
-    val pipe = new Pipe("pack specific record")
-        .map('ls -> 'l) { ls: String => ls.toLong }
-        .pack[SimpleRecord](('l, 's) -> 'r)
-        .map('r -> 'o) { r: SimpleRecord => r.getO }
-        .project('l, 's, 'o, 'r)
-
-    validateSpecificSimpleRecord(runPipe(pipe, inputFields, input))
-  }
-
-  test("A KijiPipe can pack fields into a generic Avro record.") {
-    val pipe = new Pipe("pack generic record")
-        .map('ls -> 'l) { ls: String => ls.toLong }
-        .packGenericRecord(('l, 's) -> 'r)(schema)
-        .map('r -> 'o) { r: GenericRecord => r.get("o") }
-        .project('l, 's, 'o, 'r)
-
-    validateGenericSimpleRecord(runPipe(pipe, inputFields, input))
-  }
-
-  test("A KijiPipe can unpack a specific Avro record into fields.") {
-    val pipe = new Pipe("unpack generic record")
-        .unpack[SimpleRecord]('r -> ('l, 's, 'o))
-        .project('l, 's, 'o, 'r)
-
-    validateSpecificSimpleRecord(runPipe(pipe, recordFields, specificRecords))
-  }
-
-  test("A KijiPipe can unpack a generic Avro record into fields.") {
-    val pipe = new Pipe("unpack generic record")
-        .unpack[GenericRecord]('r -> ('l, 's, 'o))
-        .project('l, 's, 'o, 'r)
-
-    validateGenericSimpleRecord(runPipe(pipe, recordFields, genericRecords))
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/PagedCellsSuite.scala b/src/test/scala/org/kiji/express/flow/PagedCellsSuite.scala
deleted file mode 100644
index 8851143cd3a4c79b76096ae1de068ce4044a9b3a..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/PagedCellsSuite.scala
+++ /dev/null
@@ -1,175 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import scala.collection.mutable.Buffer
-
-import com.twitter.scalding.Args
-import com.twitter.scalding.JobTest
-import com.twitter.scalding.Tsv
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.flow.util.Resources.doAndRelease
-import org.kiji.schema.KijiTable
-import org.kiji.schema.layout.KijiTableLayout
-import org.kiji.schema.layout.KijiTableLayouts
-
-/**
- * A job that extracts the most recent string value from the column "family:column1" for all rows
- * in a Kiji table, and then concatenates those strings into a single word.
- *
- * @param args to the job. Two arguments are expected: "input", which should specify the URI
- *     to the Kiji table the job should be run on, and "output", which specifies the output
- *     Tsv file.
- */
-class WordConcatJob(args: Args) extends KijiJob(args) {
-  // Setup input to bind values from the "family:column1" column to the symbol 'word.
-  KijiInput(
-      args("input"),
-      Map(ColumnInputSpec("family:column1", all, paging = PagingSpec.Cells(3)) -> 'word))
-    // Sanitize the word.
-    .map('word -> 'cleanword) { words: Seq[FlowCell[CharSequence]] =>
-      words.foldLeft("")((a: String, b: FlowCell[CharSequence]) => a + b.datum.toString)
-    }
-    // Count the occurrences of each word.
-    .groupBy('cleanword) { occurences => occurences.size }
-    // Write the result to a file.
-    .write(Tsv(args("output")))
-}
-
-/**
- * A job that extracts the most recent string value from the column "family:column1" for all rows
- * in a Kiji table, and then concatenates those strings into a single word.
- *
- * @param args to the job. Two arguments are expected: "input", which should specify the URI
- *     to the Kiji table the job should be run on, and "output", which specifies the output
- *     Tsv file.
- */
-class WordCountFlatMapJob(args: Args) extends KijiJob(args) {
-  // Setup input to bind values from the "family:column1" column to the symbol 'word.
-  KijiInput(
-      args("input"),
-      Map(ColumnInputSpec("family:column1", all, paging = PagingSpec.Cells(3)) -> 'word))
-
-      // Sanitize the word.
-      .flatMap('word -> 'word) { words: Seq[FlowCell[CharSequence]] =>
-          words
-      }.map('word -> 'cleanword) {word: FlowCell[CharSequence] => word.datum.toString}
-      // Count the occurrences of each word.
-      .groupBy('cleanword) { occurences => occurences.size }
-      // Write the result to a file.
-      .write(Tsv(args("output")))
-}
-
-@RunWith(classOf[JUnitRunner])
-class PagedCellsSuite extends KijiSuite {
-  /** Simple table layout to use for tests. The row keys are hashed. */
-  val simpleLayout: KijiTableLayout = layout(KijiTableLayouts.SIMPLE_TWO_COLUMNS)
-
-  test("a word-concat job that reads from a Kiji table is run using Scalding's local mode") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    /** Input tuples to use for word count tests. */
-    def wordCountInput(uri: String): List[(EntityId, Seq[FlowCell[String]])] = {
-      List((EntityId("row01"), slice("family:column1",(1L, "hello"), (2L, "world"),(3L, "hello"),
-          (4L, "hello"))))
-    }
-
-    /**
-     * Validates output from WordConcatJob
-     *
-     * @param outputBuffer containing data that output buffer has in it after the job has been run.
-     */
-    def validateWordConcat(outputBuffer: Buffer[(String, Int)]) {
-      val outMap = outputBuffer.toMap
-      assert(1 === outMap("hellohelloworldhello"))
-    }
-
-    // Build test job.
-    JobTest(new WordConcatJob(_))
-      .arg("input", uri)
-      .arg("output", "outputFile")
-      .source(
-          KijiInput(
-            uri,
-            Map(ColumnInputSpec("family:column1", all, paging = PagingSpec.Cells(3)) -> 'word)),
-          wordCountInput(uri))
-      .sink(Tsv("outputFile"))(validateWordConcat)
-      // Run the test job.
-      .runHadoop
-      .finish
-  }
-
-  test("a word-count job that reads from a Kiji table is run using Scalding's local mode") {
-    // Create test Kiji table.
-    val uri: String = doAndRelease(makeTestKijiTable(simpleLayout)) { table: KijiTable =>
-      table.getURI().toString()
-    }
-
-    /** Input tuples to use for word count tests. */
-    def wordCountInput(uri: String): List[(EntityId, Seq[FlowCell[String]])] = {
-      List((EntityId("row01"), slice("family:column1",(1L, "hello"), (2L, "world"),(3L, "hello"),
-        (4L, "hello"))))
-    }
-
-    /**
-     * Validates output from WordConcatJob
-     *
-     * @param outputBuffer containing data that output buffer has in it after the job has been run.
-     */
-    def validateWordConcat(outputBuffer: Buffer[(String, Int)]) {
-      val outMap = outputBuffer.toMap
-      assert(1 === outMap("hellohelloworldhello"))
-    }
-
-    def validateWordCount(outputBuffer: Buffer[(String, Int)]) {
-      val outMap = outputBuffer.toMap
-
-      // Validate that the output is as expected.
-      assert(3 === outMap("hello"))
-      assert(1 === outMap("world"))
-    }
-
-    val column1 = ColumnInputSpec(
-        column = "family:column1",
-        maxVersions = all,
-        paging = PagingSpec.Cells(3)
-    )
-
-    // Build test job.
-    JobTest(new WordCountFlatMapJob(_))
-        .arg("input", uri)
-        .arg("output", "outputFile")
-        .source(
-            KijiInput(
-                uri,
-                Map(column1 -> 'word)),
-            wordCountInput(uri))
-        .sink(Tsv("outputFile"))(validateWordCount)
-        // Run the test job.
-        .runHadoop
-        .finish
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/ReaderSchemaSuite.scala b/src/test/scala/org/kiji/express/flow/ReaderSchemaSuite.scala
deleted file mode 100644
index 0773fc221b22a845ea431890cb0cd7c55cf09445..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/ReaderSchemaSuite.scala
+++ /dev/null
@@ -1,253 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import com.twitter.scalding.Args
-import com.twitter.scalding.Mode
-import org.apache.avro.generic.GenericEnumSymbol
-import org.apache.avro.generic.GenericFixed
-import org.apache.avro.generic.GenericRecord
-import org.apache.hadoop.conf.Configuration
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.avro.SimpleRecord
-import org.kiji.express.flow.SchemaSpec.Generic
-import org.kiji.express.flow.SchemaSpec.Specific
-import org.kiji.express.flow.SchemaSpec.Writer
-import org.kiji.schema.{ EntityId => JEntityId }
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiClientTest
-import org.kiji.schema.KijiDataRequest
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiTableReader
-import org.kiji.schema.KijiTableWriter
-import org.kiji.schema.layout.KijiTableLayout
-
-@RunWith(classOf[JUnitRunner])
-class ReaderSchemaSuite extends KijiClientTest with KijiSuite {
-  import org.kiji.express.flow.util.AvroTypesComplete._
-  setupKijiTest()
-  val kiji: Kiji = createTestKiji()
-  val layout: KijiTableLayout = layout("layout/avro-types-complete.json")
-  val table: KijiTable = {
-    kiji.createTable(layout.getDesc)
-    kiji.openTable(layout.getName)
-  }
-  val conf: Configuration = getConf
-  val uri: String = table.getURI.toString
-  val reader: KijiTableReader = table.openTableReader()
-  val writer: KijiTableWriter = table.openTableWriter()
-
-  private def entityId(s: String): JEntityId = { table.getEntityId(s) }
-
-  private def writeValue(eid: String, column: String, value: Any) {
-    writer.put(entityId(eid), family, column, value)
-    writer.flush()
-  }
-
-  private def getValue[T](eid: String, column: String): T = {
-    val get = reader.get(entityId(eid), KijiDataRequest.create(family, column))
-    require(get.containsColumn(family, column)) // Require the cell exists for null case
-    get.getMostRecentValue(family, column)
-  }
-
-  private def testExpressReadWrite[T](
-      column: String,
-      value: Any,
-      schemaSpec: SchemaSpec,
-      overrideSchema: Option[SchemaSpec] = None
-  ) {
-    val readEid = column + "-in"
-    val writeEid = column + "-out"
-    writeValue(readEid, column, value)
-
-    val outputSchema = overrideSchema.getOrElse(schemaSpec)
-
-    val inputCol = QualifiedColumnInputSpec(family, column, schemaSpec = schemaSpec)
-    val outputCol = QualifiedColumnOutputSpec(family, column, outputSchema)
-
-    val args = Args("--hdfs")
-    Mode.mode = Mode(args, conf)
-    new ReadWriteJob[T](uri, inputCol, outputCol, writeEid, args).run
-    assert(value === getValue[T](writeEid, column))
-  }
-
-  test("A KijiJob can read a counter column with the writer schema.") {
-    testExpressReadWrite[Long](counterColumn, longs.head, Writer)
-  }
-
-  test("A KijiJob can read a raw bytes column with the writer schema.") {
-    testExpressReadWrite[Array[Byte]](rawColumn, bytes.head, Writer)
-  }
-
-  test("A KijiJob can read a null column with the writer schema.") {
-    testExpressReadWrite[Null](nullColumn, null, Writer)
-  }
-
-  test("A KijiJob can read a null column with a generic reader schema.") {
-    testExpressReadWrite[Null](nullColumn, null, Generic(nullSchema))
-  }
-
-  test("A KijiJob can read a boolean column with the writer schema.") {
-    testExpressReadWrite[Boolean](booleanColumn, booleans.head, Writer)
-  }
-
-  test("A KijiJob can read a boolean column with a generic reader schema.") {
-    testExpressReadWrite[Boolean](booleanColumn, booleans.head, Generic(booleanSchema))
-  }
-
-  test("A KijiJob can read an int column with the writer schema.") {
-    testExpressReadWrite[Int](intColumn, ints.head, Writer)
-  }
-
-  test("A KijiJob can read an int column with a generic reader schema.") {
-    testExpressReadWrite[Int](intColumn, ints.head, Generic(intSchema))
-  }
-
-  test("A KijiJob can read a long column with the writer schema.") {
-    testExpressReadWrite[Long](longColumn, longs.head, Writer)
-  }
-
-  test("A KijiJob can read a long column with a generic reader schema.") {
-    testExpressReadWrite[Long](longColumn, longs.head, Generic(longSchema))
-  }
-
-  test("A KijiJob can read a float column with the writer schema.") {
-    testExpressReadWrite[Float](floatColumn, floats.head, Writer)
-  }
-
-  test("A KijiJob can read a float column with a generic reader schema.") {
-    testExpressReadWrite[Float](floatColumn, floats.head, Generic(floatSchema))
-  }
-
-  test("A KijiJob can read a double column with the writer schema.") {
-    testExpressReadWrite[Double](doubleColumn, doubles.head, Writer)
-  }
-
-  test("A KijiJob can read a double column with a generic reader schema.") {
-    testExpressReadWrite[Double](doubleColumn, doubles.head, Generic(doubleSchema))
-  }
-
-  /** TODO: reenable when Schema-594 is fixed. */
-  ignore("A KijiJob can read a bytes column with the writer schema.") {
-    testExpressReadWrite[Array[Byte]](bytesColumn, bytes.head, Writer)
-  }
-
-  /** TODO: reenable when Schema-594 is fixed. */
-  ignore("A KijiJob can read a bytes column with a generic reader schema.") {
-    testExpressReadWrite[Array[Byte]](bytesColumn, bytes.head, Generic(bytesSchema))
-  }
-
-  test("A KijiJob can read a string column with the writer schema.") {
-    testExpressReadWrite[String](stringColumn, strings.head, Writer)
-  }
-
-  test("A KijiJob can read a string column with a generic reader schema.") {
-    testExpressReadWrite[String](stringColumn, strings.head, Generic(stringSchema))
-  }
-
-  test("A KijiJob can read a specific record column with the writer schema.") {
-    testExpressReadWrite[SimpleRecord](specificColumn, specificRecords.head, Writer)
-  }
-
-  test("A KijiJob can read a specific record column with a generic reader schema.") {
-    testExpressReadWrite[SimpleRecord](specificColumn, specificRecords.head,
-        Generic(specificSchema))
-  }
-
-  test("A KijiJob can read a specific record column with a specific reader schema.") {
-    testExpressReadWrite[SimpleRecord](specificColumn, specificRecords.head,
-      Specific(classOf[SimpleRecord]))
-  }
-
-  test("A KijiJob can read a generic record column with the writer schema.") {
-    testExpressReadWrite[GenericRecord](genericColumn, genericRecords.head, Writer)
-  }
-
-  test("A KijiJob can read a generic record column with a generic reader schema.") {
-    testExpressReadWrite[GenericRecord](genericColumn, genericRecords.head, Generic(genericSchema))
-  }
-
-  test("A KijiJob can read an enum column with the writer schema.") {
-    testExpressReadWrite[GenericEnumSymbol](enumColumn, enums.head, Writer,
-        Some(Generic(enumSchema)))
-  }
-
-  test("A KijiJob can read an enum column with a generic reader schema.") {
-    testExpressReadWrite[String](enumColumn, enums.head, Generic(enumSchema))
-  }
-
-  test("A KijiJob can read an array column with the writer schema.") {
-    testExpressReadWrite[List[String]](arrayColumn, avroArrays.head, Writer,
-        Some(Generic(arraySchema)))
-  }
-
-  test("A KijiJob can read an array column with a generic reader schema.") {
-    testExpressReadWrite[List[String]](arrayColumn, avroArrays.head, Generic(arraySchema))
-  }
-
-  test("A KijiJob can read a union column with the writer schema [INT].") {
-    testExpressReadWrite[Any](unionColumn, ints.head, Writer, Some(Generic(unionSchema)))
-  }
-
-  test("A KijiJob can read a union column with the writer schema [STRING].") {
-    testExpressReadWrite[Any](unionColumn, strings.head, Writer, Some(Generic(unionSchema)))
-  }
-
-  test("A KijiJob can read a fixed column with the writer schema [INT].") {
-    testExpressReadWrite[GenericFixed](fixedColumn, fixeds.head, Writer,
-        Some(Generic(fixedSchema)))
-  }
-
-  test("A KijiJob can read a fixed column with a generic reader schema.") {
-    testExpressReadWrite[GenericFixed](fixedColumn, fixeds.head, Generic(fixedSchema))
-  }
-}
-
-// Must be its own top-level class for mystical serialization reasons
-class ReadWriteJob[T](
-    uri: String,
-    input: ColumnInputSpec,
-    output: ColumnOutputSpec,
-    writeEid: String,
-    args: Args
-) extends KijiJob(args) {
-
-  /**
-   * Unwraps the latest value from an iterable of cells and verifies that the type is as expected.
-   *
-   * @param slice containing the value to unwrap.
-   * @return unwrapped value of type T.
-   */
-  private def unwrap(slice: Seq[FlowCell[T]]): (T, Long) = {
-    require(slice.size == 1)
-    val cell = slice.head
-    (cell.datum, cell.version)
-  }
-
-  KijiInput(uri, Map(input -> 'slice))
-      .read
-      .mapTo('slice -> ('value, 'time))(unwrap)
-      .map('value -> 'entityId) { _: T => EntityId(writeEid)}
-      .write(KijiOutput(uri, 'time, Map('value -> output)))
-}
-
diff --git a/src/test/scala/org/kiji/express/flow/TimeRangeSuite.scala b/src/test/scala/org/kiji/express/flow/TimeRangeSuite.scala
deleted file mode 100644
index 782b4f3d92ff8754f4ad334b46da33c72ba3088e..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/TimeRangeSuite.scala
+++ /dev/null
@@ -1,74 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.schema.KConstants
-
-@RunWith(classOf[JUnitRunner])
-class TimeRangeSuite extends FunSuite {
-  test("TimeRange fails fast when invalid time range arguments are specified.") {
-    val thrown: IllegalArgumentException = intercept[IllegalArgumentException] {
-      Between(10L, 1L)
-    }
-
-    val expectedMessage = "requirement failed: Invalid time range specified: (%d, %d)"
-        .format(10L, 1L)
-    assert(thrown.getMessage == expectedMessage)
-  }
-
-  test("All constructs a TimeRange correctly") {
-    val range: TimeRange = All
-
-    assert(range.begin == KConstants.BEGINNING_OF_TIME)
-    assert(range.end == KConstants.END_OF_TIME)
-  }
-
-  test("At constructs a TimeRange correctly") {
-    val range: TimeRange = At(42L)
-
-    assert(range.begin == 42L)
-    assert(range.end == 42L)
-  }
-
-  test("After constructs a TimeRange correctly") {
-    val range: TimeRange = After(42L)
-
-    assert(range.begin == 42L)
-    assert(range.end == KConstants.END_OF_TIME)
-  }
-
-  test("Before constructs a TimeRange correctly") {
-    val range: TimeRange = Before(42L)
-
-    assert(range.begin == KConstants.BEGINNING_OF_TIME)
-    assert(range.end == 42L)
-  }
-
-  test("Between constructs a TimeRange correctly") {
-    val range: TimeRange = Between(10L, 42L)
-
-    assert(range.begin == 10L)
-    assert(range.end == 42L)
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/TransientSeqSuite.scala b/src/test/scala/org/kiji/express/flow/TransientSeqSuite.scala
deleted file mode 100644
index 05f9c4031bf01b2ccff9f503c4be71ede96aa8a6..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/TransientSeqSuite.scala
+++ /dev/null
@@ -1,383 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import java.util.concurrent.atomic.AtomicLong
-
-import scala.collection.mutable
-
-import org.kiji.express.KijiSuite
-
-class TransientSeqSuite extends KijiSuite {
-
-  test("Paging through a TransientSeq twice will generate two iterators.") {
-    var counter = 0
-    def genItr(): Iterator[Int] = {
-      counter += 1
-      List().iterator
-    }
-
-    val seq = new TransientSeq(genItr)
-    for (_ <- seq) {}
-    for (_ <- seq) {}
-    assert(counter === 2)
-  }
-
-  test("TransientSeq will not reuse iterators when doing transformations.") {
-    var counter = 0
-    def genItr(): Iterator[Int] =
-      new Iterator[Int] {
-        val itr = (0 until 5).iterator
-        def hasNext: Boolean = itr.hasNext
-
-        def next(): Int = {
-          counter += 1
-          itr.next()
-        }
-      }
-
-    val seq = new TransientSeq(genItr)
-    for (_ <- seq) {}
-    for (_ <- seq) {}
-    assert(counter === 10)
-  }
-
-  test("TransientSeq does not hold onto the results of an iterator.") {
-    // The only (sure) way to test this is to make sure an OOME doesn't occur
-    val max = 250 * 1000 * 1000 // 1 GB of Ints
-    def genItr(): Iterator[Int] = (1 to max).iterator
-
-    val seq = new TransientSeq(genItr)
-    // val seq = genItr().toStream // Control case.  Will crash
-
-    // Make sure there is a side effect when running through the iterator so it won't get JIT'd away
-    var counter: Int = 0
-    for (i <- seq) { counter = i }
-    assert(counter === max)
-  }
-
-  test("Chaining transformations on a TransientSeq does not require an iterator.") {
-    var counter = 0
-    def genItr(): Iterator[Int] = {
-      counter += 1
-      (1 to Int.MaxValue).iterator
-    }
-
-    val tseq = new TransientSeq(genItr)
-        .map(x => x + 1)
-        .map(x => x * x)
-        .take(10)
-        .drop(3)
-        .filter(x => x % 2 == 0)
-
-    assert(counter === 0)
-  }
-
-  // C(i, n) is a complexity measure of the number of iterators requested from the TransientSeq, i,
-  // and the number of elements requested from all iterators, n.  N is the number of elements held
-  // by the underlying iterators of the TransientSeq.
-
-  /** Good. */
-  test("force is C(1, 1)") { assertComplexity(1, 1, tseq => tseq.force) }
-
-  /** Good. */
-  test("head is C(1, 1)") { assertComplexity(1, 1, tseq => tseq.head) }
-
-  /** Poor. See below. */
-  test("tail is C(1, 0)") { assertComplexity(1, 0, tseq => tseq.tail) }
-
-  /** This is egregious. */
-  test("tail * m is C(m, 0 + 1 + ... + m - 1)") {
-    assertComplexity(4, 6, tseq => tseq.tail.tail.tail.tail)
-  }
-
-  /** And this is even worse. */
-  test("tail * m then force is C(m + 1, 0 + 1 + m - 1 + m + 1)") {
-    assertComplexity(5, 11, tseq => tseq.tail.tail.tail.tail.force)
-  }
-
-  /** Good. */
-  test("force then tail * m is C(1, m + 1)") {
-    assertComplexity(1, 5, tseq => tseq.force.tail.tail.tail.tail)
-  }
-
-  /** Good. */
-  test("take(m) is C(0, 0)") { assertComplexity(0, 0, tseq => tseq.take(42)) }
-
-  /** Good. */
-  test("take(m) then force is C(1, 1)") { assertComplexity(1, 1, tseq => tseq.take(42).force) }
-
-  /** Good. */
-  test("drop(m) is C(0, 0)") { assertComplexity(0, 0, tseq => tseq.drop(42)) }
-
-  /** Good. */
-  test("drop(m) then force is C(1, m + 1)") { assertComplexity(1, 43, tseq => tseq.drop(42).force) }
-
-  /** Good. */
-  test("dropWhile is C(0, 0)") {
-    assertComplexity(0, 0, tseq => tseq.dropWhile(x => x < 100))
-  }
-
-  /** Good. */
-  test("dropWhile then force is C(1, O(N))") {
-    assertComplexity(1, 100, tseq => tseq.dropWhile(x => x < 100).force)
-  }
-
-  /** Good. */
-  test("immutable additions are C(0, 0)") {
-    assertComplexity(0, 0, tseq => tseq ++ List(1, 2, 3))
-    assertComplexity(0, 0, tseq => tseq ++: List(1, 2, 3))
-    assertComplexity(0, 0, tseq => tseq.+:(1))
-    assertComplexity(0, 0, tseq => tseq.:+(1))
-  }
-
-  def sum(a: Int, b: Int): Int = a + b
-
-  /** Good. */
-  test("fold is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).foldLeft(0)(sum))
-    assertComplexity(1, 10, tseq => tseq.take(10).foldRight(0)(sum))
-    assertComplexity(1, 10, tseq => tseq.take(10)./:(0)(sum))
-    assertComplexity(1, 10, tseq => tseq.take(10).:\(0)(sum))
-    assertComplexity(1, 10, tseq => tseq.take(10).aggregate(0)(sum, sum))
-  }
-
-  /** Poor. Use fold over reduce where possible. */
-  test("reduce left is C(2, N)") {
-    assertComplexity(2, 10, tseq => tseq.take(10).reduce(sum))
-    assertComplexity(2, 10, tseq => tseq.take(10).reduceLeft(sum))
-  }
-
-  /** OK, but unsafe because of memory usage. */
-  test("reduce right is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).reduceRight(sum))
-  }
-
-  /** Poor. */
-  test("reduce option adds 1 iterator access") {
-    assertComplexity(3, 10, tseq => tseq.take(10).reduceLeftOption(sum))
-    assertComplexity(2, 10, tseq => tseq.take(10).reduceRightOption(sum))
-  }
-
-  /** Good. */
-  test("flatten is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).map(x => List(x)).flatten)
-  }
-
-  /** Good. */
-  test("map is C(0, 0)") {
-    assertComplexity(0, 0, tseq => tseq.map(x => x + 1))
-    assertComplexity(0, 0, tseq => tseq.flatMap(x => List(x, x)))
-    assertComplexity(0, 0, tseq => tseq.reverseMap(x => x + 1))
-  }
-
-  /** Good. */
-  test("map then force is C(1, 1)") {
-    assertComplexity(1, 1, tseq => tseq.map(x => x + 1).force)
-    assertComplexity(1, 1, tseq => tseq.flatMap(x => List(x, x)).force)
-  }
-
-  /** OK, but unsafe because of memory usage. */
-  test("reverseMap then force is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).reverseMap(x => x + 1).force)
-  }
-
-  /** Good. */
-  test("map then toList is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).map(x => x + 1).toList)
-    assertComplexity(1, 10, tseq => tseq.take(10).flatMap(x => List(x, x)).toList)
-    assertComplexity(1, 10, tseq => tseq.take(10).reverseMap(x => x + 1).toList)
-  }
-
-  /** Good. */
-  test("filter is C(0, 0)") {
-    assertComplexity(0, 0, tseq => tseq.filter(x => x % 2 == 0))
-    assertComplexity(0, 0, tseq => tseq.filterNot(x => x % 2 == 0))
-    assertComplexity(0, 0, tseq => tseq.collect { case x if x % 2 == 0 => x })
-  }
-
-  /** Good. */
-  test("filter then force is C(1, x) where x is the index of the first non-filtered element") {
-    assertComplexity(1, 2, tseq => tseq.filter(x => x % 2 == 0).force)
-    assertComplexity(1, 1, tseq => tseq.filterNot(x => x % 2 == 0).force)
-  }
-
-  /** Good. */
-  test("filter then toList is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).filter(x => x % 2 == 0).toList)
-    assertComplexity(1, 10, tseq => tseq.take(10).filterNot(x => x % 2 == 0).toList)
-  }
-
-  /** Good. */
-  test("membership ops are C(1, O(N))") {
-    assertComplexity(1, 10, tseq => tseq.collectFirst { case x if x == 10 => x })
-    assertComplexity(1, 10, tseq => tseq.contains(10))
-    assertComplexity(1, 10, tseq => tseq.indexOf(10))
-    assertComplexity(1, 10, tseq => tseq.indexWhere(_ == 10))
-    assertComplexity(1, 10, tseq => tseq.indexWhere(_ == 10, 3))
-    assertComplexity(1, 10, tseq => tseq.exists(_ == 10))
-    assertComplexity(1, 10, tseq => tseq.find(_ == 10))
-  }
-
-  /** Good. */
-  test("size is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).size)
-    assertComplexity(1, 10, tseq => tseq.take(10).length)
-  }
-
-  /** Good. */
-  test("copy ops are C(1, L) where L is the number of elements to copy") {
-    assertComplexity(1, 10, tseq => tseq.copyToArray(Array.ofDim[Int](10)))
-    assertComplexity(1, 10, tseq => tseq.copyToArray(Array.ofDim[Int](10), 0))
-    assertComplexity(1, 10, tseq => tseq.copyToArray(Array.ofDim[Int](10), 0, 10))
-    assertComplexity(1, 10, tseq => tseq.take(10).copyToBuffer(mutable.Buffer[Int]()))
-  }
-
-  /** Poor. */
-  test("force then updated is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).updated(2, 99))
-  }
-
-  /** Good. */
-  test("corresponds is C(1, O(Min(N, L))) where L is the length of the other Seq") {
-    assertComplexity(1, 5, tseq => tseq.corresponds(1 to 5)(_ equals _))
-  }
-
-  /** Poor. Not lazy, will not complete without the take. */
-  test("combinations is C(2, 2 * N)") {
-    assertComplexity(2, 20, tseq => tseq.take(10).combinations(3))
-  }
-
-  /** Poor. Not lazy, will not complete without the take. */
-  test("permutations is C(2, N)") {
-    assertComplexity(2, 10, tseq => tseq.take(10).permutations)
-  }
-
-  /** OK, but unsafe because of memory usage. */
-  test("groupBy is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).groupBy(x => x % 4 == 0))
-  }
-
-  /** Good. */
-  test("grouped is C(1, 0)") {
-    assertComplexity(1, 0, tseq => tseq.grouped(2))
-  }
-
-  /** Good. */
-  test("zip is C(0, 0)") {
-    assertComplexity(0, 0, tseq => tseq.zip(1 to 100))
-    assertComplexity(0, 0, tseq => tseq.zipAll(1 to 100, 99, 99))
-    assertComplexity(0, 0, tseq => tseq.zipWithIndex)
-  }
-
-  /** Poor. Not lazy, will not complete without the take. */
-  test("unzip is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).zipWithIndex.unzip)
-  }
-
-  /** OK, but unsafe because of memory usage. */
-  test("sorted is C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).sorted)
-  }
-
-  /** OK, but unsafe because of memory usage. */
-  test("set operations are C(1, N)") {
-    assertComplexity(1, 10, tseq => tseq.take(10).union(List(1, 2, 3, 99)))
-    assertComplexity(1, 10, tseq => tseq.take(10).intersect(List(1, 2, 3, 99)))
-    assertComplexity(1, 10, tseq => tseq.take(10).diff(List(1, 2, 3, 99)))
-  }
-
-  /** Good. */
-  test("patch is C(0, 0)") {
-    assertComplexity(0, 0, tseq => tseq.take(12).patch(0, List(10), 10))
-  }
-
-  /** Good. */
-  test("startsWith is C(1, _)") {
-    assertComplexity(1, 1, tseq => tseq.startsWith(List(10)))
-    assertComplexity(1, 4, tseq => tseq.startsWith(List(10), 3))
-  }
-
-  /** Poor. */
-  test("endsWith is C(2, 2 * N)") {
-    assertComplexity(2, 20, tseq => tseq.take(10).endsWith(List(10)))
-  }
-
-  /** Extremely poor. */
-  test("some operations are unsupported.") {
-    intercept[UnsupportedOperationException] {
-      assertComplexity(1, 10, tseq => tseq.take(12).containsSlice(List(9, 10)))
-    }
-    intercept[UnsupportedOperationException] {
-      assertComplexity(1, 10, tseq => tseq.take(10).distinct)
-    }
-    intercept[UnsupportedOperationException] {
-      assertComplexity(1, 10, tseq => tseq.take(12).indexOfSlice(List(10)))
-    }
-    intercept[UnsupportedOperationException] {
-      assertComplexity(1, 10, tseq => tseq.take(10).takeRight(2))
-    }
-    intercept[UnsupportedOperationException] {
-      assertComplexity(1, 100, tseq => tseq.take(10).dropRight(2))
-    }
-    intercept[UnsupportedOperationException] {
-      assertComplexity(1, 10, tseq => tseq.take(10).lastIndexOfSlice(List(10)))
-    }
-    intercept[UnsupportedOperationException] {
-      assertComplexity(1, 10, tseq => tseq.take(12).lastIndexOfSlice(List(10), 10))
-    }
-  }
-
-  test("unsupported operations are supported after force.") {
-    assertComplexity(1, 10, tseq => tseq.force.take(12).containsSlice(List(9, 10)))
-    assertComplexity(1, 10, tseq => tseq.force.take(10).distinct.toList)
-    assertComplexity(1, 10, tseq => tseq.force.take(12).indexOfSlice(List(10)))
-    assertComplexity(1, 10, tseq => tseq.force.take(10).takeRight(2))
-    assertComplexity(1, 10, tseq => tseq.force.take(10).dropRight(2))
-    assertComplexity(1, 10, tseq => tseq.force.take(10).lastIndexOfSlice(List(10)))
-    assertComplexity(1, 10, tseq => tseq.force.take(10).lastIndexOfSlice(List(10), 10))
-  }
-
-  def countingItr(): (AtomicLong, AtomicLong, () => Iterator[Int]) = {
-    val i = new AtomicLong() // number of iterators requested.
-    val n = new AtomicLong() // number of elements requested across all iterators.
-    def genItr(): Iterator[Int] = {
-      i.incrementAndGet()
-      new Iterator[Int] {
-        val itr = (1 to Int.MaxValue).iterator
-        def hasNext: Boolean = itr.hasNext
-        def next(): Int = { n.incrementAndGet() ; itr.next() }
-      }
-    }
-    (i, n , genItr)
-  }
-
-  def countingTSeq(): (AtomicLong, AtomicLong, TransientSeq[Int]) = {
-    val (i, n, genItr) = countingItr()
-    (i, n, new TransientSeq(genItr))
-  }
-
-  def assertComplexity(i: Long, n: Long, op: TransientSeq[Int] => Unit): Unit = {
-    val (iActual, nActual, tseq) = countingTSeq()
-    op(tseq)
-    assert(i === iActual.get, "iterator count")
-    assert(n === nActual.get, "element count")
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/WriterSchemaSuite.scala b/src/test/scala/org/kiji/express/flow/WriterSchemaSuite.scala
deleted file mode 100644
index 1f9b312c144217a169be1af78939412851906b90..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/WriterSchemaSuite.scala
+++ /dev/null
@@ -1,312 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow
-
-import scala.collection.JavaConversions
-
-import cascading.tuple.Fields
-import com.twitter.scalding.Args
-import com.twitter.scalding.IterableSource
-import com.twitter.scalding.Mode
-import com.twitter.scalding.TupleConverter
-import com.twitter.scalding.TupleSetter
-import org.apache.avro.Schema
-import org.apache.avro.generic.GenericData
-import org.apache.avro.generic.GenericData.Fixed
-import org.apache.hadoop.conf.Configuration
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.flow.SchemaSpec.Writer
-import org.kiji.express.flow.util.AvroTypesComplete
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiClientTest
-import org.kiji.schema.KijiColumnName
-import org.kiji.schema.KijiDataRequest
-import org.kiji.schema.KijiTable
-import org.kiji.schema.KijiTableReader
-import org.kiji.schema.KijiTableWriter
-import org.kiji.schema.{EntityId => SchemaEntityId}
-
-
-@RunWith(classOf[JUnitRunner])
-class WriterSchemaSuite extends KijiClientTest with KijiSuite {
-  import WriterSchemaSuite._
-  import AvroTypesComplete._
-
-  // TODO: These non-test things can be moved to the companion object after SCHEMA-539 fix
-  setupKijiTest()
-  val kiji: Kiji = createTestKiji()
-  val table: KijiTable = {
-    kiji.createTable(AvroTypesComplete.layout.getDesc)
-    kiji.openTable(AvroTypesComplete.layout.getName)
-  }
-  val conf: Configuration = getConf
-  val uri = table.getURI.toString
-  val reader: KijiTableReader = table.openTableReader()
-  val writer: KijiTableWriter = table.openTableWriter()
-
-  /**
-   * Get value from HBase.
-   * @param eid string of row
-   * @param column column containing requested value
-   * @tparam T expected type of value
-   * @return the value
-   */
-  def getValue[T](eid: String, column: KijiColumnName): T = {
-    def entityId(s: String): SchemaEntityId = table.getEntityId(s)
-    val (family, qualifier) = column.getFamily -> column.getQualifier
-    val get = reader.get(entityId(eid), KijiDataRequest.create(family, qualifier))
-    require(get.containsColumn(family, qualifier)) // Require the cell exists for null case
-    get.getMostRecentValue(family, qualifier)
-  }
-
-  /**
-   * Verify that the inputs have been persisted into the Kiji column.  Checks that the types and
-   * values match.
-   * @param inputs to check against
-   * @param column column that the inputs are stored in
-   * @tparam T expected return type of value in HBase
-   */
-  def verify[T](inputs: Iterable[(EntityId, T)],
-                column: KijiColumnName,
-                verifier: (T, T) => Unit): Unit = {
-    inputs.foreach { input: (EntityId, T) =>
-      val (eid, value) = input
-      val retrieved: T = getValue(eid.components.head.toString, column)
-      verifier(value, retrieved)
-    }
-  }
-
-  def valueVerifier[T](input: T, retrieved: T): Unit = {
-    assert(input === retrieved)
-  }
-
-  def nullVerifier(input: Any, retrieved: Any): Unit = {
-    assert(input === null)
-    assert(retrieved === null)
-  }
-
-  def arrayVerifier[T](input: T, retrieved: T): Unit = {
-    assert(retrieved.isInstanceOf[GenericData.Array[_]])
-    val ret = JavaConversions.JListWrapper(retrieved.asInstanceOf[GenericData.Array[_]]).toSeq
-    assert(input.asInstanceOf[Iterable[_]].toSeq === ret)
-  }
-
-  def fixedVerifier[T](input: T, retrieved: T): Unit = {
-    assert(retrieved.isInstanceOf[Fixed])
-    assert(input === retrieved.asInstanceOf[Fixed].bytes())
-  }
-
-  def enumVerifier[T](schema: Schema)(input: T, retrieved: T): Unit = {
-    assert(retrieved.isInstanceOf[GenericData.EnumSymbol])
-    assert(
-      retrieved.asInstanceOf[GenericData.EnumSymbol] ===
-        new GenericData().createEnum(input.toString, schema))
-  }
-
-  /**                                                                         bfffff
-   * Write provided values with express into an HBase column with options as specified in output,
-   * and verify that the values have been persisted correctly.
-   * @param values to test
-   * @param output options to write with
-   * @tparam T type of values to write
-   * @return
-   */
-  def testWrite[T](values: Iterable[T],
-                   output: ColumnOutputSpec,
-                   verifier: (T, T) => Unit =  valueVerifier _) {
-    val outputSource = KijiOutput(uri, Map('value -> output))
-    val inputs = eids.zip(values)
-    expressWrite(conf, new Fields("entityId", "value"), inputs, outputSource)
-    verify(inputs, output.columnName, verifier)
-  }
-
-  test("A KijiJob can write to a counter column with a Writer schema spec.") {
-    testWrite(longs, QualifiedColumnOutputSpec(family, counterColumn, Writer))
-  }
-
-  test("A KijiJob can write to a raw bytes column with a Writer schema spec.")    {
-    testWrite(bytes, QualifiedColumnOutputSpec(family, rawColumn, Writer))
-  }
-
-  test("A KijiJob can write to an Avro null column with a Generic schema spec.") {
-    testWrite(nulls, QualifiedColumnOutputSpec(family, nullColumn, nullSchema), nullVerifier)
-  }
-
-  test("A KijiJob can write to an Avro null column with a Writer schema spec.") {
-    testWrite(nulls, QualifiedColumnOutputSpec(family, nullColumn), nullVerifier)
-  }
-
-  test("A KijiJob can write to an Avro boolean column with a Generic schema spec.") {
-    testWrite(booleans, QualifiedColumnOutputSpec(family, booleanColumn, booleanSchema))
-  }
-
-  test("A KijiJob can write to an Avro boolean column with a Writer schema spec.") {
-    testWrite(booleans, QualifiedColumnOutputSpec(family, booleanColumn, Writer))
-  }
-
-  test("A KijiJob can write to an Avro int column with a Generic schema spec.") {
-    testWrite(ints, QualifiedColumnOutputSpec(family, intColumn, intSchema))
-  }
-
-  test("A KijiJob can write to an Avro int column with a Writer schema spec.") {
-    testWrite(ints, QualifiedColumnOutputSpec(family, intColumn, Writer))
-  }
-
-  test("A KijiJob can write to an Avro long column with a Generic schema spec.") {
-    testWrite(longs, QualifiedColumnOutputSpec(family, longColumn, longSchema))
-  }
-
-  test("A KijiJob can write to an Avro long column with a Writer schema spec.") {
-    testWrite(longs, QualifiedColumnOutputSpec(family, longColumn, Writer))
-  }
-
-  test("A KijiJob can write ints to an Avro long column with an int schema.") {
-    testWrite(ints, QualifiedColumnOutputSpec(family, longColumn, intSchema))
-  }
-
-  test("A KijiJob can write to an Avro float column with a Generic schema spec.") {
-    testWrite(floats, QualifiedColumnOutputSpec(family, floatColumn, floatSchema))
-  }
-
-  test("A KijiJob can write to an Avro float column with a Writer schema spec.") {
-    testWrite(floats, QualifiedColumnOutputSpec(family, floatColumn, Writer))
-  }
-
-  test("A KijiJob can write to an Avro double column with a Generic schema spec.") {
-    testWrite(doubles, QualifiedColumnOutputSpec(family, doubleColumn, doubleSchema))
-  }
-
-  test("A KijiJob can write to an Avro double column with a Writer schema spec.") {
-    testWrite(doubles, QualifiedColumnOutputSpec(family, doubleColumn, Writer))
-  }
-
-  test("A KijiJob can write floats to an Avro double column with a float schema.") {
-    testWrite(floats, QualifiedColumnOutputSpec(family, doubleColumn, intSchema))
-  }
-
-  /** TODO: reenable when Schema-594 is fixed. */
-  ignore("A KijiJob can write to an Avro bytes column with a Generic schema spec.") {
-    testWrite(bytes, QualifiedColumnOutputSpec(family, bytesColumn, bytesSchema))
-  }
-
-  /** TODO: reenable when Schema-594 is fixed. */
-  ignore("A KijiJob can write to an Avro bytes column with a Writer schema spec.") {
-    testWrite(bytes, QualifiedColumnOutputSpec(family, bytesColumn, Writer))
-  }
-
-  test("A KijiJob can write to an Avro string column with a Generic schema spec.") {
-    testWrite(strings, QualifiedColumnOutputSpec(family, stringColumn, stringSchema))
-  }
-
-  test("A KijiJob can write to an Avro string column with a Writer schema spec.") {
-    testWrite(strings, QualifiedColumnOutputSpec(family, stringColumn, Writer))
-  }
-
-  test("A KijiJob can write to an Avro specific record column with a Generic schema spec.") {
-    testWrite(specificRecords, QualifiedColumnOutputSpec(family, specificColumn, specificSchema))
-  }
-
-  test("A KijiJob can write to an Avro specific record column with a Writer schema spec.") {
-    testWrite(specificRecords, QualifiedColumnOutputSpec(family, specificColumn, Writer))
-  }
-
-  test("A KijiJob can write to a generic record column with a Generic schema spec.") {
-    testWrite(genericRecords, QualifiedColumnOutputSpec(family, genericColumn, genericSchema))
-  }
-
-  test("A KijiJob can write to a generic record column with a Writer schema spec.") {
-    testWrite(genericRecords, QualifiedColumnOutputSpec(family, genericColumn, Writer))
-  }
-
-  test("A KijiJob can write to an enum column with a Generic schema spec.") {
-    testWrite(enums, QualifiedColumnOutputSpec(family, enumColumn, enumSchema))
-  }
-
-  test("A KijiJob can write to an enum column with a Writer schema spec.") {
-    testWrite(enums, QualifiedColumnOutputSpec(family, enumColumn, Writer))
-  }
-
-  test("A KijiJob can write a string to an enum column with a Generic schema spec.") {
-    testWrite(enumStrings, QualifiedColumnOutputSpec(family, enumColumn, enumSchema),
-      enumVerifier(enumSchema))
-  }
-
-  test("A KijiJob can write an avro array to an array column with a Generic schema spec.") {
-    testWrite(avroArrays, QualifiedColumnOutputSpec(family, arrayColumn, arraySchema))
-  }
-
-  test("A KijiJob can write an avro array to an array column with a Writer schema spec."){
-    testWrite(avroArrays, QualifiedColumnOutputSpec(family, arrayColumn, Writer))
-  }
-
-  test("A KijiJob can write an Iterable to an array column with a Generic schema spec.") {
-    testWrite(arrays, QualifiedColumnOutputSpec(family, arrayColumn, arraySchema), arrayVerifier)
-  }
-
-  test("A KijiJob can write to a union column with a Generic schema spec.") {
-    testWrite(unions, QualifiedColumnOutputSpec(family, unionColumn, unionSchema))
-  }
-
-  test("A KijiJob can write to a union column with a Writer schema spec.") {
-    testWrite(unions, QualifiedColumnOutputSpec(family, unionColumn, Writer))
-  }
-
-  test("A KijiJob can write to a fixed column with a Generic schema spec.") {
-    testWrite(fixeds, QualifiedColumnOutputSpec(family, fixedColumn, fixedSchema))
-  }
-
-  test("A KijiJob can write to a fixed column with a Writer schema spec.") {
-    testWrite(fixeds, QualifiedColumnOutputSpec(family, fixedColumn, Writer))
-  }
-
-  test("A KijiJob can write a byte array to a fixed column with a Generic schema spec.") {
-    testWrite(fixedByteArrays, QualifiedColumnOutputSpec(family, fixedColumn, fixedSchema),
-      fixedVerifier)
-  }
-}
-
-object WriterSchemaSuite {
-  /**
-   * Writes inputs to outputSource with Express.
-   * @param fs fields contained in the input tuples.
-   * @param inputs contains tuples to write to HBase with Express.
-   * @param outputSource KijiSource with options for how to write values to HBase.
-   * @param setter necessary for some implicit shenanigans.  Don't explicitly pass in.
-   * @tparam A type of values to be written.
-   */
-  def expressWrite[A](conf: Configuration,
-                      fs: Fields,
-                      inputs: Iterable[A],
-                      outputSource: KijiSource)
-                     (implicit setter: TupleSetter[A]): Boolean = {
-    val args = Args("--hdfs")
-    Mode.mode = Mode(args, conf) // HDFS mode
-    new IdentityJob(fs, inputs, outputSource, args).run
-  }
-}
-
-// Must be its own top-level class for mystical serialization reasons
-class IdentityJob[A](fs: Fields, inputs: Iterable[A], output: KijiSource, args: Args)
-                    (implicit setter: TupleSetter[A]) extends KijiJob(args) {
-  IterableSource(inputs, fs)(setter, implicitly[TupleConverter[A]]).write(output)
-}
diff --git a/src/test/scala/org/kiji/express/flow/framework/AvroSerializerSuite.scala b/src/test/scala/org/kiji/express/flow/framework/AvroSerializerSuite.scala
deleted file mode 100644
index 073cef7891b8d6df02f7cefd589b62602e20e52d..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/framework/AvroSerializerSuite.scala
+++ /dev/null
@@ -1,133 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import scala.collection.JavaConverters.seqAsJavaListConverter
-
-import cascading.kryo.KryoFactory
-import com.esotericsoftware.kryo.Kryo
-import com.esotericsoftware.kryo.io.Input
-import com.esotericsoftware.kryo.io.Output
-import org.apache.avro.Schema
-import org.apache.avro.generic.GenericContainer
-import org.apache.avro.generic.GenericRecordBuilder
-import org.apache.avro.specific.SpecificRecord
-import org.apache.hadoop.hbase.HBaseConfiguration
-import org.scalatest.FunSuite
-
-import org.kiji.express.avro.SimpleRecord
-import org.kiji.express.flow.framework.serialization.AvroSpecificSerializer
-import org.kiji.express.flow.framework.serialization.AvroGenericSerializer
-import org.kiji.express.flow.framework.serialization.AvroSchemaSerializer
-
-class AvroSerializerSuite
-    extends FunSuite {
-  def serDeTest[I](inputName: String, serdeName: String, input: => I)(operation: I => I) {
-    test("Serialization/Deserialization of a %s using %s".format(inputName, serdeName)) {
-      val expected: I = input
-      val actual: I = operation(expected)
-
-      assert(expected === actual)
-    }
-  }
-
-  val recordSchema: Schema = {
-    val fields = Seq(
-        new Schema.Field("field1", Schema.create(Schema.Type.INT), "First test field.", null),
-        new Schema.Field("field2", Schema.create(Schema.Type.STRING), "First test field.", null),
-        new Schema.Field("field3", Schema.create(Schema.Type.FLOAT), "First test field.", null))
-
-    val record = Schema.createRecord("TestRecord", "", "", false)
-    record.setFields(fields.asJava)
-
-    record
-  }
-
-  val genericRecord: GenericContainer = {
-    new GenericRecordBuilder(recordSchema)
-        .set("field1", 42)
-        .set("field2", "foo")
-        .set("field3", 3.14f)
-        .build()
-  }
-
-  val specificRecord: SpecificRecord = {
-    SimpleRecord
-        .newBuilder()
-        .setL(42L)
-        .setO("foo")
-        .setS("bar")
-        .build()
-  }
-
-  serDeTest("Schema", "Avro", recordSchema) { actual =>
-    // Use cascading.kryo to mimic scalding's actual behavior.
-    val kryo = new Kryo()
-    val kryoFactory = new KryoFactory(HBaseConfiguration.create())
-    val registrations = Seq(
-        new KryoFactory.ClassPair(classOf[Schema], classOf[AvroSchemaSerializer]))
-    kryoFactory.setHierarchyRegistrations(registrations.asJava)
-    kryoFactory.populateKryo(kryo)
-
-    // Serialize the schema.
-    val output = new Output(1024)
-    kryo.writeObject(output, actual)
-
-    // Deserialize the schema.
-    val input = new Input(output.getBuffer)
-    kryo.readObject(input, classOf[Schema])
-  }
-
-  serDeTest("GenericRecord", "Avro", genericRecord) { actual =>
-    // Use cascading.kryo to mimic scalding's actual behavior.
-    val kryo = new Kryo()
-    val kryoFactory = new KryoFactory(HBaseConfiguration.create())
-    val registrations = Seq(
-        new KryoFactory.ClassPair(classOf[GenericContainer], classOf[AvroGenericSerializer]))
-    kryoFactory.setHierarchyRegistrations(registrations.asJava)
-    kryoFactory.populateKryo(kryo)
-
-    // Serialize the schema.
-    val output = new Output(1024)
-    kryo.writeObject(output, actual)
-
-    // Deserialize the schema.
-    val input = new Input(output.getBuffer)
-    kryo.readObject(input, classOf[GenericContainer])
-  }
-
-  serDeTest("SpecificRecord", "Avro", specificRecord) { actual =>
-    // Use cascading.kryo to mimic scalding's actual behavior.
-    val kryo = new Kryo()
-    val kryoFactory = new KryoFactory(HBaseConfiguration.create())
-    val registrations = Seq(
-        new KryoFactory.ClassPair(classOf[SpecificRecord], classOf[AvroSpecificSerializer]))
-    kryoFactory.setHierarchyRegistrations(registrations.asJava)
-    kryoFactory.populateKryo(kryo)
-
-    // Serialize the schema.
-    val output = new Output(1024)
-    kryo.writeObject(output, actual)
-
-    // Deserialize the schema.
-    val input = new Input(output.getBuffer)
-    kryo.readObject(input, classOf[SimpleRecord])
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/framework/KijiKeySuite.scala b/src/test/scala/org/kiji/express/flow/framework/KijiKeySuite.scala
deleted file mode 100644
index 676d3c5829f8fb371dccaaf6bb65ee42f8e05c19..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/framework/KijiKeySuite.scala
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.schema.EntityIdFactory
-import org.kiji.schema.avro.RowKeyEncoding
-import org.kiji.schema.avro.RowKeyFormat
-
-@RunWith(classOf[JUnitRunner])
-class KijiKeySuite extends FunSuite {
-  test("KijiKey should get the same EntityId you put in.") {
-    val entityIdFactory = EntityIdFactory.getFactory(
-      RowKeyFormat.newBuilder().setEncoding(RowKeyEncoding.RAW).build())
-    val testId = entityIdFactory.getEntityId("foob")
-    val testKey = new KijiKey()
-    testKey.set(testId)
-
-    assert(testId == testKey.get())
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/framework/KijiSchemeSuite.scala b/src/test/scala/org/kiji/express/flow/framework/KijiSchemeSuite.scala
deleted file mode 100644
index b04ea8531bc89ee41bb33e6c1f6fbcc1e1e51ac9..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/framework/KijiSchemeSuite.scala
+++ /dev/null
@@ -1,102 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import cascading.tuple.Tuple
-import cascading.tuple.TupleEntry
-import org.apache.avro.generic.GenericRecord
-import org.apache.avro.generic.GenericRecordBuilder
-import org.apache.hadoop.conf.Configuration
-import org.apache.hadoop.hbase.HBaseConfiguration
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.flow.All
-import org.kiji.express.flow.FlowCell
-import org.kiji.express.flow.ColumnInputSpec
-import org.kiji.express.flow.EntityId
-import org.kiji.express.flow.QualifiedColumnOutputSpec
-import org.kiji.express.flow.util.GenericCellSpecs
-import org.kiji.schema.EntityIdFactory
-import org.kiji.schema.avro.HashSpec
-import org.kiji.schema.avro.HashType
-
-@RunWith(classOf[JUnitRunner])
-class KijiSchemeSuite extends KijiSuite {
-  test("putTuple and rowToTuple can write and read a generic AvroRecord.") {
-    // Set up the table.
-    val configuration: Configuration = HBaseConfiguration.create()
-    val tableLayout = layout("layout/avro-types.json")
-    val table = makeTestKijiTable(tableLayout)
-    val kiji = table.getKiji
-    val uri = table.getURI
-    val writer = table.openTableWriter()
-    val reader = table.getReaderFactory.openTableReader(GenericCellSpecs(table))
-
-    // Set up the columns and fields.
-    val columnsOutput = Map("columnSymbol" -> QualifiedColumnOutputSpec("family:column3"))
-    val columnsInput = Map("columnSymbol" -> ColumnInputSpec("family:column3"))
-    val sourceFields = KijiScheme.buildSourceFields(columnsOutput.keys)
-
-    // Create a dummy record with an entity ID to put in the table.
-    val dummyEid = EntityId("dummy")
-    val record: GenericRecord = {
-      val builder = new GenericRecordBuilder(HashSpec.getClassSchema)
-      builder.set("hash_type", HashType.MD5)
-      builder.set("hash_size", 13)
-      builder.set("suppress_key_materialization", false)
-      builder.build()
-    }
-    val writeValue = new TupleEntry(sourceFields, new Tuple(dummyEid, record))
-
-    val eidFactory = EntityIdFactory.getFactory(tableLayout)
-
-    // Put the tuple.
-    KijiScheme.putTuple(
-        columnsOutput,
-        uri,
-        kiji,
-        None,
-        writeValue,
-        writer,
-        tableLayout,
-        configuration)
-
-    // Read the tuple back.
-    val rowData = reader.get(
-        dummyEid.toJavaEntityId(eidFactory),
-        KijiScheme.buildRequest(All, columnsInput.values))
-    val readValue: Tuple = KijiScheme.rowToTuple(
-        columnsInput,
-        sourceFields,
-        None,
-        rowData,
-        uri,
-        configuration)
-
-    val readRecord = readValue.getObject(1).asInstanceOf[Seq[FlowCell[_]]].head.datum
-    assert(record === readRecord)
-
-    reader.close()
-    writer.close()
-    table.release()
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/framework/KijiTableSplitSuite.scala b/src/test/scala/org/kiji/express/flow/framework/KijiTableSplitSuite.scala
deleted file mode 100644
index 32ff0f4f3e76b617bf9bcba29bb34055f6ccdf87..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/framework/KijiTableSplitSuite.scala
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import org.apache.hadoop.hbase.mapreduce.TableSplit
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-
-@RunWith(classOf[JUnitRunner])
-class KijiTableSplitSuite extends FunSuite {
-  val hTableSplit =
-      new TableSplit("name".getBytes, "startrow".getBytes, "endrow".getBytes, "location")
-  val kTableSplit = new KijiTableSplit(hTableSplit)
-
-  test("KijiTableSplit should have the same startrow as the hTableSplit.") {
-    assert(hTableSplit.getStartRow.toSeq == kTableSplit.getStartRow.toSeq)
-  }
-
-  test("KijiTableSplit should have the same endrow as the hTableSplit.") {
-    assert(hTableSplit.getEndRow.toSeq ==  kTableSplit.getEndRow.toSeq)
-  }
-
-  test("KijiTableSplit should have the same locations as the hTableSplit.") {
-    assert(hTableSplit.getLocations.toSeq == kTableSplit.getLocations.toSeq)
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/framework/KijiTapSuite.scala b/src/test/scala/org/kiji/express/flow/framework/KijiTapSuite.scala
deleted file mode 100644
index ad819ed0d7aadc346477bd284ae54a9e7908db26..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/framework/KijiTapSuite.scala
+++ /dev/null
@@ -1,140 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import org.apache.hadoop.hbase.HBaseConfiguration
-import org.apache.hadoop.mapred.JobConf
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.flow.All
-import org.kiji.express.flow.ColumnInputSpec
-import org.kiji.express.flow.InvalidKijiTapException
-import org.kiji.schema.KijiURI
-import org.kiji.schema.layout.KijiTableLayout
-
-@RunWith(classOf[JUnitRunner])
-class KijiTapSuite extends KijiSuite {
-  val instanceName: String = "test_KijiTap_instance"
-  val testKijiTableLayout: KijiTableLayout = layout("layout/avro-types.json")
-  val config: JobConf = new JobConf(HBaseConfiguration.create())
-
-  test("KijiTap validates a valid instance/table/column.") {
-    val testTable = makeTestKijiTable(testKijiTableLayout, instanceName)
-    val kijiURI = testTable.getURI
-
-    val testScheme: KijiScheme = new KijiScheme(
-        timeRange = All,
-        timestampField = None,
-        inputColumns = Map(
-            "dummy_field1" -> ColumnInputSpec("searches"),
-            "dummy_field2" -> ColumnInputSpec("family:column1")))
-
-    val testTap: KijiTap = new KijiTap(kijiURI, testScheme)
-
-    testTap.validate(config)
-  }
-
-  test("KijiTap validates a nonexistent instance.") {
-    val testTable = makeTestKijiTable(testKijiTableLayout, instanceName)
-    val kijiURI = testTable.getURI
-
-    val testScheme: KijiScheme = new KijiScheme(
-        timeRange = All,
-        timestampField = None,
-        inputColumns = Map(
-            "dummy_field1" -> ColumnInputSpec("searches"),
-            "dummy_field2" -> ColumnInputSpec("family:column1")))
-
-    val testURI: KijiURI = KijiURI.newBuilder(kijiURI)
-        .withInstanceName("nonexistent_instance")
-        .build()
-
-    val testTap: KijiTap = new KijiTap(testURI, testScheme)
-
-    intercept[InvalidKijiTapException] {
-      testTap.validate(config)
-    }
-  }
-
-  test("KijiTap validates a nonexistent table.") {
-    val testTable = makeTestKijiTable(testKijiTableLayout, instanceName)
-    val kijiURI = testTable.getURI
-
-    val testScheme: KijiScheme = new KijiScheme(
-        timeRange = All,
-        timestampField = None,
-        inputColumns = Map(
-            "dummy_field1" -> ColumnInputSpec("searches"),
-            "dummy_field2" -> ColumnInputSpec("family:column1")))
-
-    val testURI: KijiURI = KijiURI.newBuilder(kijiURI)
-        .withTableName("nonexistent_table")
-        .build()
-
-    val testTap: KijiTap = new KijiTap(testURI, testScheme)
-
-    intercept[InvalidKijiTapException] {
-      testTap.validate(config)
-    }
-  }
-
-  test("KijiTap validates a nonexistent column.") {
-    val testTable = makeTestKijiTable(testKijiTableLayout, instanceName)
-    val kijiURI = testTable.getURI
-
-    val testScheme: KijiScheme = new KijiScheme(
-        timeRange = All,
-        timestampField = None,
-        inputColumns = Map(
-            "dummy_field1" -> ColumnInputSpec("searches"),
-            "dummy_field2" -> ColumnInputSpec("family:nonexistent")))
-
-    val testTap: KijiTap = new KijiTap(kijiURI, testScheme)
-
-    val exception = intercept[InvalidKijiTapException] {
-      testTap.validate(config)
-    }
-
-    assert(exception.getMessage.contains("nonexistent"))
-  }
-
-  test("KijiTap validates multiple nonexistent columns.") {
-    val testTable = makeTestKijiTable(testKijiTableLayout, instanceName)
-    val kijiURI = testTable.getURI
-
-    val testScheme: KijiScheme = new KijiScheme(
-        timeRange = All,
-        timestampField = None,
-        inputColumns = Map(
-            "dummy_field1" -> ColumnInputSpec("nonexistent1"),
-            "dummy_field2" -> ColumnInputSpec("family:nonexistent2")))
-
-    val testTap: KijiTap = new KijiTap(kijiURI, testScheme)
-
-    val exception = intercept[InvalidKijiTapException] {
-      testTap.validate(config)
-    }
-
-    assert(exception.getMessage.contains("nonexistent1"))
-    assert(exception.getMessage.contains("nonexistent2"))
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/framework/KijiValueSuite.scala b/src/test/scala/org/kiji/express/flow/framework/KijiValueSuite.scala
deleted file mode 100644
index 662653ecf9f3e8ad3dae952a4af6a9459f74bcde..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/framework/KijiValueSuite.scala
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework
-
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-import org.scalatest.mock.EasyMockSugar
-
-import org.kiji.schema.KijiRowData
-
-@RunWith(classOf[JUnitRunner])
-class KijiValueSuite extends FunSuite with EasyMockSugar {
-  test("KijiValue should get the same RowData you put in.") {
-    val value = new KijiValue()
-    val row = mock[KijiRowData]
-
-    value.set(row)
-
-    assert(row == value.get())
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/framework/hfile/HFileKijiJobIntegration.scala b/src/test/scala/org/kiji/express/flow/framework/hfile/HFileKijiJobIntegration.scala
deleted file mode 100644
index 8e63ac86670b860a9aedb614923399989b6121c3..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/framework/hfile/HFileKijiJobIntegration.scala
+++ /dev/null
@@ -1,291 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.hfile
-
-import java.io.File
-
-import com.twitter.scalding.Args
-import com.twitter.scalding.TextLine
-import com.twitter.scalding.Tool
-import com.twitter.scalding.Tsv
-import org.apache.commons.io.FileUtils
-import org.apache.hadoop.fs.Path
-import org.junit.After
-import org.junit.Assert
-import org.junit.Before
-import org.junit.Test
-
-import org.kiji.express.flow.ColumnFamilyOutputSpec
-import org.kiji.express.flow.EntityId
-import org.kiji.express.flow.util.Resources
-import org.kiji.mapreduce.HFileLoader
-import org.kiji.schema.Kiji
-import org.kiji.schema.KijiDataRequest
-import org.kiji.schema.KijiDataRequestBuilder
-import org.kiji.schema.KijiTable
-import org.kiji.schema.layout.KijiTableLayouts
-import org.kiji.schema.testutil.AbstractKijiIntegrationTest
-
-class HFileKijiJobIntegration extends AbstractKijiIntegrationTest {
-
-  private var mKiji: Kiji = null
-
-  @Before
-  def setupTest {
-    val desc = KijiTableLayouts.getLayout("layout/avro-types-1.3.json")
-    mKiji = Kiji.Factory.open(getKijiURI())
-    mKiji.createTable(desc)
-  }
-
-  @After
-  def cleanupTest {
-    mKiji.release()
-  }
-
-  @Test
-  def testShouldBulkLoadMapReduceJob {
-    Resources.withKijiTable(mKiji, "table") { table =>
-      val tempHFileFolder = mTempDir.newFolder()
-      FileUtils.deleteDirectory(tempHFileFolder)
-
-      val toolRunnerArgs = Array(
-        classOf[SimpleAverageJob].getName(),
-        "--input",
-        "src/test/resources/data/input_lines.txt",
-        "--output",
-        table.getURI().toString(),
-        "--hfile-output",
-        tempHFileFolder.toString(),
-        "--hdfs")
-
-      Tool.main(toolRunnerArgs)
-
-      bulkLoad(tempHFileFolder, table)
-
-      Resources.withKijiTableReader(table) { myReader =>
-        val request = KijiDataRequest.create("family", "double_column")
-        val result = myReader.get(table.getEntityId("key1"), request)
-
-        Assert.assertEquals(20.0, result.getMostRecentValue("family", "double_column"), 0.0d)
-      }
-    }
-  }
-
-  @Test
-  def testShouldBulkLoadMapOnlyJob {
-    Resources.withKijiTable(mKiji, "table") { table =>
-      val tempHFileFolder = mTempDir.newFolder()
-      FileUtils.deleteDirectory(tempHFileFolder)
-
-      val toolRunnerArgs = Array(
-        classOf[SimpleLoaderJob].getName(),
-        "--input",
-        "src/test/resources/data/input_lines.txt",
-        "--output",
-        table.getURI().toString(),
-        "--hfile-output",
-        tempHFileFolder.toString(),
-        "--hdfs")
-
-      Tool.main(toolRunnerArgs)
-
-      bulkLoad(tempHFileFolder, table)
-
-      Resources.withKijiTableReader(table) { myReader =>
-        val colBuilder = KijiDataRequestBuilder.ColumnsDef
-          .create()
-          .withMaxVersions(10).add("family", "double_column")
-
-        val request = KijiDataRequest.builder().addColumns(colBuilder).build()
-        val result = myReader.get(table.getEntityId("key1"), request)
-        val cells = result.getCells("family", "double_column")
-
-        Assert.assertEquals(3, cells.size())
-        Assert.assertEquals(30.0, result.getMostRecentValue("family", "double_column"), 0.0d)
-      }
-    }
-  }
-
-  @Test
-  def testShouldBulkLoadMapOnlyJobWithAnotherOutput {
-    Resources.withKijiTable(mKiji, "table") { table =>
-      val tempHFileFolder = mTempDir.newFolder()
-      FileUtils.deleteDirectory(tempHFileFolder)
-
-      val tempTsvFolder = mTempDir.newFolder()
-      FileUtils.deleteDirectory(tempTsvFolder)
-
-      val toolRunnerArgs = Array(
-        classOf[SimpleLoaderMultiOutputJob].getName(),
-        "--input",
-        "src/test/resources/data/input_lines.txt",
-        "--output",
-        table.getURI().toString(),
-        "--tsv_output",
-        tempTsvFolder.toString(),
-        "--hfile-output",
-        tempHFileFolder.toString(),
-        "--hdfs")
-
-      Tool.main(toolRunnerArgs)
-
-      bulkLoad(tempHFileFolder, table)
-
-      Resources.withKijiTableReader(table) { myReader =>
-        val colBuilder = KijiDataRequestBuilder.ColumnsDef
-          .create()
-          .withMaxVersions(10).add("family", "double_column")
-
-        val request = KijiDataRequest.builder().addColumns(colBuilder).build()
-        val result = myReader.get(table.getEntityId("key1"), request)
-        val cells = result.getCells("family", "double_column")
-
-        Assert.assertEquals(3, cells.size())
-        Assert.assertEquals(30.0, result.getMostRecentValue("family", "double_column"), 0.0d)
-      }
-    }
-  }
-
-  @Test
-  def testShouldBulkLoadIntoMapFamily {
-    Resources.withKijiTable(mKiji, "table") { table =>
-      val tempHFileFolder = mTempDir.newFolder()
-      FileUtils.deleteDirectory(tempHFileFolder)
-
-      val tempTsvFolder = mTempDir.newFolder()
-      FileUtils.deleteDirectory(tempTsvFolder)
-
-      val toolRunnerArgs = Array(
-        classOf[SimpleLoaderMapTypeFamilyJob].getName(),
-        "--input",
-        "src/test/resources/data/input_lines.txt",
-        "--output",
-        table.getURI().toString(),
-        "--hfile-output",
-        tempHFileFolder.toString(),
-        "--hdfs")
-
-      Tool.main(toolRunnerArgs)
-
-      bulkLoad(tempHFileFolder, table)
-
-      Resources.withKijiTableReader(table) { myReader =>
-        val colBuilder = KijiDataRequestBuilder.ColumnsDef
-          .create()
-          .withMaxVersions(10).addFamily("searches_dev")
-
-        val request = KijiDataRequest.builder().addColumns(colBuilder).build()
-        val result = myReader.get(table.getEntityId("key1"), request)
-        val cells = result.getCells("searches_dev")
-
-        Assert.assertEquals(3, cells.size())
-      }
-    }
-  }
-
-  private def bulkLoad(hFilePath: File, table: KijiTable) {
-    val hFileLoader = HFileLoader.create(super.getConf())
-    hFileLoader.load(new Path(hFilePath.toString()), table)
-  }
-}
-
-class SimpleAverageJob(args: Args) extends HFileKijiJob(args) {
-
-  // Parse arguments
-  val inputUri: String = args("input")
-  val outputUri: String = args("output")
-  val hFileOutput = args("hfile-output")
-
-  // Read each line. Split on " " which should yield string, value
-  // string part eventually is the entity_id, value will be averaged in the end.
-
-  TextLine(inputUri)
-    .map('line -> ('entityId, 'numViews)) { line: String =>
-      val parts = line.split(" ")
-      (EntityId(parts(0)), parts(1).toInt)
-    }
-    .groupBy('entityId) { _.average('numViews) }
-    .write(HFileKijiOutput(outputUri, hFileOutput, ('numViews -> "family:double_column")))
-}
-
-class SimpleLoaderJob(args: Args) extends HFileKijiJob(args) {
-
-  // Parse arguments
-  val inputUri: String = args("input")
-  val outputUri: String = args("output")
-  val hFileOutput = args("hfile-output")
-
-  // Read each line. Generate an entityId and numViews. The entityId here is duplicated
-  // so there should be multiple versions of each in HBase.
-  TextLine(inputUri)
-    .read
-    .mapTo('line -> ('entityId, 'numViews, 'ts)) { line: String =>
-      val parts = line.split(" ")
-      Thread.sleep(2) // Force a sleep so that we get unique timestamps
-      (EntityId(parts(0)), parts(1).toDouble, System.currentTimeMillis())
-    }
-    .write(HFileKijiOutput(outputUri, hFileOutput, 'ts, ('numViews -> "family:double_column")))
-}
-
-class SimpleLoaderMapTypeFamilyJob(args: Args) extends HFileKijiJob(args) {
-
-  // Parse arguments
-  val inputUri: String = args("input")
-  val outputUri: String = args("output")
-  val hFileOutput = args("hfile-output")
-
-  @transient
-  lazy val outputCols = Map('numViews -> ColumnFamilyOutputSpec("searches_dev",
-      qualifierSelector='numViews))
-
-  // Read each line. Generate an entityId and numViews. The entityId here is duplicated
-  // so there should be multiple versions of each in HBase.
-  TextLine(inputUri)
-    .read
-    .mapTo('line -> ('entityId, 'numViews, 'ts)) { line: String =>
-      val parts = line.split(" ")
-      Thread.sleep(2) // Force a sleep so that we get unique timestamps
-      (EntityId(parts(0)), parts(1).toInt, System.currentTimeMillis())
-    }
-    .write(HFileKijiOutput(outputUri, hFileOutput, 'ts, outputCols))
-}
-
-class SimpleLoaderMultiOutputJob(args: Args) extends HFileKijiJob(args) {
-
-  // Parse arguments
-  val inputUri: String = args("input")
-  val outputUri: String = args("output")
-  val tsvOutputURI: String = args("tsv_output")
-  val hFileOutput = args("hfile-output")
-
-  // Read each line. Generate an entityId and numViews. The entityId here is duplicated
-  // so there should be multiple versions of each in HBase.
-  val computePipe = TextLine(inputUri)
-    .read
-    .mapTo('line -> ('entityId, 'numViews, 'ts)) { line: String =>
-      val parts = line.split(" ")
-      Thread.sleep(2) // Force a sleep so that we get unique timestamps
-      (EntityId(parts(0)), parts(1).toDouble, System.currentTimeMillis())
-    }
-
-  computePipe.write(HFileKijiOutput(outputUri, hFileOutput, 'ts,
-      ('numViews -> "family:double_column")))
-  computePipe.write(Tsv(tsvOutputURI))
-}
diff --git a/src/test/scala/org/kiji/express/flow/framework/serialization/KijiLockerSuite.scala b/src/test/scala/org/kiji/express/flow/framework/serialization/KijiLockerSuite.scala
deleted file mode 100644
index 415791ab37d01d9aa5a8a52ec29837811d6491eb..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/framework/serialization/KijiLockerSuite.scala
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.framework.serialization
-
-import java.io.ObjectOutputStream
-import java.io.ObjectInputStream
-import java.io.ByteArrayOutputStream
-import java.io.ByteArrayInputStream
-
-import org.apache.avro.Schema
-import org.scalatest.junit.JUnitRunner
-import org.junit.runner.RunWith
-
-import org.kiji.express.KijiSuite
-
-@RunWith(classOf[JUnitRunner])
-class KijiLockerSuite extends KijiSuite {
-
-  test("KijiLocker can serialize and deserialize a Schema.") {
-    val schema = Schema.create(Schema.Type.STRING)
-
-    val copy = lockerRoundtrip(schema)
-
-    assert(schema === copy)
-    assert(schema.toString(false) === copy.toString(false))
-  }
-
-  def lockerRoundtrip[T <: AnyRef](value: T): T = {
-    val baos = new ByteArrayOutputStream
-    val oos = new ObjectOutputStream(baos)
-    oos.writeObject(KijiLocker(value))
-
-    val ois = new ObjectInputStream(new ByteArrayInputStream(baos.toByteArray))
-
-    ois.readObject().asInstanceOf[KijiLocker[T]].get
-  }
-}
-
diff --git a/src/test/scala/org/kiji/express/flow/tool/TmpJarsToolSuite.scala b/src/test/scala/org/kiji/express/flow/tool/TmpJarsToolSuite.scala
deleted file mode 100644
index 212979a3db0e0427dd76b631846d2cede5fddddc..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/tool/TmpJarsToolSuite.scala
+++ /dev/null
@@ -1,119 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.tool
-
-import java.io.File
-
-import com.google.common.io.Files
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-
-@RunWith(classOf[JUnitRunner])
-class TmpJarsToolSuite extends FunSuite {
-
-  test("entryToFile transforms string paths to files.") {
-    assert(new File("/is/a/classpath/entry") == TmpJarsTool.entryToFile("/is/a/classpath/entry"))
-  }
-
-  def makeGlobTestDirectory(): File = {
-    // Make a temporary directory and put a directory, a .jar file, and a .txt file in it.
-    val tempDirectory = Files.createTempDir()
-    val subDirectory = new File(tempDirectory, "subdir")
-    subDirectory.mkdir()
-    val jarFile = new File(tempDirectory, "aJar.jar")
-    jarFile.createNewFile()
-    val txtFile = new File(tempDirectory, "aText.txt")
-    txtFile.createNewFile()
-    tempDirectory
-  }
-
-  test("globToFiles will unglob a file that is a glob (whose name is *).") {
-    val globTestDir = makeGlobTestDirectory()
-    val globTest = new File(globTestDir, "*")
-    val unglobbedFiles = TmpJarsTool.globToFiles(globTest)
-    assert(2 == unglobbedFiles.size)
-
-    assert(unglobbedFiles.contains(new File(globTestDir, "aJar.jar")))
-    assert(unglobbedFiles.contains(new File(globTestDir, "aText.txt")))
-    assert(!unglobbedFiles.contains(new File(globTestDir, "subdir")))
-  }
-
-  test("globToFiles will leave a file whose name is not * alone.") {
-    val testDir = new File("/i/am/a/file")
-    val unGlobbedFiles = TmpJarsTool.globToFiles(testDir)
-    assert(1 == unGlobbedFiles.size)
-    assert(testDir == unGlobbedFiles(0))
-  }
-
-  test("isJar says yes to a .jar file.") {
-    assert(TmpJarsTool.isJar(new File("/i/am/a/jarFile.jar")))
-  }
-
-  test("isJar says no to a file that isn't a .jar file.") {
-    assert(!TmpJarsTool.isJar(new File("/i/am/not/a/jarFile.txt")))
-  }
-
-  test("getJarsFromClasspath will ignore empty classpath entries.") {
-    val classpath = ":/i/am/aJar.jar::/i/am/anotherJar.jar:"
-    val jars = TmpJarsTool.getJarsFromClasspath(classpath)
-    assert(2 == jars.size)
-    assert(jars.contains(new File("/i/am/aJar.jar")))
-    assert(jars.contains(new File("/i/am/anotherJar.jar")))
-  }
-
-  test("getJarsFromClasspath will ignore directories.") {
-    val globTestDir = makeGlobTestDirectory()
-    val classpath = globTestDir.getAbsolutePath + ":" + new File("/i/am/aJar.jar")
-    val jars = TmpJarsTool.getJarsFromClasspath(classpath)
-    assert(1 == jars.size)
-    assert(jars.contains(new File("/i/am/aJar.jar")))
-  }
-
-  test("getJarsFromClasspath will unglob glob entries and leave regular files alone.") {
-    val globTestDir = makeGlobTestDirectory()
-    val globTest = new File(globTestDir, "*")
-    val classpath = globTest.getAbsolutePath + ":" + "/i/am/aJar.jar"
-    val jars = TmpJarsTool.getJarsFromClasspath(classpath)
-    assert(2 == jars.size)
-    assert(jars.contains(new File(globTestDir, "aJar.jar")))
-    assert(jars.contains(new File("/i/am/aJar.jar")))
-  }
-
-  test("getJarsFromClasspath will ignore unJars.") {
-    val classpath = "/i/am/aJar.jar:/i/am/text.txt"
-    val jars = TmpJarsTool.getJarsFromClasspath(classpath)
-    assert(1 == jars.size)
-    assert(jars.contains(new File("/i/am/aJar.jar")))
-  }
-
-  test("getjarsFromClasspath will transform a classpath into jar files accessible from that "
-      + "classpath.") {
-    val globTestDir = makeGlobTestDirectory()
-    val globTest = new File(globTestDir, "*")
-    val anotherTmpDir = Files.createTempDir()
-    val classpath = globTest.getAbsolutePath +
-        ":" + "/i/am/aJar.jar" + ":" + "/i/am/text.txt" + ":" + anotherTmpDir.getAbsolutePath
-    val jars = TmpJarsTool.getJarsFromClasspath(classpath)
-    assert(2 == jars.size)
-    assert(jars.contains(new File(globTestDir, "aJar.jar")))
-    assert(jars.contains(new File("/i/am/aJar.jar")))
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/util/AvroTupleConversionsSuite.scala b/src/test/scala/org/kiji/express/flow/util/AvroTupleConversionsSuite.scala
deleted file mode 100644
index 6396df245ba8c8900d2a3b0e6f44f5528d259597..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/util/AvroTupleConversionsSuite.scala
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import cascading.tuple.Fields
-import cascading.tuple.Tuple
-import cascading.tuple.TupleEntry
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.avro.NameFormats
-
-@RunWith(classOf[JUnitRunner])
-class AvroTupleConversionsSuite
-    extends KijiSuite {
-
-  test("Specific records get packed correctly with any variable name style.") {
-    val inputs = Map(
-        "snake_case_ugh" -> 13,
-        "CamelCaseEvenWorse" -> 14,
-        "camelPupCase" -> 15,
-        "SCREAMING_SNAKE_CASE_YAH" -> 16)
-    val fields = new Fields(inputs.keys.toSeq:_*)
-
-    val converter = AvroSpecificTupleConverter[NameFormats](fields,
-        implicitly[Manifest[NameFormats]])
-
-    val tuple = new Tuple()
-    inputs.values.foreach(tuple.addInteger)
-
-    val record: NameFormats = converter.apply(new TupleEntry(fields, tuple))
-
-
-    inputs.foreach { case (k, v) => assert(v === record.get(k)) }
-
-  }
-
-}
diff --git a/src/test/scala/org/kiji/express/flow/util/AvroTypesComplete.scala b/src/test/scala/org/kiji/express/flow/util/AvroTypesComplete.scala
deleted file mode 100644
index 5230bf6fefd3906b00877d8e8f0f1b3ccad4066a..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/util/AvroTypesComplete.scala
+++ /dev/null
@@ -1,137 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import scala.util.Random
-import scala.collection.JavaConverters.seqAsJavaListConverter
-
-import org.apache.avro.Schema
-import org.apache.avro.generic.GenericArray
-import org.apache.avro.generic.GenericData
-import org.apache.avro.generic.GenericData.Fixed
-import org.apache.avro.generic.GenericEnumSymbol
-import org.apache.avro.generic.GenericFixed
-import org.apache.avro.generic.GenericRecord
-import org.apache.avro.generic.GenericRecordBuilder
-
-import org.kiji.express.avro.SimpleRecord
-import org.kiji.express.flow.EntityId
-import org.kiji.schema.layout.KijiTableLayout
-import org.kiji.schema.layout.KijiTableLayouts
-
-/** Utils for testing against the avro-types-complete layout. */
-object AvroTypesComplete {
-
-  val layout = KijiTableLayout.newLayout(
-      KijiTableLayouts.getLayout("layout/avro-types-complete.json"))
-
-  val family = "strict"
-
-  /** Column names. */
-  val counterColumn = "counter"
-  val rawColumn = "raw"
-  val nullColumn = "null"
-  val booleanColumn = "boolean"
-  val intColumn = "int"
-  val longColumn = "long"
-  val floatColumn = "float"
-  val doubleColumn = "double"
-  val bytesColumn = "bytes"
-  val stringColumn = "string"
-  val specificColumn = "specific"
-  val genericColumn = "generic"
-  val enumColumn = "enum"
-  val arrayColumn = "array"
-  val mapColumn = "map"
-  val unionColumn = "union"
-  val fixedColumn = "fixed"
-
-  private val schemaParser = new Schema.Parser()
-
-  /** Column schemas. */
-  val nullSchema = Schema.create(Schema.Type.NULL)
-  val booleanSchema = Schema.create(Schema.Type.BOOLEAN)
-  val intSchema = Schema.create(Schema.Type.INT)
-  val longSchema = Schema.create(Schema.Type.LONG)
-  val floatSchema = Schema.create(Schema.Type.FLOAT)
-  val doubleSchema = Schema.create(Schema.Type.DOUBLE)
-  val bytesSchema = Schema.create(Schema.Type.BYTES)
-  val stringSchema = schemaParser.parse(
-      "{ \"type\": \"string\", \"avro.java.string\": \"String\" }")
-  val specificSchema = SimpleRecord.getClassSchema
-  val genericSchema = schemaParser.parse(
-      "{\"type\": \"record\", \"name\": \"Vector\", \"fields\": [" +
-          "{\"name\": \"length\", \"type\": \"int\"}," +
-          " {\"name\": \"angle\", \"type\": \"float\"}]}")
-  val enumSchema = schemaParser.parse(
-      "{\"type\": \"enum\", \"name\": \"Direction\", \"symbols\":" +
-          " [\"NORTH\", \"EAST\", \"SOUTH\", \"WEST\"]}")
-  val arraySchema = schemaParser.parse(
-    "{\"type\": \"array\", \"items\": { \"type\": \"string\", \"avro.java.string\": \"String\" }}")
-  val mapSchema = schemaParser.parse("{\"type\": \"map\", \"values\": \"int\"}")
-  val unionSchema = schemaParser.parse("[\"string\", \"int\"]")
-  val fixedSchema = schemaParser.parse("{\"type\": \"fixed\", \"size\": 10, \"name\": \"hash\"}")
-
-  /** Record builders. */
-  val specificBuilder = SimpleRecord.newBuilder()
-  val genericBuilder = new GenericRecordBuilder(genericSchema)
-  val genericData = new GenericData()
-
-
-  /** Value generators. */
-  val rand = new Random
-  val base: Iterable[_] = Range(0, 10) // Determines the number of inputs per test
-  val nulls = base.map { _ => null }
-  def booleans: Iterable[Boolean] = base.map { _ => rand.nextBoolean() }
-  def ints: Iterable[Int] = base.map { _ => rand.nextInt() }
-  def longs: Iterable[Long] = base.map { _ => rand.nextLong() }
-  def floats: Iterable[Float] = base.map { _ => rand.nextFloat() }
-  def doubles: Iterable[Double] = base.map { _ => rand.nextDouble() }
-  def bytes: Iterable[Array[Byte]] = base.map { _ =>
-    val ary = Array.ofDim[Byte](32)
-    rand.nextBytes(ary)
-    ary
-  }
-  def strings: Iterable[String] = base.map { _ => rand.nextString(32) }
-  def specificRecords: Iterable[SimpleRecord] = longs.zip(strings)
-      .map { fields => specificBuilder.setL(fields._1).setS(fields._2).build() }
-  def genericRecords: Iterable[GenericRecord] = ints.zip(floats)
-      .map { fields => genericBuilder.set("length", fields._1).set("angle", fields._2).build() }
-  def enumValues: Vector[String] = Vector("NORTH", "EAST", "SOUTH", "WEST")
-  def enums: Iterable[GenericEnumSymbol] = base.map { _ =>
-    genericData.createEnum(enumValues(rand.nextInt(4)), enumSchema).asInstanceOf[GenericEnumSymbol]}
-  def enumStrings: Iterable[String] = base.map { _ => enumValues(rand.nextInt(4))}
-  def arrays: Iterable[Iterable[String]] = base.map { _ => strings }
-  def avroArrays: Iterable[GenericArray[String]] = arrays.map { strings =>
-    new GenericData.Array(arraySchema, strings.toSeq.asJava)
-  }
-  def maps: Iterable[Map[String, Int]] = base.map { _ => strings.zip(ints).toMap }
-  def unions: Iterable[Any] = booleans.zip(strings.zip(ints)).map { t =>
-    val (bool, (string, int)) = t
-    if (bool) string else int
-  }
-  def fixedByteArrays: Iterable[Array[Byte]] = base.map { _ =>
-    val ary = Array.ofDim[Byte](10)
-    rand.nextBytes(ary)
-    ary
-  }
-  def fixeds: Iterable[GenericFixed] = fixedByteArrays.map { bs => new Fixed(fixedSchema, bs) }
-  def eids: Iterable[EntityId] = strings.map(EntityId(_))
-}
diff --git a/src/test/scala/org/kiji/express/flow/util/AvroUtilWithSchemaSuite.scala b/src/test/scala/org/kiji/express/flow/util/AvroUtilWithSchemaSuite.scala
deleted file mode 100644
index 6aaf1283e403916d89c615155be163dc6afa6540..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/util/AvroUtilWithSchemaSuite.scala
+++ /dev/null
@@ -1,71 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import scala.collection.JavaConverters.seqAsJavaListConverter
-
-import org.apache.avro.Schema
-import org.apache.avro.SchemaBuilder
-import org.apache.avro.generic.GenericData
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-
-@RunWith(classOf[JUnitRunner])
-class AvroUtilWithSchemaSuite extends FunSuite {
-
-  test("avroToScala will convert an Avro GenericArray to a Scala equivalent") {
-    val list = List("foo", "bar", "baz")
-    val schema = SchemaBuilder.array.items(Schema.create(Schema.Type.STRING))
-    val glist = new GenericData.Array(schema, list.asJava)
-    assert(list === AvroUtil.avroToScala(glist))
-  }
-
-  test("avroToScala will convert a GenericFixed to a Array[Byte]") {
-    val bytes = "foo bar, baz?".getBytes
-    val schema = SchemaBuilder.fixed("name").size(bytes.length)
-    val fixed = new GenericData.Fixed(schema, bytes)
-    assert(bytes === AvroUtil.avroToScala(fixed))
-  }
-
-  test("avroToScala will convert a GenericEnumSymbol to the String equivalent") {
-    val name = "CACTUS"
-    val schema = SchemaBuilder.enumeration("name").symbols("CACTUS", "CHERRY", "PIZZA")
-    val enum = new GenericData.EnumSymbol(schema, name)
-    assert(name === AvroUtil.avroToScala(enum))
-  }
-
-  /** TODO: implement once Avro generic map object exists. */
-  ignore("avroToScala will convert a GenericMap to the Scala equivalent.") { }
-
-  test("avroToScala will recursively convert a GenericArray to the Scala equivalents.") {
-    val names = List("CACTUS", "CHERRY", "CHERRY", "PIZZA")
-    val enumSchema = SchemaBuilder.enumeration("name").symbols("CACTUS", "CHERRY", "PIZZA")
-    val listSchema = SchemaBuilder.array.items(enumSchema)
-
-    val glist = new GenericData.Array(listSchema,
-        names.map(new GenericData.EnumSymbol(enumSchema, _)).asJava)
-
-    assert(names === AvroUtil.avroToScala(glist))
-  }
-
-  /** TODO: implement once Avro generic map object exists. */
-  ignore("avroToScala will recursively convert a GenericMap's values to the Scala equivalent.") { }
-}
diff --git a/src/test/scala/org/kiji/express/flow/util/CellMathUtilSuite.scala b/src/test/scala/org/kiji/express/flow/util/CellMathUtilSuite.scala
deleted file mode 100644
index 0cb65800eeda5b1c4c6decef5fc563368cc081ec..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/util/CellMathUtilSuite.scala
+++ /dev/null
@@ -1,73 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-import org.scalatest.matchers.ShouldMatchers
-
-import org.kiji.express.flow.FlowCell
-
-@RunWith(classOf[JUnitRunner])
-class CellMathUtilSuite
-    extends FunSuite
-    with ShouldMatchers {
-  val mapCell0: FlowCell[Long] = FlowCell("info", "a", 0L, 0L)
-  val mapCell1: FlowCell[Long] = FlowCell("info", "a", 1L, 1L)
-  val mapCell2: FlowCell[Long] = FlowCell("info", "b", 2L, 0L)
-  val mapCell3: FlowCell[Long] = FlowCell("info", "b", 3L, 3L)
-  val mapCell4: FlowCell[Long] = FlowCell("info", "c", 4L, 0L)
-  val mapCellSeq: List[FlowCell[Long]] = List(
-      mapCell4,
-      mapCell3,
-      mapCell2,
-      mapCell1,
-      mapCell0
-  )
-
-  test("CellMathUtils should properly sum simple types") {
-    assert(4 === CellMathUtil.sum(mapCellSeq))
-  }
-
-  test("CellMathUtils should properly compute the squared sum of simple types") {
-    assert(10.0 === CellMathUtil.sumSquares(mapCellSeq))
-  }
-
-  test("CellMathUtils should properly compute the average of simple types") {
-    assert(0.8 === CellMathUtil.mean(mapCellSeq))
-  }
-
-  test("CellMathUtils should properly compute the standard deviation of simple types") {
-    CellMathUtil.stddev(mapCellSeq) should be (1.16619 plusOrMinus 0.1)
-  }
-
-  test("CellMathUtils should properly compute the variance of simple types") {
-    CellMathUtil.variance(mapCellSeq) should be (1.36 plusOrMinus 0.1)
-  }
-
-  test("CellMathUtils should properly find the minimum of simple types") {
-    assert(0.0 === CellMathUtil.min(mapCellSeq))
-  }
-
-  test("CellMathUtils should properly find the maximum of simple types") {
-    assert(3.0 === CellMathUtil.max(mapCellSeq))
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/util/EntityIdFactoryCacheSuite.scala b/src/test/scala/org/kiji/express/flow/util/EntityIdFactoryCacheSuite.scala
deleted file mode 100644
index 93afd0fad5eecef738b9dc0958f3196234a9fc82..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/util/EntityIdFactoryCacheSuite.scala
+++ /dev/null
@@ -1,94 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-
-import org.apache.hadoop.hbase.HBaseConfiguration
-import org.junit.runner.RunWith
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.KijiSuite
-import org.kiji.express.flow.util.Resources._
-import org.kiji.schema.KijiTable
-import org.kiji.schema.layout.KijiTableLayout
-import org.kiji.schema.layout.KijiTableLayouts
-
-/**
- * Unit tests for [[org.kiji.express.flow.util.EntityIdFactoryCache]]
- */
-@RunWith(classOf[JUnitRunner])
-class EntityIdFactoryCacheSuite extends KijiSuite {
-  test("Test serializing/deserializing configurations") {
-    val configuration = HBaseConfiguration.create()
-    configuration.set("blah", "hi")
-
-    val deserializedConf =
-        EntityIdFactoryCache.deserializeConf(EntityIdFactoryCache.serializeConf(configuration))
-
-    assert(deserializedConf.get("blah") === configuration.get("blah"))
-  }
-
-  test("Test for caching for Row Key Format given Table Uri") {
-    /** Table layout to use for tests. */
-    val tableLayout = KijiTableLayout.newLayout(
-        KijiTableLayouts.getLayout(KijiTableLayouts.FORMATTED_RKF))
-    val uri: String = doAndRelease(makeTestKijiTable(tableLayout)) { table: KijiTable =>
-      table.getURI.toString
-    }
-    val configuration = HBaseConfiguration.create()
-
-    val eidFactory = EntityIdFactoryCache.getFactory(uri, configuration)
-
-    val components = new java.util.ArrayList[Object]()
-    components.add("a")
-    components.add("b")
-    components.add("c")
-    components.add(new java.lang.Integer(1))
-    components.add(new java.lang.Long(7))
-
-    // If this works, we successfully created a Formatted EntityId
-    // which means we were returned the right EID factory.
-    val eid = eidFactory.getEntityId(components)
-    assert(components == eid.getComponents)
-
-    val eidFactoryCached = EntityIdFactoryCache.getFactory(uri, configuration)
-    assert(eidFactory === eidFactoryCached)
-  }
-
-  test("Test cache for two equivalent configurations and table URIs memoizes correctly.") {
-    /** Table layout to use for tests. */
-    val tableLayout = KijiTableLayout.newLayout(
-      KijiTableLayouts.getLayout(KijiTableLayouts.FORMATTED_RKF))
-    val uri: String = doAndRelease(makeTestKijiTable(tableLayout)) { table: KijiTable =>
-      table.getURI.toString
-    }
-    val configuration = HBaseConfiguration.create()
-    configuration.addResource("blah")
-
-    val configurationCopy = HBaseConfiguration.create()
-    configurationCopy.addResource("blah")
-
-    val eidFactory1 = EntityIdFactoryCache.getFactory(uri, configuration)
-
-    val eidFactory2 = EntityIdFactoryCache.getFactory(uri, configurationCopy)
-
-    assert(eidFactory1 === eidFactory2)
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/util/MemoizeSuite.scala b/src/test/scala/org/kiji/express/flow/util/MemoizeSuite.scala
deleted file mode 100644
index 541365400378726b51ca79193ff1c496dd60960d..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/util/MemoizeSuite.scala
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-
-@RunWith(classOf[JUnitRunner])
-class MemoizeSuite extends FunSuite {
-  test("Test for memoization") {
-    // outer class without equals defined
-    class Outer(val data: Int)
-
-    def strSqLen(s: String): Outer = new Outer(s.length * s.length)
-    val strSqLenMemoized = Memoize(strSqLen)
-    val a = strSqLenMemoized("hello Memo")
-    val b = strSqLen("hello Memo")
-    assert(a.data == b.data)
-    // should go to cache for result
-    val c = strSqLenMemoized("hello Memo")
-    assert(a == c)
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/util/PipeRunner.scala b/src/test/scala/org/kiji/express/flow/util/PipeRunner.scala
deleted file mode 100644
index d19b291fd690c148462d485f457dce0c6561c70d..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/util/PipeRunner.scala
+++ /dev/null
@@ -1,129 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import java.lang.reflect.Constructor
-
-import scala.collection.mutable
-
-import cascading.flow.Flow
-import cascading.pipe.Pipe
-import cascading.tuple.Fields
-import cascading.tuple.Tuple
-import cascading.tuple.TupleEntry
-import com.twitter.scalding.Args
-import com.twitter.scalding.Job
-import com.twitter.scalding.JobTest
-import com.twitter.scalding.Mode
-import com.twitter.scalding.Read
-import com.twitter.scalding.Tsv
-import com.twitter.scalding.TupleConverter
-import com.twitter.scalding.TupleSetter
-import com.twitter.scalding.Write
-
-/**
- * Module for providing a test framework around testing Express pipes.  Allows testing of pipes
- * independent of the Source and Sink.  See `runPipe`.
- */
-object PipeRunner {
-
-  /**
-   * Unpacks [[scala.Product]] instances into [[cascading.tuple.Tuple]] instances.
-   * @param n number of fields expected in product type
-   */
-  private class ProductTupleSetter[T](n: Int) extends TupleSetter[T] {
-    def arity: Int = n
-
-    def apply(t: T): Tuple = {
-      if (n == 1) {
-        new Tuple(t.asInstanceOf[Object])
-      } else {
-        val p: Product = t.asInstanceOf[Product]
-        require(p.productArity == n)
-        val tuple = new Tuple()
-        p.productIterator.foreach(tuple.add)
-        tuple
-      }
-    }
-  }
-
-  /**
-   * Converts [[cascading.tuple.TupleEntry]]s into scala Tuples via frightening black majik.
-   */
-  private class ProductTupleConverter[O] extends TupleConverter[O] {
-
-    override def arity: Int = -1
-
-    var ctors: Map[Int, Constructor[_]] = Map()
-    private def tupleCtor(arity: Int) = {
-      if (!ctors.contains(arity)) {
-        ctors += arity -> Class.forName("scala.Tuple" + arity).getConstructors.apply(0)
-      }
-      ctors(arity)
-    }
-
-    override def apply(entry: TupleEntry): O = {
-      val arity = entry.size()
-      if (arity == 1) {
-        entry.getObject(0).asInstanceOf[O]
-      } else {
-        tupleCtor(entry.size())
-            .newInstance(Range(0, entry.size()).map(entry.getObject):_*)
-            .asInstanceOf[O]
-      }
-    }
-  }
-
-  /**
-   * Simulates running a pipe with given inputs containing given fields.  Returns a
-   * [[scala.collection.immutable.List]] of result values.  The returned value type is not known
-   * statically; therefore runtime type errors on the result are possible.
-   *
-   * @tparam I Type of input values.
-   * @tparam O Type of output values.  Statically resolves to [[scala.Nothing]].
-   * @param pipe Pipe to run.
-   * @param fields Field names of input tuples.
-   * @param input Iterable of input tuples.
-   * @return List of output values.
-   */
-  def runPipe[I, O](pipe: Pipe, fields: Fields, input: Iterable[I]): List[O] = {
-    val source = Tsv("input", fields)
-    val sink = Tsv("output")
-
-    implicit val setter: TupleSetter[I] = new ProductTupleSetter(fields.size())
-    implicit val converter: TupleConverter[O] = new ProductTupleConverter[O]
-
-    class InnerJob(args: Args) extends Job(args) {
-      override def buildFlow(implicit mode : Mode): Flow[_]  = {
-        validateSources(mode)
-        mode.newFlowConnector(config).connect(
-          pipe.getName, source.createTap(Read), sink.createTap(Write), pipe)
-      }
-    }
-
-    var buffer: mutable.Buffer[O] = null
-    val jobTest = JobTest(new InnerJob(_))
-        .source(source, input)
-        .sink(sink) { b: mutable.Buffer[O] => buffer = b}
-
-    jobTest.run
-    buffer.toList
-  }
-}
diff --git a/src/test/scala/org/kiji/express/flow/util/ResourcesSuite.scala b/src/test/scala/org/kiji/express/flow/util/ResourcesSuite.scala
deleted file mode 100644
index 7baa72774943dbd4141ea0c032adc064836ffc9e..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/flow/util/ResourcesSuite.scala
+++ /dev/null
@@ -1,95 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.flow.util
-
-import java.io.Closeable
-
-import org.junit.runner.RunWith
-import org.scalatest.FunSuite
-import org.scalatest.junit.JUnitRunner
-
-import org.kiji.express.flow.util.Resources._
-
-@RunWith(classOf[JUnitRunner])
-class ResourcesSuite extends FunSuite {
-  test("doAnd runs an operation") {
-    var isClosed = false
-    def resource: Closeable = new Closeable {
-      def close() {
-        isClosed = true
-      }
-    }
-    def after(r: Closeable) { r.close() }
-    doAnd(resource, after) { _ => () }
-
-    assert(isClosed)
-  }
-
-  test("doAnd catches one normal exception") {
-    var isClosed = false
-    def resource: Closeable = new Closeable {
-      def close() {
-        isClosed = true
-      }
-    }
-    def after(r: Closeable) { r.close() }
-    val result = try {
-      doAnd(resource, after) { _ => sys.error("test") }
-      false
-    } catch {
-      case err: RuntimeException => true
-    }
-
-    assert(isClosed)
-    assert(result)
-  }
-
-  test("doAnd catches one exception while closing") {
-    def resource: Closeable = new Closeable {
-      def close() {
-        sys.error("test")
-      }
-    }
-    def after(r: Closeable) { r.close() }
-    val result = try {
-      doAnd(resource, after) { _ => () }
-      false
-    } catch {
-      case err: RuntimeException => true
-    }
-
-    assert(result)
-  }
-
-  test("doAnd catches two exceptions") {
-    def resource: Closeable = new Closeable {
-      def close() {
-        sys.error("test")
-      }
-    }
-    def after(r: Closeable) { r.close() }
-    val result = try {
-      doAnd(resource, after) { _ => sys.error("t") }
-      false
-    } catch {
-      case CompoundException(_, Seq(_, _)) => true
-    }
-  }
-}
diff --git a/src/test/scala/org/kiji/express/repl/ExpressShellSuite.scala b/src/test/scala/org/kiji/express/repl/ExpressShellSuite.scala
deleted file mode 100644
index 68476ed065351169e65300926bebd5e4e3d9a3cb..0000000000000000000000000000000000000000
--- a/src/test/scala/org/kiji/express/repl/ExpressShellSuite.scala
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * (c) Copyright 2013 WibiData, Inc.
- *
- * See the NOTICE file distributed with this work for additional
- * information regarding copyright ownership.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kiji.express.repl
-
-import org.junit.runner.RunWith
-import org.scalatest._
-import org.scalatest.junit.JUnitRunner
-
-/**
- * Tests the functionality of [[org.kiji.express.repl.ExpressShell]].
- */
-@RunWith(classOf[JUnitRunner])
-class ExpressShellSuite extends FunSuite {
-  test("The shell can start a REPL using command line arguments.") {
-    pending
-  }
-
-  test("The shell runner can write a jar file containing classes in a virtual directory.") {
-    pending
-  }
-}
